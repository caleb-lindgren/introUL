---
title: "IntroUL_Manifold_Learning"
author: "Cenhao Zhu"
date: "2023-08-15"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Manifold Learning

## Background 

In the previous sections, we have focused on methods which seek to approximate our data through a linear combination of feature vectors.  As we have seen, the resulting approximations live on linear (or affine) subspaces in the case of PCA and SVD and positive spans or convex combinations in the case of NMF.  While our data may exhibit some low-dimensional structure, there is no practical reason to expect such behavior to be inherently linear.   In the resulting sections, we will explore methods which consider **nonlinear** structure and assume the data reside on or near a manifold.  Such methods are referred to as nonlinear dimension reduction or manifold learning.  Critical to this discussion is the notion of a manifold.

::: {.definition #def-manifold name="Informal Definition of a Manifold"}
A manifold is a (topological) space which locally resembles Euclidean space.  Each point on a $k$-dimensional manifold has a neighborhood that can be mapped continuously to $\mathbb{R}^k$.
:::

To guide your intuition, think of a manifold as a smooth, possibly curved surface. Here are a few examples.

::: {.example #ex-manifolds name="Examples of Manifolds"}
Add line, sphere, plane, and S
:::

And here is an example of something which isn't a manifold.

::: {.example #ex-non-manifold name="Non-manifold"}
figure 8
:::

Much more could be said about the mathematical foundations of manifolds which are far beyond the scope of this book. For those interested in the such details consider checking out REFERENCES HERE.  We will appeal to a more intuitive understanding of manifolds and when necessary provide informal, descriptive "definitions" of important concepts.  For now, let's turn to the standard manifold assumption which is common to this area of unsupervised learning. 

<!-- Often, more restrictive assumptions are made about the manifold. -->

<!-- ::: {.example #def-diff-manifold name="Differentiable Manifolds"} -->
<!-- A differentiable manifold is a manifold which is sufficiently smooth so that we could do calculus on it. -->
<!-- ::: -->

<!-- ::: {.example #def-riem-manifold name="Riemannian Manifold"} -->
<!-- A Riemannian manifold is a smooth manifold with a notion of inner products. From the inner product, we can define a notion of angles and distances allowing one to define distances between point son the manifold. -->
<!-- ::: -->


### Data on a manifold 

In the simplest setting, we will assume there are points $\vec{z}_1,\dots,\vec{z}_N\in A \subset \mathbb{R}^k$ which are *iid* random samples. These points are (nonlinearly) mapped into a higher dimensional space $\mathbb{R}^d$ by a smooth map $\Psi$ giving data $\vec{x}_i = \Psi(\vec{z}_i)$ for $i=1,\dots,N.$   Hereafter, we refer to $\Psi$ as the manifold map.  In this setting, we are only given $\vec{x}_1,\dots,\vec{x}_N$, and we want to recover the lower-dimensional $\vec{z}_1,\dots,\vec{z}_N$. If possible, we would also like recover $\Psi$ and $\Psi^{-1}$ and in the most ideal case, the sampling distribution that generated the lower-dimensional coordinates $\vec{z}_1,\dots,\vec{z}_N$. 

::: {.example #ex-swiss-roll name="Mapping to the Swiss Roll"}
Let $A = (\pi/2,9\pi/2)\times (0,15)$.  We define the map $\Psi:A\to \mathbb{R}^3$ as follows
$$\Psi(\vec{z}) = \Psi(z_1,z_2) = \begin{bmatrix} z_1\sin(z_1) \\ z_1\cos(z_1) \\ z_2 \end{bmatrix}$$
Below we show $N=10^4$ \emph{iid} samples which are drawn uniformly from $A$.  We then show the resulting observations after applying map $\Psi$ to each sample.
```{r, echo = FALSE}
N <- 1e4
myColorRamp <- function(colors, values) {
    v <- (values - min(values))/diff(range(values))
    x <- colorRamp(colors)(v)
    rgb(x[,1], x[,2], x[,3], maxColorValue = 255)
}
library(scatterplot3d)
S <- matrix(NA, nrow = N, ncol = 2)
Swiss <- matrix(NA, nrow = N, ncol = 3)
for( n in 1:N){
    s <- runif(1, min = pi/2, max = 9*pi/2)
    t <- runif(1, min = 0,      max = 15)
    S[n,] <- c(s,t)
    Swiss[n, ] <- c( s*sin(s), s*cos(s),t )
}
par(mfrow = c(1,2))
plot(S[,1],S[,2],
     xlab = expression(z[1]), ylab = expression(z[2]),
     main = "Low-Dimensional Samples",
     cex = 0.1)
scatterplot3d(Swiss, color = myColorRamp(c("red","purple","blue","green","yellow"), S[,1] ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              main = "Samples after applying \nthe Manifold Map",
              cex.symbols = 0.5)
```
:::


We may also consider the more complicated case where the observations are corrupted by additive noise.  In this setting, the typical assumption is that the noise follows after the manifold map so that our data are $$\vec{x}_i = \Psi(\vec{z}_i) + \vec{\epsilon}_i, \qquad i = 1,\dots, N$$ for some \emph{iid} noise vectors $\{\vec{\epsilon}_i\}_{i=1,\dots,N}.$  

::: {.example #ex-swiss-w-noise name="Swiss Roll with Additive Gaussian Noise"}

Here, we perturb the observations in the preceding example with additive $\mathcal{N}(\vec{0},0.1{\bf I})$ noise.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library("threejs")
scatterplot3js(Swiss+matrix(rnorm(prod(dim(Swiss)),mean = 0,sd = 0.5), nrow = nrow(Swiss)), color = myColorRamp(c("red","purple","blue","green","yellow"), S[,1] ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              main = "Swiss Roll Data Perturbed by Additive Noise", 
              pch = '.',
              size = 0.1)
```
:::

In addition to the goals in the noiseless case, we may also add the goal of learning the noiseless version of the data which reside on a manifold.

However, there are a number of practical issues to this setup.  First, the dimension, $k$, of the original lower-dimensional points is typically unknown.  Similar to previous methods, we could pick a value of $k$ with the goal of visualization, base our choice off of prior knowledge, or run our algorithms different choices of $k$ and compare the results.  More advanced methods for estimating the true value of $k$ are an open area of research (REFERENCES NEEDED).  

There is also a issue with the uniqueness problem statement.  Given only the high dimensional observations, there is no way we could identify the original lower-dimensional points without more information.  In fact, one could find an unlimited sources of equally suitable results.  Here is the issue.

Let $\Phi:\mathbb{R}^k\to\mathbb{R}^k$ be some invertible function.  As an example, you could think of $\Phi$ as defining a translation, reflection, rotation, or some composition of these operations.  If our original observed data are $\vec{x}_i = \Psi(\vec{z}_i)$, our manifold learning algorithm could instead infer that the manifold map is $\Psi \circ \Phi^{-1}$ and the lower-dimensional points are $\Phi(\vec{z}_i)$. This is a perfectly reasonable result since $(\Psi\circ \Phi^{-1}\circ)\Phi(\vec{z}_i) = \Psi(\vec{z}_i)= \vec{x}_i$ for $i=1,\dots,N$, which is the only result we require.  Without additional information, there is little we could do to address this issue.  For the purposes of visualization, however, we will typically be most interested in the relationship between the lower-dimensional points rather than their specific location or orientation.  As such, we need not be concerned about a manifold learning algorithm that provides a translated or rotated representation of $\vec{z}_1,\dots,\vec{z}_N.$ More complicated transformations of the lower-dimensional coordinates are of greater concern and may be addressed through additional assumptions about the manifold map $\Psi.$

In the following sections, we will review a small collection of different methods which address the manifold learning problem.  This collection is by no means exhaustive so we provide a small list with associated references to conclude the chapter.


<!-- <!-- ISOMAP --> -->
<!-- ```{r child = 'topics/ISOMAP.Rmd'} -->
<!-- ``` -->

# ISOMAP

## Introduction

The isometric feature mapping (ISOMAP) algorithm, first proposed in 2000 by Tenenbaum et al., is a widely-used nonlinear dimensional reduction method. It combines the major algorithmic features of PCA and MDS --- computational efficiency, global optimality, and asymptotic convergence guarantees. Thanks to these extraordinary features, ISOMAP is capable of learning a broad class of nonlinear manifolds. 


## Assumptions

In addition to the basic assumptions adopted by all manifold learning methods, ISOMAP assumes that the smooth manifold $\mathbb{M}$, where the $n$ data points $z_1, z_2, \dots, z_N \in  \mathbb{R}^{k}$ are from; and that the manifold map $\Psi (\cdot )$ is an isometry.

Among these two assumptions, the $\textbf{isometry}$ assumption is believed to be fairly reasonable. It is to say the geodesic distance is invariant before and after the mapping.
$$d^{\mathbb{M}}(z_i, z_j) = d^{\mathbb{X}}(\Psi(z_i),\Psi(z_j)) for \forall 1 \leq i,j \leq N$$

However, the second assumption --- the $\textbf{convexity}$ of the manifold $\mathbb{M}$ is sort of restrictive.

## Algorithm

ISOMAP algorithm is quite similar to that of MDS, with just some changes in the construction of the distance matrix.

1) Neighborhood Graph

MDS uses Euclidean distance to measure how far two data points $x_i$ and $x_j$ are, while ISOMAP uses the geodesic distance in order to reveal the underlying manifold structure. However, when the data points in the high dimensional space $\mathbb{X}$ have a manifold structure, usually the Euclidean distance between two points is quite different from their geodesic distance. Fortunately, when these two data points are close enough, their Euclidean distance is almost equivalent to their geodesic distance. Actually, it can be proofed that the Euclidean distance between two neighboring data points converge to their geodesic distance when the density of the data points in $\mathbb{X}$ goes to infinity. 

Two intuitive example can explain this point very clearly. First, consider a two-dimensional manifold --- Swiss Roll(a real one!) in a three-dimensional space. For an ant crawling on that, since it is too tiny in size compared to that Swiss Roll, from its' view, the area it is standing on is flat. The length between its two adjacent footsteps is almost the same from a human being's perspective (the Euclidean distance) and the ant's perspective (the geodesic distance). Another example is in a much larger scale, say, the Earth. Imagine some aliens have the technology to pass through the crust and mantle, and they travel in a straight line to have the shortest possible distance (Euclidean distance). Then they may save themselves several hundred miles if they travel from LA to New York compared to their human friends, however, they have few advantage when they travel from Science Center to Smith Center. 

As a result, when it comes to the measurement of geodesic distance, it is reasonable to only look at those data points that are close to each other. First, calculate all the pairwise Euclidean distance $d_{ij}=||x_i-x_j||^2_2$, then determine which points are neighbors on the manifold by connecting each point to Either (i) All points that lie within a ball of radius $\epsilon$ of that point; OR (ii) all points which are K-nearest neighbors with it. (Two different criteria, $K$ and $\epsilon$ are tuning parameters)

According to this rule, a weighted neighborhood graph $G = G(V,E)$ can be built. The set of vertices (data points in $\mathbb{X}$): $V = \{x_1, \dots , x_N\}$ are the input data points, and the set of edges $E = \{e_{ij}\}$ indicate neighborhood relationships between the points. $e_{ij} = d_{ij}$ if (i) $||x_i - x_j||_2 \leq \epsilon$; OR (ii) $x_j$ is one of the K-nearest neighbors of $x_i$, or otherwise. $e_{ij}=0$ if the two points are not connected.


2) Compute Graph Distances

In this step, we want to estimate the unknown true geodesic distances $\{d^{\mathbb{M}}_{ij}\}$ between all pairs of points with the help of the neighborhood graph $G$ we have just built. We use the graph distances $\{d^{\mathbb{G}}_{ij}\}$--- the shortest distances between all pairs of points in the graph $G$ to estimate $\{d^{\mathbb{M}}_{ij}\}$. For $x_i$ and $x_j$ that are not connected to each other, we try to find the shortest path that goes along the connected points on the graph. Following this particular sequence of neighbor-to-neighbor links, the sum of all the link weights along the path is defined as $\{d^{\mathbb{G}}_{ij}\}$. In other words, we use a number of short Euclidean distances (representing the local structure of the manifold) to approximate the geodesic distance $\{d^{\mathbb{X}}_{ij}\}$. 

This path finding step is usually done by Floyd-Warshall algorithm, which iteratively try all transit points $k$ and find those that $\tilde{d}_{ik} + \tilde{d}_{kj} < \tilde{d}_{ij}$, and update $\tilde{d}_{ij} = \tilde{d}_{ik} + \tilde{d}_{kj}$ for all possible combination of $i,j$. The algorithm works best in dense neighboring graph scenario, with a computational complexity of $O(n^3)$. 

The theoretical guarantee of this graph distance computation method is given by Bernstein et, al. one year after they first proposed ISOMAP in their previous paper. They show that asymptotically (as $n \rightarrow \infty$), the estimate $d^{\mathbb{G}}$ converges to $d^{\mathbb{M}}$ as long as the data points are sampled from a probability distribution that is supported by the entire manifold, and the manifold itself is flat.

The distance matrix $\Delta$ can be expressed as:
$$\Delta_{ij} = d^{\mathbb{G}}_{ij}$$

3) Applying MDS to $\Delta$

As mentioned before, ISOMAP can be viewed as the application of MDS in non-linear case. As a result, the reconstruction of $\{z_{i}\}$ in the $k$ dimensional feature space $\mathbb{Z}$ follows similar steps as that of ISOMAP. The main goal is to preserve the geodesic distance of the manifold $\mathbb{M}$ as much as possible. 

Without any additional information, there are infinite $\{z_{i}\}$ that can be viewed as the optimal solution. For some invertible function $\Phi:\mathbb{R}^k\to\mathbb{R}^k$, a new manifold mapping $\Psi \circ \Phi^{-1}$ can be constructed. $x_{i} = \Psi \circ \Phi^{-1} (\Phi(z_{i}))$, which proofs that $\{Phi(z_{i})\}$ is equivalent to $\{z_{i}\}$ when it comes to the reconstruction of the lower dimensional space.

Without loss of generality, we assume that $\{z_{i}\}$ are actually centered. So the distance matrix of $\{z_{i}\}$ can be expressed as $B=Z^T Z$, so that $B_{ii}=||z_i||^2_2$ and $B_{ij}={z_i}^T z_j$.

The embedding vectors $\{\hat{z}_{i}\}$ (estimate of points in lower dimensional feature space) are chosen in order to minimize:
$$(\sum ||z_i - z_j||_2 - \Delta_{ij})^2$$

Following the same procedure as in MDS, we can compute each entry of $B$:
$$B_{ij}= -\frac{1}{2} \Delta^2_{ij} + \frac{1}{d} \sum^{d}_{i=1} \Delta^2_{ij} + \frac{1}{d} \sum^{d}_{j=1} \Delta^2_{ij} - \frac{1}{2d^2} \sum^{d}_{i=1} \sum^{d}_{j=1} \Delta^2_{ij}$$

To express it in matrix form, it is actually, $B = - \frac{1}{2} H \Delta H$, where $H=I_n - \frac{1}{n} \mathbb{1}^T \mathbb{1}$.

The next step is just a PCA problem. Implement eigen decomposition on matrix B, $B=U \Lambda U^T= (\Lambda^{1/2} U)^T (\Lambda^{1/2} U)$, then arrange the singular value in descending order, find the first $k^{\prime}$ ones.We acquire $\Lambda_{k^{\prime}}$ and $U_{k^{\prime}}$. 
$$\{\hat{z}_1, \hat{z}_2, \dots, \hat{z}_N\} = \Lambda_{k^{\prime}} U_{k^{\prime}}$$

Since we don't know the dimension of the underlying feature space, here $k^{\prime}$ is a tuning parameter. Usually, we use a scree plot ($k^{\prime}$ against the sum of the omitted eigenvalues ) and find the elbow point.

## Limitation of ISOMAP

Though ISOMAP is a powerful manifold learning method that works well under most circumstances. It still has some limitations in certain scenarios.

1) One of the two major assumptions of ISOMAP is the convexity of the manifold, that is to say, if the manifold contains many holes and concave margins, then the result of ISOMAP will probably be not ideal. 

2) If the data points in some parts of the manifold is sparse, chances are that the learnt manifold structure will be ruined. The example below best illustrates this point:

3) If the noises $\{\epsilon_i\}$ is not negligible, then ISOMAP may fail to identify the manifold. To alleviate the negative impact, it's highly suggested to start with a relatively small $\epsilon$ or $K$, and increase them gradually.








<!-- <!-- LLE --> -->
<!-- ```{r child = 'topics/LLE.Rmd'} -->
<!-- ``` -->

<!-- <!-- AE --> -->
<!-- ```{r child = 'topics/AE.Rmd'} -->
<!-- ``` -->

## Additional methods

## Exercises








