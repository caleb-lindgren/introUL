```{r, echo = FALSE}
library(MASS)
library(threejs)
```
# Central goals and assumptions

In the remainder of this text, we will largely focus on the case where we are given a dataset containing samples $\vec{x}_1,\dots,\vec{x}_N \in \mathbb{R}^d$.  We will assume that the vectors were drawn independently from some unknown data generating process.  As we discussed briefly in Chapter \@ref(intro)

## Dimension reduction and manifold learning

## Clustering

## Generating synthetic data 

At the beginning of this chapter, we indicated that our data are drawn from some unknown distribution.  This is a practical assumption, but in many cases, it is also helpful to consider examples where we generate the data ourselves.  In doing so, we can create whatever complicated structure we would like such as different clustering arrangements or lower dimensional structure. We can test an Unsupervised Learning algorithm of interest on these synthetically generated data to see if important relationships or properties are accurately preserved.  This is a helpful method for evaluating how well an algorithm works in a specific case, and importantly, can be used to build intuition on a number of natural complexities such as appropriately choosing tuning parameters, evaluating the effects of noise, and seeing how these algorithms may break when certain assumptions are not met. 

First, let us consider the case of generating data with a known lower dimensional structure which will be valuable when testing a dimension reduction or manifold learning algorithm.  We'll begin with data on a hyperplane. Later in Chapter \@ref(ch-linear), we consider data on a hyperplane with additional constraints which can be generated by small changes to the method discussed below. 

:::{.example #hyperplane name="Generating data on a hyperplane"}
Suppose we want to generate a set of $d$-dimensional data which is on a $k<d$ dimensional hyperplane.  The span of $k$ linearly independent vectors $\vec{z}_1,\dots,\vec{z}_k \in \mathbb{R}^d$ defines a $k$ dimensional hyperplane.  If we then generated random coefficient $c_1,\dots,c_k$, then the vector $$\vec{x} = c_{1}\vec{z}_1+\dots +c_{k}\vec{z}_k$$ would be an element on this hyperplane.  To generate a data set we could then

1) Specify $\vec{z}_1,\dots,\vec{z}_k$ or generate them randomly
2) Draw random coefficients $c_1,\dots,c_k$ and compute the random sample $\vec{x}=c_{1}\vec{z}_1+\dots +c_{k}\vec{z}_k$.  
3) Repeat step 2, $N$ times to generate the $N$ samples.

In Figure \@ref(fig:hyperplane), we show an example of data generated to reside on a random plane in $\mathbb{R}^3$.  We first generate $\vec{z}_1,\dots,\vec{z}_k$ randomly by drawing each vector from a $\mathcal{N}(\vec{0},{\bf I})$ distribution.  These vectors will be independent with probability 1.  When then take coefficients $c_1,\dots,c_k$ which are **iid** $N(0,1)$.

```{r hyperplane, fig.cap="Randomly generated points concentrated on a two-dimensional hyperplane."}
set.seed(185)
N <- 100
# basis
zs <- mvrnorm(n=2, mu = rep(0,3), Sigma = diag(1,3))
#coeffs
coeffs <- matrix(rnorm(2*N),ncol = 2)
# generate data matrix
samples <- coeffs %*% zs
# plot results
scatterplot3js(samples,
              xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              pch = '.',
              size = 0.2)
```
:::

Generating data on a curved surface is generally more complicated. In some cases, we may define them implicitly via a constraint such as the unit sphere in $d$-dimensions $$S^{d-1} = \{\vec{x}\in\mathbb{R}^d: \|\vec{x}\| = 1\}.$$  Generating data on the unit sphere can then be accomplished by drawing a vector from any distribution on $\mathbb{R}^d$ then rescaling the vector to have unit length.  Alternatively, we could consider a function which parameterizes a surface.  We show one such example below.

::: {.example s-manifold name="Generating data on the S-manifold"}

:::


In Figure \@ref(fig:ex-clusters), we show two different ways to generate and define clusters in a set of vectors in $\mathbb{R}^2$. 

```{r ex-clusters, fig.cap = "In each subfigure below, different subsets of points were generated using different rules and are colored accordingly .  Ideally, a clustering algorithm could detect the different cluster shapes (left: ellipsoids, right: concentric rings) and correctly group points depending on how they were generated.", fig.align = "left", echo = FALSE}
## use Gaussian mixture model to generate ellipsoidal clusters
set.seed(185)
gmm <- rbind(mvrnorm(n=100, mu = c(0,3), 0.25*matrix(c(1,0,0,1),nrow = 2)),
             mvrnorm(n=100, mu = c(-1.5,0), 0.4*matrix(c(1,0.4,0.4,1),nrow = 2)),
             mvrnorm(n=100, mu = c(1.5,0), Sigma = diag(0.1,nrow = 2)))
## generate rings data
rings <- matrix(rnorm(600), ncol = 2)
for (j in 1:300){
  if (j <= 100){
    rings[j, ] <- rings[j,]/sqrt(sum(rings[j,]^2))
  } else if ((j > 100)  & (j <= 200)){
    rings[j, ] <- 2*rings[j,]/sqrt(sum(rings[j,]^2))
  } else
    rings[j, ] <- 3*rings[j,]/sqrt(sum(rings[j,]^2))
}
par(mfrow = c(1,2))
plot(gmm, col = c(rep("blue",100),rep("red",100),rep("black",100)),
     xlab = '', ylab = '')
plot(rings, col = c(rep("blue",100),rep("red",100),rep("black",100)),
     xlab = '', ylab = '')
```

If one did not have access to the actual data generating process (depicted by the different colors), it is still likely that they could recover the correct groupings upon visual inspection.  In general, this strategy is not tractable. Naturally, we would like an Unsupervised clustering algorithm that can learn these clusters directly from the data automatically.  As we shall see in Chapter \@ref(ch-clustering), certain algorithms which excel at grouping the data contained in disjoint ellipsoids will naturally struggle data clustering in concentric rings because the shape(s) of the different clusters matters has a major impact of the accuracy of the clustering.  

 

## Exercises