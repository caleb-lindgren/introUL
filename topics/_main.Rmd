## Autoencoders (AEs)

<!--chapter:end:AE.Rmd-->

## Multidimensional Scaling {#sec-mds}

1) Introduction and Settings
 
Imagine there are $N$ objects (geographical locations, characteristics of people, etc.) In some cases, it is hard to depict each of these $N$ objects explicitly with a vector $\vec{y} \in \mathbb{R}^{N}$, however, we can measure the degree of "closeness" (proximity) between these $N$ objects. In other words, we can obtain a $N \times N$ matrix $\Delta$, with $\Delta_{rs}$ representing the distance/dissimilarity between objects $r$ and $s$. Assume $\Delta_{rr}=0$ and $\Delta_{sr} = \Delta_{rs}$. $\Delta$ is often called a Proximity/Distance/Dissimilarity Matrix. 

Given the Proximity Matrix $\Delta$, MDS (Multi-dimensional Scaling) comes to place when it comes to recovering the original/latent individual configuration $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N \in \mathbb{R}^{t}$. 

The primary goal of MDS is to reconstruct the original configuration as close as possible, say, find points $\tilde{y}_1, \tilde{y}_2, \dots, \tilde{y}_N \in \mathbb{R}^{t^{\prime}}$ s.t. the distance between $\tilde{y}_r$ and $\tilde{y}_s$ is close to $\Delta_{rs}$. Sometimes, the true dimension $t$ is unknown, so determining the number of dimensions is also a major problem to be solved.

MDS can also serve as a data visualization method. Usually, $t^{\prime}$ is set to be either two or three, and the $N$ points in the two-dimensional (or three-dimensional) plot can show an obvious sign of clustering, where points in a particular cluster are viewed as being “closer” to the other points in that cluster than to points in other clusters.


2) Example

Here we use the sphere distances of multiple large cities as an example to illustrate the power of MDS in recovering the original expressions and visualizing them.

```{r setup-mds, include=FALSE, echo = FALSE, message = FALSE, fig.align='center'}
require("scatterplot3d")
knitr::opts_chunk$set(echo = FALSE)
library('MASS')
library('maps')
library("gdata")
library('plotly')
```

The *eurodist* R package provides road distance between 21 cities in Europe and 9 cities in the US. For the moment, we will focus on the 10 US cities with names and distances given in the following tables

```{r, echo = FALSE}
knitr::kable(as.matrix(UScitiesD), "simple")
```

After conducting classical MDS (a method of MDS, details will be discussed in proceeding parts), we acquire the plot below. The plot complies to our geographical knowledge. Miami and Seattle are the farthest from our plot, and these two cities are actually thousands of miles away (Seattle in the north-west end of US mainland and Miami in the south-east corner). NY and D.C. are quite close on the plot, same for LA and San Francisco.


```{r, echo = FALSE}
city.loc <- cmdscale(UScitiesD, k =2);
plot(city.loc, type = 'n', 
     xlab = 'Dimension 1',
     ylab = 'Dimension 2')
text(city.loc, labels = attr(UScitiesD, "Labels"),
     cex = 0.75)
```
Actually, if we rotate the above plot 180 degrees, a US map appears. It is amazing!

```{r, echo = FALSE}
city.loc.rot <- -city.loc
plot(city.loc.rot, type = 'n', 
     xlab = 'Dimension 1',
     ylab = 'Dimension 2')
text(city.loc.rot, labels = attr(UScitiesD, "Labels"),
     cex = 0.75)
```
Again, let's try MDS on the 21 European cities. We make some flipping on the map to make it comply to the true European map.

```{r}
knitr::kable(as.matrix(eurodist),"simple")
```

```{r, echo = FALSE}
city.loc <- cmdscale(eurodist)
# plot(city.loc, type = 'n', 
#      xlab = 'Dimension 1',
#      ylab = 'Dimension 2')
# text(city.loc, labels = attr(eurodist, "Labels"),
#      cex = 0.75)

city.loc.flip <- city.loc %*% matrix(c(1,0,0,-1),nrow = 2)
plot(city.loc.flip, type = 'n', 
     xlab = 'Dimension 1',
     ylab = 'Dimension 2')
text(city.loc.flip, labels = attr(eurodist, "Labels"),
     cex = 0.75)
```

The above plot reconstructs the European map quite well. Gibraltar, Lisbon and Madrid are in the south-west corner, the two North European cities Stockholm and Copenhagen are in the north end, and Athens is in the south-west corner.

Finally, let's consider the 18 representative global cities. Their pairwise flight lengths (geodesic distance) are shown in the table below. As we can see, the geodesic distances between the three Southern-Hemisphere cities: Rio, Cape Town, Melbourne and other Northern-Hemisphere cities are generally large (almost all over 10,000 kilometers)

```{r, echo = FALSE}
D <- matrix(0, 18,18)
row.names(D) <- names <- c("Beijing","Cape Town","Hong Kong", "Honolulu", "London", "Melbourne", 
                  "Mexico City", "Montreal", "Moscow", "New Delhi", "New York", "Paris",
                  "Rio", "Rome", "S.F.", "Singapore", "Stockholm", "Tokyo")
continent <- c("blue","green","blue","red","black","orange","red","red","blue","blue","red","black","purple","black","red","blue","black","blue")
colnames(D) <- names

D[2:18,  1] <- c(12947, 1972, 8171, 8160, 9093, 12478, 10490, 5809, 2788, 11012, 8236, 17325, 8144, 9524, 4465, 6725, 2104)
D[3:18,  2] <- c(11867, 18562, 9635, 10388, 13703, 12744, 10101, 9284, 12551, 9307, 6075, 8417, 16487, 9671, 10334, 14737)
D[4:18,  3] <- c(8945, 9646, 7392, 14155, 12462, 7158, 3770, 12984, 9650, 17710, 9300, 11121, 2575, 8243, 2893)
D[5:18,  4] <- c(11653, 8862, 6098, 7915, 11342, 11930, 7996, 11988, 13343, 12936, 3857, 10824, 11059, 6208)
D[6:18,  5] <- c(16902, 8947, 5240, 2506, 6724, 5586, 341, 9254, 1434, 8640, 10860, 1436, 9585)
D[7:18,  6] <- c(13557, 16730, 14418, 10192, 16671, 16793, 13227, 15987, 12644, 6050, 15593, 8159)
D[8:18,  7] <- c(3728, 10740, 14679, 3362, 9213, 7669, 10260, 3038, 16623, 9603, 11319)
D[9:18,  8] <- c(7077, 11286, 533, 5522, 8175, 6601, 4092, 14816, 5900, 10409)
D[10:18, 9] <- c(4349, 7530, 2492, 11529, 2378, 9469, 8426, 1231, 7502)
D[11:18, 10] <- c(11779, 6601, 14080, 5929, 12380, 4142, 5579, 5857)
D[12:18, 11] <- c(5851, 7729, 6907, 4140, 15349, 6336, 10870)
D[13:18, 12] <- c(9146, 1108, 8975, 10743, 1546, 9738)
D[14:18, 13] <- c(9181, 10647, 15740, 10682, 18557)
D[15:18, 14] <- c(10071, 10030, 1977, 9881)
D[16:18, 15] <- c(13598, 8644, 8284)
D[17:18, 16] <- c(9646, 5317)
D[18:18, 17] <- 8193

D <- D + t(D)

knitr::kable(D, "simple")
```

```{r, echo = FALSE}
MDS1 <- cmdscale(D,k=1, eig = TRUE, x.ret = TRUE)
MDS2 <- cmdscale(D,k=2, eig = TRUE, x.ret = TRUE)
MDS3 <- cmdscale(D,k=3, eig = TRUE, x.ret = TRUE)
MDS4 <- cmdscale(D,k=4, eig = TRUE, x.ret = TRUE)
distances1 <- distances2 <- distances3 <- distances4 <- matrix(0,18,18)
for (s in 2:18){
  for (t in 1:(s-1)){
    distances1[s,t] <- (MDS1$points[s] - MDS1$points[t])^2
    distances2[s,t] <- t(MDS2$points[s,] - MDS2$points[t,]) %*% (MDS2$points[s,] - MDS2$points[t,])
    distances3[s,t] <- t(MDS3$points[s,] - MDS3$points[t,]) %*% (MDS3$points[s,] - MDS3$points[t,])
    distances4[s,t] <- t(MDS4$points[s,] - MDS4$points[t,]) %*% (MDS4$points[s,] - MDS4$points[t,])
  }
}
```

```{r,echo = FALSE, webgl=TRUE, warning = FALSE, message = FALSE}
ax <- list(
  title = "",
  zeroline = FALSE,
  showline = FALSE,
  showticklabels = FALSE,
  showgrid = TRUE
)
scene = list(
  xaxis = ax,
  yaxis = ax,
  zaxis = ax)
plot_ly(data.frame( x = MDS3$points[,1], y= MDS3$points[,2], z = MDS3$points[,3]),
        text = names,
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5, color = continent)) %>%
  add_markers() %>%
  add_text() %>%
  layout(scene = scene)
# fig <- scatterplot3d(,
#                   xlab = "", ylab = "", zlab = "", 
#                   tick.marks = FALSE, label.tick.marks = FALSE,
#                   grid = FALSE, box = TRUE, color = continent)
# text(fig$xyz.convert(MDS3$points[,1:3]),labels= rownames(MDS3$points), pos=1) 
```

The three-dimensional visualization result of MDS is shown above. You can rotate and magnify it on your laptop. The blue points represent Asian cities, the black points represent European cities, and the red points represent North American cities. They are clustered together within the group. If you try to rotate the plot and make these three clusters in the bottom, you will find the three separate cities: Rio, Melbourne and Cape Town on the top of the plot. This actually represents that they are the only cities in Southern-Hemisphere. If you inspect the plot clearly, you will find that these cities actually locate on a sphere, which complies to the true scenario. MDS is quite successful in recovering their relative locations!


3) Key features of MDS

Besides showing the capacity of MDS in recovering original expressions and visualization, we also gain two by-products from the examples above.

\noindent (i) The pairwise distances/proximity between two objects don't have to be Euclidean distance. In the above example, they are actually geodesic distance. Actually, based on the type of distance metric and the specific recovery method of the Proximity Matrix, MDS can be divided into three major types: Classical Scaling; Metric MDS; and Non-Metric MDS. 

Classical Scaling and Metric MDS generally require that the input data is a true distance metric, while Non-metric MDS is usually used when the input data doesn't satisfy the properties of a true distance metric or when the relationships are ordinal (i.e., we only know which distances are larger, but not by how much). 

To add, Classical MDS operates by applying an eigen-decomposition to the "double-centered" dissimilarity matrix, while Metric MDS and Non-Metric MDS may employ iterative optimization methods to better accommodate non-linear relationships in data structure.

(ii) As we can tell from the recovery of US map and European map, the configurations of $\tilde{y}_1, \tilde{y}_2, \dots, \tilde{y}_N$ are not unique, as we can rotate or flip the map. Actually, if $\tilde{y}_1, \tilde{y}_2, \dots, \tilde{y}_N \in \mathbb{R}^{t^{\prime}}$ is considered as the optimal solution, given any vector $\vec{b} \in \mathbb{R}^{t^{\prime}}$ and orthogonal matrix $A \in \mathbb{R}^{t^{\prime} \times t^{\prime}}$, $||(A \tilde{y}_r + \vec{b} - (A \tilde{y}_s + \vec{b})|| = ||\tilde{y}_r - \tilde{y}_s||$. Rotation, Reflection or Translation don't alter the pairwise distances. So $A \tilde{y}_1 + \vec{b}, A \tilde{y}_2 + \vec{b}, \dots, A \tilde{y}_N + \vec{b}$ is also an optimal solution.



### Classical Scaling

Before going deep into Classical Scaling, we first need to introduce some important definitions.

#### Definitions:

(i) A matrix $\Delta \in \mathbb{R}^{N \times N}$ is called a distance matrix if it possesses the following properties:

a) Symmetry

$\Delta$ is symmetric, meaning that:

$$\Delta_{rs} = \Delta_{sr} \quad \text{for all } r \text{ and } s$$

This implies that the distance between points $r$ and $s$ is the same as the distance between points $s$ and $r$.

b) Zero Diagonal

The diagonal entries of the matrix represent the distance of a point to itself, and are thus all zeros:

$$\Delta_{rr} \equiv 0 \quad \text{for all } 1 \leq r \leq N$$

c) Non-negativity

All distances are non-negative:

$$\Delta_{rs} \geq 0$$

d) Triangle Inequality

The matrix respects the triangle inequality:

$$\Delta_{rs} \leq \Delta_{rt} + \Delta_{ts}$$

This property ensures that the direct distance between two points $r$ and $s$ is never greater than the sum of the distances from $r$ to a third point $t$ and from $t$ to $s$.


(ii) Further, the distance matrix $\Delta \in \mathbb{R}^{N \times N}$ is a Euclidean distance matrix if there exists a configuration $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N$ s.t. $\Delta_{rs}$ represents the Euclidean distance between points $r$ and $s$, i.e., $||\vec{y}_r-\vec{y}_s||=\Delta_{rs} \forall r,s$. 

For configuration $\vec{y}_i, \vec{y}_j \in \mathbb{R}^t$, the Euclidean distance between point $r$ and point $s$ is defined as $\delta_{ij}=||\vec{y}_i - \vec{y}_j||= \left\{\sum_{k=1}^t \left(y_{i k}-x_{j k}\right)^2\right\}^{1/2}$. **Remember to take the square root**

3) Prerequisite for Classical Scaling: 

(i) Strictly speaking, we require that the Proximity Matrix $\Delta$ is an Euclidean distance matrix. 

(ii) Furthermore, it is assumed that the configurations $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N$ are centered. In other words, $\sum_{i=1}^N \vec{y}_i = \vec{0}$. This step eliminates the influence of location.

#### Recover Coordinates

It may sound difficult to recover the configuration $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N$ from merely the Euclidean distance matrix $\Delta$ at the first glance. However, the situation becomes clearer after some analysis.

$\Delta^2_{rs}$ can be expressed as $\Delta^2_{rs} = ||\vec{y}_r||^2 + ||\vec{y}_s||^2 - 2 \vec{y}_r^T \vec{y}_s$. It's not difficult to observe that both terms are both very easy to be expressed with the original configuration: $Y = \left(\begin{array}{c} \vec{y}_1^{\top} \\ \vec{y}_2^{\top} \\ \vdots \\ \dot{y}_N^{\top} \end{array}\right)$.

The entries of the inner product matrix, i.e. $B = YY^T \in \mathbb{R}^{N \times N}$ are able to express the Euclidean distance matrix $\Delta$. 

**(1) Computing the inner product matrix $B$**

The inner product term can be represented as: $B_{ij} = \vec{y}_r^T \vec{y}_s$, while the norms of $\vec{y}_r$ and $\vec{y}_s$ can be represented by the entry of $B$, i.e. $B_{ii}} = ||\vec{y}_i||^2$. As a result, $\Delta^2_{ij} = B_{ii} + B_{jj} - 2 B_{ij}$. 

**Key Observation** Here, we successfully express the entries of $\Delta$ (known) with entries of $B$ (unknown). However, our goal is to find a way to express entries of $B$ (unknown) with entries of $\Delta$. Intuitively, we want to cancel out $B_{ij}$ terms in the expression. Considering that $\sum_{i=1}^N \vec{y}_i = \vec{0}$, we sum both sides of the equation over the index $i$. We can get the following expression: (Because $\sum_{i=1}^N B_{ij} = \vec{y}_j^T (\sum_{i=1}^N \vec{y}_i) = 0$)
$$\sum_{i=1}^N \Delta_{ij}^2=\operatorname{tr}(B) + N B_{ii}$$

Similarly, we can also sum both sides of the equation over index $j$, and get the expression:
$$\sum_{j=1}^N \Delta_{ij}^2=\operatorname{tr}(B) + N B_{jj}$$

We successfully eliminate all the off-diagonal terms of $B$ through the above steps. Now, we want to take a step further. Sum both sides of the equations over both indexes $i$ and $j$. We acquire the following expression:
$$\sum_{i=1}^N \sum_{j=1}^N \Delta_{ij}^2 = 2N \operatorname{tr}(B)$$

Now we can solve the entries of $\Delta$ using the entries of $B$ through a backward calculation. From the last equation, we get 
$$\operatorname{tr}(B) = \frac{1}{2N} \sum_{i=1}^N \sum_{j=1}^N \Delta_{ij}^2$$

Then substitute the above expression into above formulas, we get the expression of the diagonal entries of $B$:

$$B_{ii} = \frac{1}{N} (\sum_{j=1}^{N} \Delta_{ij}^2 - \operatorname{tr} (B))$$

After that, we can finally get the off-diagonal entries of $B$:

\begin{aligned}
B_{ij} & = \frac{1}{2} (B_{ii} + B_{jj} - \Delta_{ij}^2) \\
& = -\frac{1}{2} \Delta_{ij}^2 + \frac{1}{N} \sum_{i=1}^N \Delta_{ij}^2 + \frac{1}{N} \sum_{j=1}^N \Delta_{ij}^2-\frac{1}{2 N^2} \sum_{i=1}^N \sum_{j=1}^N \Delta_{ij}^2
\end{aligned}

Actually, we may also express the inner product matrix $B$ in a matrix form, which is what we do in real data computation. Here $A \in \mathbb{R}^{N \times N}, A_{ij} = \Delta^2_{ij} \forall 1 \leq i,j \leq N$, $H = \mathbb{I}_N - \frac{1}{N} \mathbb{1}_N \mathbb{1}_N^T$. 

$$B = H A H$$



**(2) Recover the coordinates using inner product matrix $B$**

Both diagonal and off-diagonal entries of the inner product matrix $B$ has been shown. We assumed that $B$ = $YY^T$, so $B$ is symmetric and positive semi-definite (all eigenvalues are non-negative), with $t$ positive eigenvalues and $N-t$ 'zero' eigenvalues. 

Our intuition is to apply SVD to $B$ in order to recover the configuration $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N \in \mathbb{R}^t$.

\begin{align}
B & = \left(\vec{u}_1\left|\vec{u}_2\right| \ldots \mid \vec{u}_t\right) \begin{pmatrix}
\lambda_1 & \cdots & 0 \\
 \vdots & \ddots & \vdots \\
0 & \cdots & \lambda_t
\end{pmatrix} \left(\begin{array}{c}
\vec{u}_1^{\top} \\
\vec{u}_2^{\top} \\
\vdots \\ 
\vec{u}_t^{\top} 
\end{array}\right) \\
& = \tilde{u} \begin{pmatrix}
\lambda^{1/2}_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda^{1/2}_t \end{pmatrix}
\begin{pmatrix}
\lambda^{1/2}_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda^{1/2}_t \end{pmatrix}
\tilde{u}^T \\
& = (\tilde{u} \Lambda^{1/2}) (\tilde{u} \Lambda^{1/2})^T
\end{align}

Let $Y=\tilde{u} \Lambda^{1/2}$, use rows of $\tilde{u} \Lambda^{1/2}$ as $\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N$. It satisfies every entry of the inner product matrix $B$, as well as all pairwise distances $\Delta_{ij}$. 

**Dealing with real data case**

In real data case, sometimes the Euclidean condition is not met. As in the previous globe map example, the distance matrix actually computes the geodesic distance instead of the Euclidean distance. Under this circumstance, the "inner product" matrix $B = HAH$ (we put a quotation mark here because $B$ is no longer the inner product matrix) is not necssarily positive semi-definite.

Let's put a numeric example here to illustrate this point. 

Suppose we have a proximity matrix $\Delta$:

Given the matrix:
\[ 
\Delta = \begin{pmatrix} 0 & 1 & 3 \\ 1 & 0 & 1 \\ 3 & 1 & 0 \end{pmatrix} 
\]

```{r}
# Given matrix Delta
Delta <- matrix(c(0,1,3,1,0,1,3,1,0), nrow=3, byrow=TRUE)
Delta
```

Obviously, matrix $\Delta$ does not satisfy Triangle Inequality, so it's not an Euclidean distance matrix (even not a distance matrix actually)

1. Compute matrix \( A \) as:
\[ 
A = -\frac{1}{2} \Delta^2 
\]

```{r}
A <- -0.5 * Delta^2
A
```

2. Compute matrix \( B \) using:
\[ 
H = I - \frac{1}{n} \mathbf{11}^T 
\]
Where \( I \) is the identity matrix and \( n \) is the number of rows (or columns) in \( \Delta \). Then:
\[ 
B = H A H 
\]

```{r}
# compute matrix H
n <- nrow(Delta)
I <- diag(n)
one_vec <- matrix(1, n, 1)
H <- I - (1/n) * one_vec %*% t(one_vec)

# compute matrix B
B <- H %*% A %*% H
B
```

3. Finally, perform an eigen-decomposition on matrix \( B \).

```{r}
eigen_decomp_B <- eigen(B)

# Extract eigenvalues and eigenvectors
eigenvalues_B <- eigen_decomp_B$values
eigenvectors_B <- eigen_decomp_B$vectors

eigenvalues_B
eigenvectors_B
```
Here, we have a negative eigenvalue $-\frac{5}{6}$. This is because the original proximity matrix $\Delta$ is not a distance matrix.


**Handle Negative Eigenvalues**

a) **non-symetric issue:** There are cases where the proximity matrix $\Delta$ is not symmetric (though it rarely happens in classical scaling scenario). Usually we set $\Delta \leftarrow \Delta + \Delta^T$. In that way, we manually make $\Delta$ symmetric.

b) When there exist some negative eigenvalues in the inner product matrix $B$, we usually have two options to deal with it. 

i) Inflate the original proximity matrix $\Delta$ by a small constant factor $c$, i.e., $\Delta_{ij} \leftarrow \Delta_{ij} + c, \text{if} \quad i \neq j$. In this way, we can deal with the violence of **Triangular Inequality**.

ii) If there exist several negative eigenvalues with small absolute value (compared to the largest several positive eigenvalues), and there are more positive eigenvalues than our prior estimation (the dimension of the original configuration), we may just pick the largest $t$ eigenvalues and eliminate the rest.


**Global City Distance case revisit**

We can also consider the previous global city distance matrix example. Plot the scree plot of the inner product matrix. 

```{r, echo = FALSE}
plot(cmdscale(D, eig = TRUE)$eig, xlab = 'k', ylab = expression(lambda[k]))
```

We find that the first three eigenvalues are much larger than the rest, so we assume that the dimension of the original configuration is 3, which also complies to our knowledge about global map.








### Metric MDS



### Nonmetric MDS

<!--chapter:end:classic-MDS.Rmd-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, cache = TRUE)
set.seed(1)
require("dimRed")
require("energy")
require("scatterplot3d")
require("rgl")
require("plotly")
library("Rdimtools")
require("princurve")

myColorRamp <- function(colors, values) {
    v <- (values - min(values))/diff(range(values))
    x <- colorRamp(colors)(v)
    rgb(x[,1], x[,2], x[,3], maxColorValue = 255)
}
```

```{r, include=FALSE}
bookdown::render_book("ISOMAP.Rmd", "bookdown::gitbook")
```

## Isometric Feature Map (ISOMAP)

### Introduction

The first manifold learning method we are going to cover is the Isometric Feature Map (ISOMAP), originally published by Tenenbaum, de Silva, and Langford in 2000 [@isomap]. As suggested by the name, we will see that the assumption of isometry is central to this method. ISOMAP combines the major algorithmic features of PCA and MDS --- computational efficiency, global optimality, and asymptotic convergence guarantees. Thanks to these extraordinary features, ISOMAP is capable of learning a broad class of nonlinear manifolds. 

### Key Definitions

**Different notions of pointwise distance**

Prior to discussing the ISOMAP algorithm, let's briefly discuss the notion of isometry through an example which motivates different notions of distance between two points.

::: {.example #ex-helix-distance name="Distance between points on a Helix"}
Consider the helix map $\Psi:\mathbb{R}\to\mathbb{R}^3$ given by the formula
\begin{equation}
\Psi(t) = \begin{bmatrix}
\frac{1}{\sqrt{2}}\cos(t) \\
\frac{1}{\sqrt{2}}\sin(t) \\
\frac{1}{\sqrt{2}}t \\
\end{bmatrix}
\end{equation}
Below, we show the result of applying the Helix map to each point in the interval $(0,25)$.  Let's focus on two points $\vec{x}_1 = \Psi(2\pi)= (1/\sqrt{2},0,\sqrt{2}\pi)^T$ and $\vec{x}_2 = \Psi(4\pi)=(1/\sqrt{2},0,2\sqrt{2}\pi)^T$ in particular which are shown as large black dots in the figure below.

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library("threejs", quietly = T)
N <- 1e4
t <- sort(runif(N, min = 0, max = 25))
helix <- matrix(NA,nrow = length(t), ncol = 3)
for (n in 1:length(t)){
  helix[n,] <- c( 2^-0.5*cos(t[n]), 2^-0.5*sin(t[n]) , 2^-0.5*t[n])
}
# par(mfrow = c(1,2))
# plot(t,rep(0,N),
#      xlab = 'z', ylab = '',
#      main = "Low-Dimensional Samples",
#      cex = 0.1)
# points(c(2*pi,4*pi),rep(0,2), col = "red",cex = 1)
black_points <- data.frame(
  x = 2^-0.5*c(1,1),  
  y = 2^-0.5*c(0,0),  # Replace with your desired y coordinates
  z = 2^-0.5*pi*c(2,4)  # Replace with your desired z coordinates
)

black_points <- as.matrix(black_points)

s <- scatterplot3js(helix, color = geoelectrics::myColorRamp(c("red","purple","blue","green","yellow"), t),
              xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              main = "The Helix Map",
              pch = '.',
              size = 0.1) 

threejs::points3d(s, x = black_points,  color = "black", pch = '.', size = 0.15)
```


There are a few different ways we could measure the distance between the two black points.  The first approach would be to ignore the helix (manifold) structure viewing them as vectors in $\mathbb{R}^3$ and directly measure their **Euclidean distance** which gives $$\|\vec{x}_1 - \vec{x}_2\| = \sqrt{2}\pi.$$  However, we also know that these points are images of the one-dimensional coordinate $z_1 = 2\pi$ and $z_2 = 4\pi$ respectively.  Thus, we could also consider the Euclidean distance of the lower-dimemsional coordinates which is $|2\pi - 4\pi| = 2\pi$, which notably differs from the Euclidean distance.  

A third option is to return to the three-dimensional representation but to also account for the manifold structure when considering distance. Recall Euclidean distance gives the length of the shortest, **straightline** path connecting the two points. Instead, let's restrict ourselves to only those paths which stay on the helix (manifold).  You may correctly conclude that the curve starting at $\Psi(2\pi)$, rotating up the helix one rotation, and ending at $\Psi(4\pi)$ is the shortest such path. Fortunately, computing arc-length is relatively friendly in this example since $\Psi$ already parameterizes the path connecting these two points. The arc-length is then 
$$\int_{2\pi}^{4\pi} \left\|\frac{d\Psi}{dt}\right\| dt = \int_{2\pi}^{4\pi} dt = 2\pi.$$
Jumping slightly ahead, we then say the manifold distance between $\Psi(2\pi)$ and $\Psi(4\pi)$ is $2\pi$.  Importantly, the manifold distance coincides exactly with the Euclidean distance between the **lower-dimensional** coordinates.  In fact, for any two points, $s$ and $t$, on the real line their Euclidean distance, $|s-t|$ will be the same as the manifold distance between $\Psi(s)$ and $\Psi(t)$. Thus, the helix map $\Psi$ above serves as our first example of an isometric (distance preserving) map.
:::

We may generalize this idea to any smooth manifold to define a new notion of distance.  Given a manifold $\mathcal{M}$, we define the manifold distance function $d_\mathcal{M} : \mathcal{M} \times \mathcal{M} \to [0,\infty)$ as follows 

**Definition of Manifold Distance**
::: {.definition #def-manifold-dist name="Manifold Distance Function"}
Given two points $\vec{x}$ and $\vec{y}$ on a smooth manifold, $\mathcal{M}$, let $\Gamma(\vec{x},\vec{y})$ be the set of all piecewise smooth curves connecting $\vec{x}$ and $\vec{y}$ constrained to stay on $\mathcal{M}$.  Then, we define the manifold distance to be
\begin{equation}
d_\mathcal{M}(\vec{x},\vec{y}) = \inf_{\gamma \in \Gamma(\vec{x},\vec{y})} L(\gamma)
(\#eq:def-manifold-dist)
\end{equation}
where $L(\gamma)$ is the arclength of $\gamma.$  
:::

As we reviewed above, the helix example with the arclength formula is one example of a manifold and distance function.  Additional examples of a manifold and manifold distance include, 

- Euclidean space $\mathbb{R}^d$ where standard Euclidean distance gives the manifold distance. 
- The sphere in $\mathbb{R}^3$ which is a two-dimensional manifold. Its manifold distance is also called the [Great Circle Distance](https://en.wikipedia.org/wiki/Great-circle_distance).

We may now define the notion of isometry which is a central assumption of ISOMAP.

**Definition of Isometry**
::: {.definition #def-isometry name="Isometry"}
Let $\mathcal{M}_1$ be a manifold with distance function $d_{\mathcal{M}_1}$ and let $\mathcal{M}_2$ be a second manifold with distance function $d_{\mathcal{M}_2}$.  The mapping $\Psi:\mathcal{M}_1 \mapsto \mathcal{M}_2$ is an isometry if $$d_{\mathcal{M}_1}(x,y) = d_{\mathcal{M}_2}\left(\Psi(x),\Psi(y)\right) \qquad \text{ for all } x,y\in \mathcal{M}_1.$$
:::

For the purposes of ISOMAP, we will think of $\mathcal{M}_1$ as some subset of a $\mathbb{R}^k$ for $k$ small where we measure distances using the Euclidean norm.  Then $\mathcal{M}_2$ will be a $k$-dimensional manifold in $\mathbb{R}^d$ containing our data. Our first assumption is that the manifold mapping $\Psi$ is an isometry. Unfortunately, in practice we do not know the manifold nor will we have a method for parameterizing curves on the manifold to compute distances. 

### Algorithm
Instead, ISOMAP makes use of a data-driven approach to estimate the manifold distance between points following a three-step procedure.  

**1) Construct Weighted Neighborhood Graph:** 

MDS uses Euclidean distance to measure pairwise distance between points $\vec{x}_i$ and $\vec{x}_j$ (data points in space $\mathcal{M}_2$), while ISOMAP uses the geodesic distance in order to reveal the underlying manifold structure. However, when the data points in the high dimensional space $\mathcal{M}_2$ have a manifold structure, usually the Euclidean pairwise distance is quite different from their pairwise geodesic distance. Fortunately, for small distances on a smoothly embedded manifold, the geodesic path between two close-by points lies nearly flat in the ambient space. So, the length of this path will be very close to the straight line (Euclidean) distance between those points in the ambient space.

The key intuition is that as the density of data points on the manifold increases (i.e., points get closer and closer), the straight line segment in the ambient space connecting two neighboring points becomes a better and better approximation of the shortest path between those points on the manifold. In the limit of the density going to infinity, these distances converge. 

Two intuitive example may help you understand this point. First, consider a two-dimensional manifold --- Swiss Roll(a real one!) in a three-dimensional space. For an ant crawling on that, since it is too tiny in size compared to that Swiss Roll, from its' view, the area it is standing on is flat. The length between its two adjacent footsteps is almost the same from a human being's perspective (the Euclidean distance) and the ant's perspective (the geodesic distance). Another example is in a much larger scale, say, the Earth. Imagine some aliens have the technology to pass through the crust and mantle, and they travel in a straight line to have the shortest possible distance (Euclidean distance). Then they may save themselves several hundred miles if they travel from LA to New York compared to their human friends, however, they have few advantage when they travel from Science Center to Smith Center. 

As a result, when it comes to the measurement of geodesic distance, it is reasonable to only look at those data points that are close to each other. First, calculate all the pairwise Euclidean distance $d_{ij}=||\vec{x}_i - \vec{x}_j||_2$, then determine which points are neighbors on the manifold by connecting each point to Either (i) All points that lie within a ball of radius $\epsilon$ of that point; OR (ii) all points which are K-nearest neighbors with it. (Two different criteria, $K$ and $\epsilon$ are tuning parameters)

According to this rule, a weighted neighborhood graph $G = G(V,E)$ can be built. The set of vertices (data points in space $\mathcal{M}_2$): $V = \{\vec{x}_1, \dots , \vec{x}_N\}$ are the input data points, and the set of edges $E = \{e_{ij}\}$ indicate neighborhood relationships between the points. $e_{ij} = d_{ij}$ if (i) $||\vec{x}_i - \vec{x}_j||_2 \leq \epsilon$; OR (ii) $\vec{x}_j$ is one of the K-nearest neighbors of $\vec{x}_i$, otherwise $e_{ij} = \infty$. Sometimes, the tuning of $\epsilon$ (or $K$) is quite decisive in the output of ISOMAP, we will explain this later with a simulation example.


<!-- Fix tuning parameters $k >0$ or $\epsilon >0$.  Calculate all pairwise Euclidean distance $d_{ij} = \|\vec{x}_i - \vec{x}_j \|.$ Define a (directed) neighborhood graph $G$ by connecting nodes $i$ and $j$  -->

<!--     i) {\bf Using $k$}, connect $\vec{x}_i$ to $\vec{x}_j$ if $\vec{x}_j$ is one of the $k$ nearest points to $\vec{x}_i$ as measured by $d_{ij}$.  -->

<!--     ii) {\bf Using $\epsilon$}, connect $\vec{x}_i$ and \vec{x}_j$ if $d_ij < \epsilon.$ -->
    
    
**2) Compute graph distances**

In this step, we want to estimate the unknown true geodesic distances $\{d^{\mathcal{M}}_{ij}\}$ between all pairs of points with the help of the neighborhood graph $G$ we have just built. We use the graph distances $\{d^{\mathcal{G}}_{ij}\}$--- the shortest distances between all pairs of points in the graph $G$ to estimate $\{d^{\mathcal{M}}_{ij}\}$. For $\vec{x}_i$ and $\vec{x}_j$ that are not connected to each other, we try to find the shortest path that goes along the connected points on the graph. Following this particular sequence of neighbor-to-neighbor links, the sum of all the link weights along the path is defined as $\{d^{\mathcal{G}}_{ij}\}$. In other words, we use a number of short Euclidean distances (representing the local structure of the manifold) to approximate the geodesic distance $\{d^{\mathcal{M}}_{ij}\}$. 

This path finding step is usually done by Floyd-Warshall algorithm, which iteratively tries all transit points $k$ and find those that $\tilde{d}_{ik} + \tilde{d}_{kj} < \tilde{d}_{ij}$, and updates $\tilde{d}_{ij} = \tilde{d}_{ik} + \tilde{d}_{kj}$ for all possible combination of $i,j$. The algorithm works best in dense neighboring graph scenario, with a computational complexity of $O(n^3)$. 

The theoretical guarantee of this graph distance computation method is given by Bernstein et, al. one year after they first proposed ISOMAP in their previous paper. They show that asymptotically (as $n \rightarrow \infty$), the estimate $d^{\mathbb{G}}$ converges to $d^{\mathbb{M}}$ as long as the data points are sampled from a probability distribution that is supported by the entire manifold, and the manifold itself is flat.

The distance matrix $\Delta$ can be expressed as:
$$\Delta_{ij} = d^{\mathcal{G}}_{ij}$$

**Simulation Example**

Here we provide a randomly generated Neighborhood Graph for six data points, it uses the K-nearest neighbor criteria (can easily tell this since the matrix is not symmetric, $K=2$)

```{r}
# Define the matrix
matrix <- matrix(c(
  0,   3,  4,   Inf, Inf, Inf,
  7,   0,  Inf, 2,   Inf, Inf,
  6,   Inf,0,   Inf, 7,   Inf,
  Inf, 5,  Inf, 0,   Inf, 10,
  Inf, Inf,8,   Inf, 0,   13,
  Inf, Inf,Inf, 9,   14,  0
), byrow = TRUE, nrow = 6)
print(matrix)
```

Shown below is the implementation of Floyd-Warshall algorithm in R. As you can see from the three for loops, its computation complexity is $O(n^3)$.
```{r}
# Adjusting the matrix to set d_ij and d_ji to the smaller value
n <- dim(matrix)[1]

for (i in 1:n) {
  for (j in 1:n) {
    if (i != j && is.finite(matrix[i, j]) && is.finite(matrix[j, i])) {
      min_val <- min(matrix[i, j], matrix[j, i])
      matrix[i, j] <- min_val
      matrix[j, i] <- min_val
    }
  }
}

# Floyd-Warshall Algorithm
floyd_warshall <- function(mat) {
  n <- dim(mat)[1]
  dist <- mat
  
  for (k in 1:n) {
    for (i in 1:n) {
      for (j in 1:n) {
        dist[i, j] <- min(dist[i, j], dist[i, k] + dist[k, j])
      }
    }
  }
  
  return(dist)
}

# Get the result
result <- floyd_warshall(matrix)

# Print the result
print(result)
```

**3) Applying MDS to $\Delta$**

As mentioned before, ISOMAP can be viewed as the application of classical MDS in non-linear case. As a result, the reconstruction of $\{\vec{z}_i\}$ in the $k$ dimensional feature space $\mathcal{M}_1$ follows similar steps as that of classical MDS. The main goal is to preserve the geodesic distance of the manifold in $\mathcal{M}_2$ as much as possible. 

Without any additional information, there are infinite $\{\vec{z}_i\}$ that can be viewed as the optimal solution. For some invertible function $\Phi:\mathbb{R}^k\to\mathbb{R}^k$, a new manifold mapping $\Psi \circ \Phi^{-1}$ can be constructed. $\vec{x}_i = \Psi \circ \Phi^{-1} (\Phi(\vec{z}_i))$, which proofs that $\{\Phi(\vec{z}_i)\}$ is equivalent to $\{\vec{z}_i\}$ when it comes to the reconstruction of the lower dimensional feature space.

Without loss of generality, we assume that $\{\vec{z}_i\}$ are actually centered. So the distance matrix of $\{\vec{z}_i\}$ can be expressed as $B=Z^T Z$, so that $B_{ii}=||z_i||^2_2$ and $B_{ij}={z_i}^T z_j$.

The embedding vectors $\{\hat{z}_{i}\}$ (estimate of points in lower dimensional feature space $\mathcal{M}_1$) are chosen in order to minimize the objective function:

$$(\sum ||\vec{z}_i - \vec{z}_j||_2 - \Delta_{ij})^2$$

Following the same procedure explained in classical MDS chapter, we can compute each entry of $B$:
$$B_{ij}= -\frac{1}{2} \Delta^2_{ij} + \frac{1}{d} \sum^{d}_{i=1} \Delta^2_{ij} + \frac{1}{d} \sum^{d}_{j=1} \Delta^2_{ij} - \frac{1}{2d^2} \sum^{d}_{i=1} \sum^{d}_{j=1} \Delta^2_{ij}$$

To express it in matrix form, it is actually, $B = - \frac{1}{2} H \Delta H$, where $H = I_n - \frac{1}{n} \mathbb{1} \mathbb{1}^T$.

The next step is just a PCA problem. Implement eigen decomposition on matrix B, $B=U \Lambda U^T= (\Lambda^{1/2} U)^T (\Lambda^{1/2} U)$, then arrange the singular value in descending order, find the first $k^{\prime}$ ones. We acquire $\Lambda_{k^{\prime}}$ and $U_{k^{\prime}}$. 
$$\{\hat{z}_1, \hat{z}_2, \dots, \hat{z}_N\} = \Lambda_{k^{\prime}} U_{k^{\prime}}$$

Since we don't know the dimension of the underlying feature space, here $k^{\prime}$ is a tuning parameter. Usually, we use a scree plot ($k^{\prime}$ against the sum of the omitted eigenvalues ) and find the elbow point.


### Limitations of ISOMAP

Though ISOMAP is a powerful manifold learning method that works well under most circumstances. It still has some limitations in certain scenarios.

1) If the noises $\{\epsilon_i\}$ is not negligible, then ISOMAP may fail to identify the manifold. Also, ISOMAP is quite sensitive to the tuning parameters. To alleviate the negative impact, it's highly suggested to start with a relatively small $\epsilon$ or $K$, and increase them gradually. Here we use the Swiss Roll Example to help explain this point. 

```{r}
# generate Swiss roll-shaped data
S <- rep(0,2000)
Swiss <- matrix(NA, nrow = 2000, ncol = 3)
for( n in 1:2000){
    s <- runif(1, min = 3*pi/2, max = 9*pi/2)
    t <- runif(1, min = 0,      max = 15)
    S[n] <- s
    Swiss[n, ] <- c( s*cos(s), t, s*sin(s) )
}

par(mfrow = c(2,2))
# K = 20
scatterplot3d(Swiss, color = myColorRamp(c("red","purple","blue","green","yellow"), S ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]))
plot(embed(Swiss, "Isomap", .mute = c("message", "output"), ndim =2, knn = 20)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 20",
     col = myColorRamp(c("red","purple","blue","green","yellow"), S)  
     )
# K = 50
scatterplot3d(Swiss, color = myColorRamp(c("red","purple","blue","green","yellow"), S ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]))
plot(embed(Swiss, "Isomap", .mute = c("message", "output"), ndim =2, knn = 50)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 50",
     col = myColorRamp(c("red","purple","blue","green","yellow"), S)  
     )
```

Obviously, ISOMAP performs well when $K$ is small. However, as $K$ increases, the algorithm no longer recovers the lower dimensional feature space. Because the distance between nearby arms is not large, some points from different arms are considered as "close". The pairwise geodesic distances between these points is approximated by their Euclidean distance, which is not correct, of course. 

2) If the data points in some parts of the manifold is sparse, chances are that the learnt manifold structure will be ruined. The example below best illustrates this point.

We manually create a sparse region for the Swiss Roll, as shown below. From our observation, this sparse region does not affect the whole manifold that much. We suppose it is still a Swiss Roll.

```{r}
S <- rep(0,2000)
Swiss <- matrix(NA, nrow = 2000, ncol = 3)
for( n in 1:2000){
    s <- runif(1, min = 3*pi/2, max = 9*pi/2)
    t <- runif(1, min = 0,      max = 8)
    S[n] <- s
    Swiss[n, ] <- c( s*cos(s), t, s*sin(s) )
}

# Manually create a sparse region
mask <- Swiss[,1] > 5 & Swiss[,1] < 10 & Swiss[,2] > 5 & Swiss[,2] < 10
Swiss_sparse <- Swiss[!mask, ]
S <- S[!mask]

scatterplot3d(Swiss_sparse, color = myColorRamp(c("red","purple","blue","green","yellow"), S), main = "Swiss Roll with Sparse Region",
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]))
```

However, for ISOMAP, it gets into trouble recovering the lower dimensional feature space, despite using different tunning parameters.

```{r}
par(mfrow = c(2,2))
# K = 10
plot(embed(Swiss_sparse, "Isomap", .mute = c("message", "output"), ndim =2, knn = 10)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 10",
    col = myColorRamp(c("red","purple","blue","green","yellow"), S))
# k = 20
plot(embed(Swiss_sparse, "Isomap", .mute = c("message", "output"), ndim =2, knn = 20)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 20",
    col = myColorRamp(c("red","purple","blue","green","yellow"), S))
# K = 50
plot(embed(Swiss_sparse, "Isomap", .mute = c("message", "output"), ndim =2, knn = 50)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 50",
    col = myColorRamp(c("red","purple","blue","green","yellow"), S))
# K = 100
plot(embed(Swiss_sparse, "Isomap", .mute = c("message", "output"), ndim =2, knn = 100)@data@data,
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K = 100",
    col = myColorRamp(c("red","purple","blue","green","yellow"), S))
```


3) One of the two major assumptions of ISOMAP is the convexity of the manifold, that is to say, if the manifold contains many holes and concave margins, then the result of ISOMAP will probably be not ideal. 

Here we use the example of a folded washer manifold to illustrate this point. It is obvious that the folded washer is concave in its four ends. No matter how we tune the parameter $K$, ISOMAP always fails to recover the lower dimensional feature.

```{r}

# generate washer in 2d 
N <- 1e5
washer <- matrix(NA, nrow =N, ncol = 2)
for (n in 1:N){
  r <- runif(1,min=5,max = 10); 
  a = runif(1,min = 0, max = 2*pi); 
  washer[n,] <- r*c(cos(a),sin(a)) + c(20,0)
  }

# generate folded washer in 3D
washer3 <- cbind(washer, washer[,2]^2)

# generate rolled washer in 3D
washer.swiss <- washer3
for (n in 1:dim(washer3)[1] ){
  washer.swiss[n,] <- c( washer[n,1]*cos(washer[n,1]), washer[n,2], washer[n,1]*sin(washer[n,1] )) 
}
```

```{r}
N <- 2000
par(mfrow = c(2,2))
# K = 10
scatterplot3d(washer3[1:N,], color = myColorRamp(c("red","purple","blue","green","yellow"), washer[1:N,1] ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]))
plot(embed(washer3[1:N,], "Isomap", .mute = c("message", "output"), ndim =2, knn = 10)@data@data %*% matrix(c(0,1,1,0),nrow = 2),
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K=10",
     col = myColorRamp(c("red","purple","blue","green","yellow"), washer[,1])  )

# K = 20
plot(embed(washer3[1:N,], "Isomap", .mute = c("message", "output"), ndim =2, knn = 20)@data@data %*% matrix(c(0,1,1,0),nrow = 2),
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K=20",
     col = myColorRamp(c("red","purple","blue","green","yellow"), washer[,1])  )

# K = 50
plot(embed(washer3[1:N,], "Isomap", .mute = c("message", "output"), ndim =2, knn = 50)@data@data %*% matrix(c(0,1,1,0),nrow = 2),
     xlab = "1st Dimension", ylab = "2nd Dimension", main = "K=50",
     col = myColorRamp(c("red","purple","blue","green","yellow"), washer[,1])  )
```




<!-- 3) \underline{Embedding via MDS:}  The distances found in step 2 provide a distance matrix in which each entry approximates the manifold distance.  Under the assumption that the unknown manifold map is an isometry, we can seek a lower-dimensioal collection of points which have these pairwise distance.  By default, we use classical scaling to find such a configuration.  As discussed in Chapter \@ref(sec-mds), we have a f -->

<!--     - For visualization, we may typically choose a dimension (usually one, two, or three) and find the corresponding classical MDS solution. -->

<!--     - Alternatively, we use the eigenvalues of the inner product matrix based on the graph distances to choose a suitable cutoff for the appropriate  -->

    
    
    
    




<!--chapter:end:ISOMAP.Rmd-->

## Locally Linear Embeddings (LLEs)


<!--chapter:end:LLE.Rmd-->

## Nonnegative Matrix Factorization

We will continue with the usual setting focusing on an $N\times d$ data matrix ${\bf X}$. However, we will consider the additional assumption that each entry of the data matrix is non-negative which is a natural feature of many experimental data sets.

As before, our goal is to find a low-rank matrix $\hat{\bf X}$ which is as close to possible to ${\bf X}$ as possible.  How do we measure closness?  Here are a few common choices.

- Frobenius norm $\|{\bf X}-\hat{\bf X}\|_F.$

- Divergence $D({\bf X} \| \hat{\bf X}) = \sum_{i=1}^N\sum_{j=1}^d \left[{\bf X}_{ij} \log \frac{{\bf X}_{ij}}{\hat{\bf X}_{ij}} + \hat{\bf X}_{ij} - {\bf X}_{ij}\right]$

- IS Divergence

Without any restrictions on $\hat{\bf X}$ our previous analysis using SVD provides the answer when we consider the Frobenius norm of the $ell_2$ norm of the difference between ${\bf X}$ and $\hat{\bf X}.$

<!--chapter:end:NMF.Rmd-->

## Principal Component Analysis {#sec-pca}
 
```{r, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library('MASS')
library('plotly')
write_matex <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  writeLines(c(begin, X, end))
}
``` 

### Derivation 1: Iterative Projections

We begin with a data matrix $${\bf X} = \begin{bmatrix} \vec{x}_1^T\\ \vdots \\\vec{x}_N^T\end{bmatrix} \in\mathbb{R}^{N\times d}.$$  Let's begin with an example of dimension reduction where we'll seek to replace each vector $\vec{x}_1,\dots,\vec{x}_N$ with corresponding scalars $y_1,\dots,y_N$ which preserve as much of the variability between these vectors as possible.  To formalize this idea, let's introduce a few assumptions.

First, we'll assume the data $\vec{x}_1,\dots,\vec{x}_N$ are centered.  This is not a requirement, but it will simplify the analysis later.  We'll discuss how to account for this centering step later, but for now assume $\bar{x} = \vec{0}$ so that ${\bf HX} = {\bf X}$.  More importantly, let's assume that each $y_i$ is derived in the same way. Specifically, let $y_i = \vec{x}_i^T \vec{w}$ for some common vector $\vec{w}$.  Thus, we can view each one-dimensional representation as a dot product of the corresponding observed vector with the same vector $\vec{w}.$  We can compactly write this expression as $$\vec{y} = \begin{bmatrix}y_1\\ \vdots \\ y_n \end{bmatrix}=\begin{bmatrix}\vec{x}_1^T \vec{w} \\ \vdots \\ \vec{x}_N^T \vec{w}\end{bmatrix} = {\bf X} \vec{w}.$$


How do we choose $\vec{w}$?  We would like differences in the scalars $y_1,\dots,y_N$ to reflect differences in the vectors $\vec{x}_1,\dots,\vec{x}_N$ so having $y_1,\dots,y_N$ spread out is a natural goal. Thus, if $\vec{x}_i$ and $\vec{x}_j$ are far apart then so will $y_i$ and $y_j$. To do this, we'll try to maximize the sample variance of the $y$'s. The sample variance $$\frac{1}{N} \sum_{i=1}^N (y_i - \bar{y})^2 = \frac{1}{N}\sum_{i=1}^N(\vec{x}_i^T \vec{w} - \bar{y})^2$$ 
will depend on our choice of $\vec{w}$.  In the previous expression,
$$\bar{y} = \frac{1}{N} y_i = \frac{1}{N}\sum_{i=1}^N \vec{x}_i^T \vec{w} = \frac{1}{N}\vec{1}^T{\bf X}\vec{w}$$
is the sample mean of $y_1,\dots,y_N.$  Importantly, since we have assumed that $\vec{x}_1,\dots,\vec{x}_N$ are centered, it follows that $\bar{y}=0$ and the sample variance of $y_1,\dots,y_N$ simplifies to $$\frac{1}{N}\sum_{i=1}^N(\vec{x}_i^T \vec{w})^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 = \frac{1}{N} \|y\|^2 = \frac{1}{N}\vec{y}^T\vec{y}.$$

We can write the above expression more compactly. Using the identity $\vec{y} = {\bf X}\vec{w}$, we want to choose $\vec{w}$ to maximize $$\frac{1}{N}\vec{y}^T\vec{y} = \frac{1}{N}({\bf X}\vec{w})^T{\bf X}\vec{w} = \frac{1}{N}\vec{w}^T{\bf X}^T{\bf X}\vec{w} = \vec{w}^T\left(\frac{{\bf X}^T{\bf X}}{N}\right)\vec{w}.$$  Since we have assumed that ${\bf X}$ is centered it follows that ${\bf X}^T{\bf X}/N$ is the sample covariance matrix $\hat{\bf \Sigma}$! Thus, we want to make $\vec{w}^T\hat{\bf \Sigma} \vec{w}$ as large as possible.

Naturally, we could increase the entries in $\vec{w}$ and increase the above expression without bound. To make the maximization problem well posed, we will restrict $\vec{w}$ to be unit-length under the Euclidean norm so that $\|\vec{w}\|=1.$  We now have a constrained optimization problem which gives rise to the first **principal component loading**.

::: {.definition #first-pca name="First PCA Loading and Scores"}
The first **principal component loading** is the vector $\vec{w}_1$ solving the constrained optimization problem
\begin{equation}
\begin{split}
\text{Maximize  } &\vec{w}^T \hat{\bf \Sigma}\vec{w} \\
\text{subject to constraint } &\|\vec{w}\|=1.
\end{split}
\end{equation}
The first **principal component scores** are the scalars $y_i = \vec{x}_i^T\vec{w}_1$ for $i=1,\dots, N$.
:::

To find the first PCA loading we can make use of Lagrange multipliers (see exercises) to show that $\vec{w}_1$ must also satisfy the equation $$\hat{\bf \Sigma}\vec{w}_1 = \lambda \vec{w}_1$$ where $\lambda$ is the Lagrange multiplier. From this expression, we can conclude that the first principal component loading is the unit length eigenvector associated with the largest eigenvalue of the sample covariance matrix $\hat{\bf \Sigma}$ and that the Lagrange multiplier $\lambda$ is the largest eigenvalue of $\hat{\bf \Sigma}$. In this case, we refer to $\lambda$ as the first **principal component variance**. 

#### Geometric Interpretation of $\vec{w}_1$

Since $\|\vec{w}_1\| = 1$ we may interpret this vector as specifying a direction in $\mathbb{R}^d$.  Additionally, we can decompose each of our samples into two pieces: one pointing in the direction specified by $\vec{w}_1$ and a second portion perpendicular to this direction.  Thus, we may write
$$\vec{x}_i = \underbrace{\vec{w}_1 \vec{x}_i^T\vec{w}_1}_{parallel} + \underbrace{(\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1)}_{perpendicular}.$$
By the Pythagorean theorem,
\begin{align*}
\|\vec{x}_i\|^2 &= \| \vec{w}_1 \vec{x}_i^T\vec{w}_1 \|^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2 \\
&= (\vec{w}_1^T\vec{x}_i)^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2 \\
&= y_i^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2
\end{align*}
for $i=1,\dots,N$. Averaging over all of samples gives the expression
$$\frac{1}{N}\sum_{i=1}^N\|\vec{x}_i\|^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 +\frac{1}{N}\sum_{i=1}^N \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2.$$
The left-hand side of the above expression is fixed for a given set of data, whereas the first term on the right side is exactly what we sought to maximize when finding the first principal component loading.  This quantity is the average squared length of the projection of each sample onto the direction $\vec{w}_1$. As such, we can view the first principal component loading as the direction in which $\vec{x}_1,\dots,\vec{x}_N$ most greatly varies.  Let's turn to an example in $\mathbb{R}^3$ to view this.


::: {.example #pca-demo-1 name="Computing the First PCA Loading and Scores"}
Below, we show a scatterplot of $N=1000$ random points in $\mathbb{R}^3.$
```{r, echo = FALSE}
N <- 1000
set.seed(1) # fix random number generator for reproducibility
mu = c(5,5,5); # set the mean
Q <- svd(matrix(c(1,2,3,4,5,6,7,8,9), ncol = 3))$u # generate an orthonormal matrix
Sigma <- Q %*% diag(c(25 , 9, 1)) %*% t(Q) # calculate Sigma = Q D Q' by eigendecomposition
# N <- 100; # number of samples to draw
data <- mvrnorm(N,mu,Sigma) # draw samples from normal, each sample saved in a row
plot_ly(data.frame( x = data[,1], y= data[,2], z = data[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-2, 12)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-2, 12)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-2, 12)))
         )

```
Notice the oblong shape of the cloud of points. Rotating this image, it is clear that the data varies more in certain directions than in others. 
We begin by centering the data
```{r}
data <- scale(data, center = TRUE, scale = FALSE) # subtracts mean from each column
```
which appears the same as the previous figure except centered around the origin.
```{r, echo = FALSE}
plot_ly(data.frame( x = data[,1], y= data[,2], z = data[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )
Sigmahat <- (t(data) %*% data)/N # calculate the sample covariance matrix, recall the data has been centered 
Sigmahat.eigen <- eigen(Sigmahat) # calculate the eigen decomposition of Sigmahat
y <- data %*% Sigmahat.eigen$vectors # calculate the scores saved in rows
```
To find the principal component scores and loading, let us calculate  of the sample covariance matrix. We can use the largest eigenvalue to find the first PCA variance. Its associated eigenvector (unit length) will be the first loading. The sample covariance 
$$
\hat{\Sigma} =  
```{r, echo = FALSE, results = 'asis'} 
write_matex(round(Sigmahat,2))
``` 
$$
has largest eigenvalue $\lambda = `r round(Sigmahat.eigen$values[1],2)`$ and associated eigenvector $($`r round(Sigmahat.eigen$vectors[,1],2)`$)^T$.  We can conclude by plotting the first PCA scores below
```{r}
plot(y[,1], rep(0,N), ylab = '', xlab = 'First PCA scores', main = '')
```
<!-- & =  -->
<!-- ```{r, echo = FALSE, results = 'asis'} -->
<!-- write_matex(round(Sigmahat.eigen$vectors,2)) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, results = 'asis'} -->
<!-- write_matex(diag(round(Sigmahat.eigen$values,2))) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, results = 'asis'} -->
<!-- write_matex(t(round(Sigmahat.eigen$vectors,2))) -->
<!-- ``` -->

<!-- so the principal component loadings are the columns of the lower matrix on the left.  The variance of the principal components are the entries of the diagonal matrix above. -->
:::




#### Additional Principal Components

The first PCA loading provides information about the direction in which are data most greatly vary, but it is quite possible that there are still other directions wherein our data still exhibits a lot of variability.  In fact, the notion of a *first* principal component loading, scores, and variance suggests the existence of a second, third, etc. collection of these quantities. To explore these quantities, let's proceed as follows

For each datum, we can remove its component in the direction of $\vec{w}_1$, and focus on the projection onto the orthogonal complement of $\vec{w}_1$. Let $$\vec{x}_i^{(1)} = \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 = \vec{x}_i - \vec{w}_1 y_i$$
denote the portion of $\vec{x}_i$ which is orthogonal to $\vec{w}_1$.  Here, the superscript $^{(1)}$ indicates we have removed portion of the vector in the direction of the first loading.

We can organize the orthogonal components into a new data matrix $${\bf X}^{(1)} = \begin{bmatrix} \left(\vec{x}_1^{(1)}\right)^T  \\ \vdots  \\ \left(\vec{x}_N^{(1)}\right)^T  \end{bmatrix} = 
\begin{bmatrix} 
\vec{x}_1^T - \vec{x}_1^T\vec{w}_1\vec{w}_1^T \\ 
\vdots \\ 
\vec{x}_N^T - \vec{x}_N^T\vec{w}_1\vec{w}_1^T \end{bmatrix} = {\bf X} - {\bf X}\vec{w}_1\vec{w}_1^T.$$
Now let's apply PCA to the updated data matrix ${\bf X}^{(1)}$ from which we get the second principal component loading, denoted $\vec{w}_2$, the second principal component scores, and the second principal component variance.  One can show that the data matrix ${\bf X}^{(1)}$ is centered so that its sample covariance matrix is $\hat{\bf \Sigma}^{(1)} = \frac{1}{N}({\bf X}^{(1)})^T{\bf X}^{(1)}.$ Thus, the second PCA loading, $\vec{w}_2$, is a unit eigenvector associated with the largest eigenvalue of ${\bf \Sigma}^{(1)}$. This eigenvalue is the 2nd PCA variance and the 2nd PCA score of $\vec{x}_i$ is given by the inner product of $\vec{x}_i^{(1)}$ with $\vec{w}_2$.

Here is one crucial observation.  The vector $\vec{w}_2$ gives the direction of greatest variability of the vectors $\vec{x}_1^{(1)},\dots,\vec{x}_N^{(1)}.$ For each of these vectors we have removed the component in the direction of $\vec{w}_1$. Thus, $\vec{x}_1^{(1)},\dots,\vec{x}_N^{(1)}$ do not vary at all in the $\vec{w}_1$ direction.  What can we say about $\vec{w}_2$? Naturally, it must be perpendicular to $\vec{w}_1$!


We need not stop at the second PCA loading, scores, and variance. We could remove components in the direction of $\vec{w}_2$ and apply PCA to the vectors 
\begin{align*}
\vec{x}_i^{(2)} &= \vec{x}_i^{(1)} - \vec{w}_2 (\vec{x}_i^{(1)})^T\vec{w}_2\\
&= \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 - \vec{w}_2(\vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1)^T\vec{w}_2\\
&= \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 - \vec{w}_2\vec{x}_i^T\vec{w}_2 + \vec{w}_2\vec{w_1}^T\vec{x}_i\underbrace{\vec{w}_1^T\vec{w}_2}_{=0}
\end{align*}
to obtain a third loading, variance, and set of scores.   We can continue repeating this argument $d$ times for our $d$-dimensional data until we arrive at a set of $d$ unit vectors $\vec{w}_1,\dots,\vec{w}_d$ which are the $d$ PCA loadings. 

To review, we then have $d$ PCA loadings $\vec{w}_1,\dots,\vec{w}_d$ each with an associated PCA variance $\lambda_1,\dots,\lambda_d$. For each sample, we also have an associated set of PCA scores $\vec{x}_i^T\vec{w}_1,\dots,\vec{x}_i^T\vec{w}_d$ which we can organize into a large matrix $${\bf Y} = \begin{bmatrix} \vec{x}_1^T\vec{w}_1 & \dots & \vec{x}_1^T\vec{w} \\ 
\vdots & \vdots & \vdots \\
\vec{x}_N^T\vec{w} & \dots & \vec{x}_N^T\vec{w}_d \end{bmatrix} = {\bf X}\begin{bmatrix} \vec{w}_1 \,| \dots \,|\,\vec{w}_d\end{bmatrix}.$$

We formalize this idea in the following Lemma.

::: {.lemma #proj-cov name="Eigenvalues of Covariance Matrix of Projected Data"}
Suppose vectors $\vec{x}_1,\dots,\vec{x}_N\in\mathbb{R}^d$ are centered and have sample covariance matrix $\hat{\bf \Sigma}$. Let $\lambda_1 \ge \dots\ge\lambda_d \ge 0$ denote the eigenvalues of $\hat{\bf \Sigma}$ with associated eigenvectors $\vec{w}_1,\dots,\vec{w}_d.$ Not let $\vec{x}^{(k)}_i = \vec{x}_i - \sum_{j=1}^k \vec{w}_j\vec{x}_i^T\vec{w}_j$ denote the portion of vector $\vec{x}_i$ which is orthogonal to each of the first $k$ PCA loadings.  If $\hat{\bf \Sigma}^{(k)}$ denotes the sample covariance matrix of these vectors it follows that $\hat{\bf \Sigma}^{(k)}$ has eigenvalues $\underbrace{0,\dots,0}_{k}$, $\lambda_{k+1}\ge \dots \ge \lambda_d\ge 0$ with associated eigenvectors $$\underbrace{\vec{w}_1,\dots,\vec{w}_k,}_{\text{each with eigenvalue 0}} \vec{w}_{k+1},\dots,\vec{w}_d.$$
:::

Verifying this Lemma is left as an exercise.


For each loading, we have a corresponding set of PCA scores and PCA variances.  Importantly, since are data are $d$-dimensional and our loadings are $d$-mutually orthogonal unit vectors, they define a new basis in $\mathbb{R}^d$.

Let's continue our example above to see this process in the case where $d=3$.

::: {.example #pca-demo-all name="Computing the Remaining PCA Loadings and Scores"}
```{r, echo = FALSE}
data2 <- data - y[,1] %*% t(Sigmahat.eigen$vectors[,1]) # remove first PC
plot_ly(data.frame( x = data2[,1], y= data2[,2], z = data2[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )
```

We can  continue this process by also removing, from each vector, its component in the direction of $\vec{w}_2.$
```{r, echo = FALSE}
data3 <- data2 - y[,2] %*% t(Sigmahat.eigen$vectors[,2]) # remove first PC
plot_ly(data.frame( x = data3[,1], y= data3[,2], z = data3[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )
```


Finally, we can plot the data in the basis defined by the loadings.  This is equivalent to a scatterplot of the scores.

```{r}
plot_ly(data.frame( x = y[,1], y= y[,2], z = y[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "y<sub>1</sub>"),
                       yaxis = list(title = "y<sub>2</sub>"),
                       zaxis = list(title = "y<sub>3</sub>")),
         title = "Principal Component Scores"
         )
```
In this basis, the data forms an ellipsoid with principal axis directed along the $y_1,\,y_2, \text{ and }y_3$ axes.  This is a result of the uncorrelated nature of principal component scores.
:::







### Derivation 2: Optimal Linear Subspace

<!--chapter:end:PCA.Rmd-->

## Singular Value Decomposition
 
 
 yay

<!--chapter:end:SVD.Rmd-->

