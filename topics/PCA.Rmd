## Principal Component Analysis {#sec-pca}
 
```{r, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library('MASS')
library('plotly')
write_matex <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  writeLines(c(begin, X, end))
}
``` 

Principal component analysis (PCA) is arguably the first method for dimension reduction, which dates back to papers by some of the earliest contributors to statistical theory including Karl Pearson and Harold Hotelling [@pca_pearson; @pca_hotelling].  Pearson's original development of PCA borrowed ideas from mechanics which provides a clear geometric/physical interpretation of the resulting PCA **loadings**, **variances**, and **scores**, which we will define later. This interpretability and an implementation that uses scalable linear algebra methods -- allowing PCA to be conducted on massive datasets -- is one of the reasons PCA is still used prolifically to this day.  In fact, many more modern and complex methods still rely on PCA as an internal step in their algorithmic structure.  

There are number of different but equivalent derivations of PCA including the minimization of least squares error, covariance decompositions, and low rank approximations.  We'll revisit these ideas later, but first, let's discuss PCA through a geometrically motivated lens via a method called iterative projections.

### Derivation using Iterative Projections

We begin with a data matrix $${\bf X} = \begin{bmatrix} \vec{x}_1^T\\ \vdots \\\vec{x}_N^T\end{bmatrix} \in\mathbb{R}^{N\times d}.$$  Let's begin with an example of dimension reduction where we'll seek to replace each vector $\vec{x}_1,\dots,\vec{x}_N$ with corresponding scalars $y_1,\dots,y_N$ which preserve as much of the variability between these vectors as possible.  To formalize this idea, let's introduce a few assumptions.

First, we'll assume the data $\vec{x}_1,\dots,\vec{x}_N$ are centered.  To be fair, assumption may be too strong a word.  Rather, it's a first step in preprocessing that will simplify the analysis later.  We'll discuss how to account for this centering step later, but for now assume $\bar{x} = \vec{0}$ so that ${\bf HX} = {\bf X}$.  More importantly, let's assume that each $y_i$ is constructed in the same way. Specifically, let $y_i = \vec{x}_i^T \vec{w}$ for some common vector $\vec{w}$.  Thus, we can view each one-dimensional representation as a dot product of the corresponding observed vector with the same vector $\vec{w}.$  We can compactly write this expression as $$\vec{y} = \begin{bmatrix}y_1\\ \vdots \\ y_n \end{bmatrix}=\begin{bmatrix}\vec{x}_1^T \vec{w} \\ \vdots \\ \vec{x}_N^T \vec{w}\end{bmatrix} = {\bf X} \vec{w}.$$


How do we choose $\vec{w}$?  We would like differences in the scalars $y_1,\dots,y_N$ to reflect differences in the vectors $\vec{x}_1,\dots,\vec{x}_N$ so having $y_1,\dots,y_N$ spread out is a natural goal. Thus, if $\vec{x}_i$ and $\vec{x}_j$ are far apart then so will $y_i$ and $y_j$. To do this, we'll try to maximize the sample variance of the $y$'s. The sample variance $$\frac{1}{N} \sum_{i=1}^N (y_i - \bar{y})^2 = \frac{1}{N}\sum_{i=1}^N(\vec{x}_i^T \vec{w} - \bar{y})^2$$ 
will depend on our choice of $\vec{w}$.  In the previous expression,
$$\bar{y} = \frac{1}{N} y_i = \frac{1}{N}\sum_{i=1}^N \vec{x}_i^T \vec{w} = \frac{1}{N}\vec{1}^T{\bf X}\vec{w}$$
is the sample mean of $y_1,\dots,y_N.$  Importantly, since we have assumed that $\vec{x}_1,\dots,\vec{x}_N$ are centered, it follows that $\bar{y}=0$ and the sample variance of $y_1,\dots,y_N$ simplifies to $$\frac{1}{N}\sum_{i=1}^N(\vec{x}_i^T \vec{w})^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 = \frac{1}{N} \|y\|^2 = \frac{1}{N}\vec{y}^T\vec{y}.$$

We can write the above in terms of ${\bf X}$ and $\vec{w}$. Using the identity $\vec{y} = {\bf X}\vec{w}$, we want to choose $\vec{w}$ to maximize $$\frac{1}{N}\vec{y}^T\vec{y} = \frac{1}{N}({\bf X}\vec{w})^T{\bf X}\vec{w} = \frac{1}{N}\vec{w}^T{\bf X}^T{\bf X}\vec{w} = \vec{w}^T\left(\frac{{\bf X}^T{\bf X}}{N}\right)\vec{w}.$$  Since we have assumed that ${\bf X}$ is centered it follows that ${\bf X}^T{\bf X}/N$ is the sample covariance matrix $\hat{\bf \Sigma}$! Thus, we want to make $\vec{w}^T\hat{\bf \Sigma} \vec{w}$ as large as possible.

Naturally, we could increase the entries in $\vec{w}$ and increase the above expression without bound. To make the maximization problem well posed, we will restrict $\vec{w}$ to be unit-length under the Euclidean norm so that $\|\vec{w}\|=1.$  We now have a constrained optimization problem which gives rise to the first **principal component loading**.

::: {.definition #first-pca name="First PCA Loading and Scores"}
The first **principal component loading** is the vector $\vec{w}_1$ solving the constrained optimization problem
\begin{equation}
\begin{split}
\text{Maximize  } &\vec{w}^T \hat{\bf \Sigma}\vec{w} \\
\text{subject to constraint } &\|\vec{w}\|=1.
\end{split}
\end{equation}
The first **principal component scores** are the projections, $y_i = \vec{x}_i^T\vec{w}_1$ for $i=1,\dots, N$, of each sample onto the first loading.
:::

To find the first PCA loading we can make use of Lagrange multipliers (see exercises) to show that $\vec{w}_1$ must also satisfy the equation $$\hat{\bf \Sigma}\vec{w}_1 = \lambda \vec{w}_1$$ where $\lambda$ is the Lagrange multiplier. From this expression, we can conclude that the first principal component loading is the unit length eigenvector associated with the largest eigenvalue of the sample covariance matrix $\hat{\bf \Sigma}$ and that the Lagrange multiplier $\lambda$ is the largest eigenvalue of $\hat{\bf \Sigma}$. In this case, we refer to $\lambda$ as the first **principal component variance**. 

#### Geometric Interpretation of $\vec{w}_1$

Since $\|\vec{w}_1\| = 1$ we may interpret this vector as specifying a direction in $\mathbb{R}^d$.  Additionally, we can decompose each of our samples into two pieces: one pointing in the direction specified by $\vec{w}_1$ and a second portion perpendicular to this direction.  Thus, we may write
$$\vec{x}_i = \underbrace{\vec{w}_1 \vec{x}_i^T\vec{w}_1}_{parallel} + \underbrace{(\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1)}_{perpendicular}.$$
By the Pythagorean theorem,
\begin{align*}
\|\vec{x}_i\|^2 &= \| \vec{w}_1 \vec{x}_i^T\vec{w}_1 \|^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2 \\
&= (\vec{w}_1^T\vec{x}_i)^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2 \\
&= y_i^2 + \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2
\end{align*}
for $i=1,\dots,N$. Averaging over all of samples gives the expression
$$\frac{1}{N}\sum_{i=1}^N\|\vec{x}_i\|^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 +\frac{1}{N}\sum_{i=1}^N \|\vec{x}_i -\vec{w}_1 \vec{x}_i^T\vec{w}_1\|^2.$$
The left-hand side of the above expression is fixed for a given set of data, whereas the first term on the right side is exactly what we sought to maximize when finding the first principal component loading.  This quantity is the average squared length of the projection of each sample onto the direction $\vec{w}_1$. As such, we can view the first principal component loading as the direction in which $\vec{x}_1,\dots,\vec{x}_N$ most greatly varies.  Let's turn to an example in $\mathbb{R}^3$ to view this.


::: {.example #ex-pca-demo-1 name="Computing the First PCA Loading and Scores"}
Below, we show a scatterplot of $N=500$ points in $\mathbb{R}^3$ drawn randomly from a MVN. These data have been centered.
```{r, echo = FALSE}
set.seed(185)
N <- 500 # number of samples to draw
set.seed(1) # fix random number generator for reproducibility
mu = c(5,5,5); # set the mean
Q <- svd(matrix(c(1,2,3,4,5,6,7,8,9), ncol = 3))$u # generate an orthonormal matrix
Sigma <- Q %*% diag(c(25 , 9, 1)) %*% t(Q) # calculate Sigma = Q D Q' by its eigendecomposition
data <- mvrnorm(N,mu,Sigma) # draw samples from normal, each sample saved in a row
data <- scale(data, center = TRUE, scale = FALSE) # subtracts mean from each column
plot_ly(data.frame( x = data[,1], y= data[,2], z = data[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 2)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-15, 15)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-15, 15)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-15, 15)))
         )
Sigmahat <- (t(data) %*% data)/N # calculate the sample covariance matrix, recall the data has been centered 
```
Notice the oblong shape of the cloud of points. Rotating this image, it is clear that the data varies more in certain directions than in others. 

The sample covariance matrix of these data is 
$$
\hat{\Sigma} =  
```{r, echo = FALSE, results = 'asis'} 
write_matex(round(Sigmahat,2))
``` 
$$ 
We can use the eigendecomposition of this matrix to find the first PCA loading and variance.  Given the first loading we can also compute the first PCA scores.  A short snippet of code for this calculation is shown below.

```{r}
Sigmahat <- (t(data) %*% data)/N # calculate the sample covariance matrix, recall the data has been centered 
Sigmahat.eigen <- eigen(Sigmahat) # calculate the eigen decomposition of Sigmahat
y <- data %*% Sigmahat.eigen$vectors[,1] # calculate the scores 
```

The largest eigenvalue (first PC variance) is $\lambda = `r round(Sigmahat.eigen$values[1],2)`$ with associated eigenvector $(`r round(Sigmahat.eigen$vectors[,1],2)`)^T$ which is the first PC loading.  

We can visualize these results in a few different ways.  First, we can add the span of $\vec{w}_1$ (shown in red) to the scatterplot of the data. One can see that $\vec{w}_1$ is oriented along the direction where the data is most spread out.
```{r, echo = FALSE, fig.cap="Samples with Span of First Loading"}
plot_ly(data.frame(x = 20*Sigmahat.eigen$vectors[1,1]*c(-1,1),
                   y = 20*Sigmahat.eigen$vectors[2,1]*c(-1,1),
                   z = 20*Sigmahat.eigen$vectors[3,1]*c(-1,1)),
        x=~x, y=~y,z=~z,
        color = I("red"),
        name = "1st PC Loading") %>% add_lines() %>%
  add_markers(x = data[,1],y=data[,2],z=data[,3],
              marker = (list(size = 2)),
              color = I("blue"),
              name = "Data") %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-15, 15)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-15, 15)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-15, 15)))
         )
```

We could also generate a scatterplot of the scores, but we'll show these scores along the $\vec{w}_1$ axes so that they correspond to the projection of each sample onto span$\{\vec{w}_1\}$; equivalently, we are plotting the vectors $y_i \vec{w}_i$.  

```{r pca1, echo = FALSE, fig.width= 10, fig.cap="Decomposition of Samples into Components Parallel and Perpendicular to First Loading"}
data1 <- y %*% t(Sigmahat.eigen$vectors[,1])
fig1 <- plot_ly(data.frame(x = 20*Sigmahat.eigen$vectors[1,1]*c(-1,1),
                   y = 20*Sigmahat.eigen$vectors[2,1]*c(-1,1),
                   z = 20*Sigmahat.eigen$vectors[3,1]*c(-1,1)),
        x=~x, y=~y,z=~z,
        color = I("red"),
        scene = "scene1") %>% add_lines() %>% 
  add_markers(x = data1[,1], y=data1[,2], z=data1[,3],
              marker = (list(size = 2)),
              color = I("blue")) %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-15, 15)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-15, 15)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-15, 15)))
  )

fig2 <- plot_ly(data.frame(x = 20*Sigmahat.eigen$vectors[1,1]*c(-1,1),
                   y = 20*Sigmahat.eigen$vectors[2,1]*c(-1,1),
                   z = 20*Sigmahat.eigen$vectors[3,1]*c(-1,1)),
        x=~x, y=~y,z=~z,
        color = I("red"),
        scene = "scene2") %>% add_lines() %>% 
  add_markers(x = data[,1]-data1[,1], y=data[,2]-data1[,2], z=data[,3]-data1[,3],
              marker = (list(size = 2)),
              color = I("blue")) %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-15, 15)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-15, 15)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-15, 15)))
  )

scene = list(camera = list(eye = list(x = 2, y = 2, z = 2)))

annotations = list( 
  list( 
    x = 0.2,  
    y = 1.0,  
    text = "Parallel component",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ),  
  list( 
    x = 0.8,  
    y = 1,  
    text = "Perpendicular component",  
    xref = "paper",  
    yref = "paper",  
    xanchor = "center",  
    yanchor = "bottom",  
    showarrow = FALSE 
  ))

subplot(fig1,fig2, nrows = 1) %>% layout(scene = scene,
                                         annotations = annotations) 
```
:::




#### Additional Principal Components
```{r, echo = FALSE}
Sigmahat1 <- (t(data-data1) %*% (data-data1))/N
Sigmahat1.eigen <- eigen(Sigmahat1)
lam2 <- Sigmahat1.eigen$values[1]
w2 <- Sigmahat1.eigen$vectors[,1]
```
The first PCA loading provides information about the direction in which are data most greatly vary, but it is quite possible that there are still other directions wherein our data still exhibits a lot of variability.  In fact, the notion of a *first* principal component loading, scores, and variance suggests the existence of a second, third, etc. iteration of these quantities. To explore these quantities, let's proceed as follows

For each datum, we can remove its component in the direction of $\vec{w}_1$, and focus on the projection onto the orthogonal complement of $\vec{w}_1$. Let $$\vec{x}_i^{(1)} = \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 = \vec{x}_i - \vec{w}_1 y_i$$
denote the portion of $\vec{x}_i$ which is orthogonal to $\vec{w}_1$.  These points are shown on the right side of Fig \@{fig:pca1}. Here, the superscript $^{(1)}$ indicates we have removed the portion of each vector in the direction of the first loading.

We can organize the orthogonal components into a new data matrix $${\bf X}^{(1)} = \begin{bmatrix} \left(\vec{x}_1^{(1)}\right)^T  \\ \vdots  \\ \left(\vec{x}_N^{(1)}\right)^T  \end{bmatrix} = 
\begin{bmatrix} 
\vec{x}_1^T - \vec{x}_1^T\vec{w}_1\vec{w}_1^T \\ 
\vdots \\ 
\vec{x}_N^T - \vec{x}_N^T\vec{w}_1\vec{w}_1^T \end{bmatrix} = {\bf X} - {\bf X}\vec{w}_1\vec{w}_1^T.$$
Now let's apply PCA to the updated data matrix ${\bf X}^{(1)}$ from which we get the second principal component loading, denoted $\vec{w}_2$, the second principal component scores, and the second principal component variance.  One can show that the data matrix ${\bf X}^{(1)}$ is centered so that its sample covariance matrix is 
\begin{equation}
\hat{\bf \Sigma}^{(1)} = \frac{1}{N}({\bf X}^{(1)})^T{\bf X}^{(1)}.
\end{equation}
The 2nd PC variance is the largest eigenvalue of $\hat{\bf \Sigma}^{(1)}$ and its associated unit length eigenvector is the 2nd PC loading, denoted $\vec{w}_2$.  The second PC scores $\vec{w}_2^T\vec{x}_i^{(1)}$ for $i=1,\dots,N.$ 

Continuing \@ref(ex-pca-demo-1), we have 
$$
\hat{\bf \Sigma}^{(1)} = 
```{r, echo = FALSE, results = 'asis'} 
write_matex(round(Sigmahat1,2))
``` 
$$
which has largest eigenvalue `r round(lam2,2)` (2nd PC variance) and associated eigenvector $\vec{w}_2 = (`r round(w2,2)`)^T$ (2nd PC Loading). The second set of PC scores are given by $\vec{w}_2^T\vec{x}_i^{(1)}$ for $i=1,\dots, N.$

Here is one crucial observation.  The vector $\vec{w}_2$ gives the direction of greatest variability of the vectors $\vec{x}_1^{(1)},\dots,\vec{x}_N^{(1)}.$ For each of these vectors we have removed the component in the direction of $\vec{w}_1$. Thus, $\vec{x}_1^{(1)},\dots,\vec{x}_N^{(1)}$ do not vary at all in the $\vec{w}_1$ direction.  What can we say about $\vec{w}_2$? Naturally, it must be perpendicular to $\vec{w}_1$!  We can see this geometric relationship if we plot the vectors $\vec{x}^{(1)}_i$ in our previous example along with the span of the first and second loading.

```{r pca2, echo = FALSE, fig.cap="First and 2nd Loadings with Data after component along first loading has been removed"}
plot_ly(data.frame(x = 20*Sigmahat.eigen$vectors[1,1]*c(-1,1),
                   y = 20*Sigmahat.eigen$vectors[2,1]*c(-1,1),
                   z = 20*Sigmahat.eigen$vectors[3,1]*c(-1,1)),
        x=~x, y=~y,z=~z,
        color = I("red"),
        name = "1st Loading") %>% add_lines() %>% 
  add_lines(data = data.frame(x = 20*Sigmahat.eigen$vectors[1,2]*c(-1,1),
                   y = 20*Sigmahat.eigen$vectors[2,2]*c(-1,1),
                   z = 20*Sigmahat.eigen$vectors[3,2]*c(-1,1)),
        x=~x, y=~y,z=~z,
        color = I("black"),
        name = "2nd Loading") %>%
  add_markers(x = data[,1]-data1[,1], y=data[,2]-data1[,2], z=data[,3]-data1[,3],
              marker = (list(size = 2)),
              color = I("blue"),
              name = "Data w/o 1st PC") %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-15, 15)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-15, 15)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-15, 15)))
  )

```



We need not stop at the second PCA loading, scores, and variance. We could remove components in the direction of $\vec{w}_2$ and apply PCA to the vectors 
\begin{align*}
\vec{x}_i^{(2)} &= \vec{x}_i^{(1)} - \vec{w}_2 (\vec{x}_i^{(1)})^T\vec{w}_2\\
&= \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 - \vec{w}_2(\vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1)^T\vec{w}_2\\
&= \vec{x}_i - \vec{w}_1\vec{x}_i^T\vec{w}_1 - \vec{w}_2\vec{x}_i^T\vec{w}_2 + \vec{w}_2\vec{w_1}^T\vec{x}_i\underbrace{\vec{w}_1^T\vec{w}_2}_{=0}
\end{align*}
which gives rise to the centered data matrix 
\begin{equation}
{\bf X}^{(2)} = \begin{bmatrix}
\vec{x}_1^T- \vec{w}_1^T\vec{x}_1\vec{w}_1^T - \vec{w}_2^T\vec{x}_1\vec{w}_2^T \\ 
\vdots \\
\vec{x}_d^T- \vec{w}_1^T\vec{x}_d\vec{w}_1^T - \vec{w}_2^T\vec{x}_d\vec{w}_2^T
\end{bmatrix} = {\bf X} - {\bf X}\vec{w}_1\vec{w}_1^T - {\bf X}\vec{w}_2\vec{w}_2^T
\end{equation}
with corresponding covariance matrix
$$\hat{\Sigma}^{(2)} = \frac{1}{N}({\bf X}^{(2)})^T{\bf X}^{(2)}$$ from which we can obtain a third loading ($\vec{w}_3$), variance ($\lambda_3)$,  and set of scores $\vec{w}_3\vec{x}_i^{(2)}$ for $i=1,\dots,N$.  


We can continue repeating this argument $d$ times for our $d$-dimensional data until we arrive at a set of $d$ unit vectors $\vec{w}_1,\dots,\vec{w}_d$ which are the $d$ PCA loadings. Why do we stop after $d$?  The principal component loadings $\vec{w}_1,\dots,\vec{w}_d$ are all mutually orthogonal and unit length so they form an orthonormal basis for $\mathbb{R}^d$.  All of the variability in each sample can be expressed in terms of these $d$ basis vectors.

This iterative approach is admittedly, to intensive for most practical applications.  Fortunately, we do not need to follow the sequence of projections and then eigenvalue computations thanks to the following theorem.

::: {.theorem #pca-eig}
Suppose $\vec{w}_1,\dots,\vec{w}_d$ are the orthonormal eigenvectors of the sample covariance $\hat{\Sigma}=\frac{1}{N}{\bf X}^T{\bf X}$ with eigenvalues $\lambda_1\ge \dots\ge \lambda_d \ge 0$ respectively, i.e. $\hat{\Sigma}\vec{w}_j = \lambda_j\vec{w}_j$. Then $\vec{w}_1,\dots,\vec{w}_d$ are also eigenvectors or the matrix $\hat{\Sigma}^{(1)}=\frac{1}{N}({\bf X}^{(1)})^T{\bf X}^{(1)}$ with eigenvalues $0,\lambda_2,\dots,\lambda_d$ respectively.
:::

This result follows nicely from the geometric observation that the loadings are mutually orthogonal.  The second loading is then the eigenvector associated with the second largest eigenvalue of the original covariance matrix. Taking the orthogonality of the loadings further, we can get even more from the following corollary.

::: {.corollary #pca-all}
Suppose $\vec{w}_1,\dots,\vec{w}_d$ are the orthonormal eigenvectors of the sample covariance $\hat{\Sigma}=\frac{1}{N}{\bf X}^T{\bf X}$ with eigenvalues $\lambda_1\ge \dots\ge \lambda_d \ge 0$ respectively. Then for $k=2,\dots,d-1$, the vectors$\vec{w}_1,\dots,\vec{w}_d$ are also eigenvectors of the matrix $\hat{\Sigma}^{(k)}=\frac{1}{N}({\bf X}^{(k)})^T{\bf X}^{(k)}$ with eigenvalues $\underbrace{0,\dots,0}_k,\lambda_{k+1},\dots,\lambda_d$ respectively.
:::

As a result, we can immediately compute the PCA variances and loadings given the full spectral (eigenvalue) decomposition \begin{equation}
\hat{\Sigma} = 
\begin{bmatrix} 
\vec{w}_1 & | & \dots & | &\vec{w}_d 
\end{bmatrix} 
\begin{bmatrix}
\lambda_1 &0 & 0 \\
0 &\ddots & 0 \\
0 & 0 &\lambda_d
\end{bmatrix}
\begin{bmatrix}
\vec{w}_1^T \\ \vdots \\\vec{w}_d^T
\end{bmatrix}.
\end{equation}


Again, thanks to orthogonality, we can also compute the PCA scores directly from the original data without iteratively removing components of each vector in the direction of the various loadings.  To summarize PCA, given data ${\bf X}$ we first compute the spectral decomposition of the sample covariance matrix $\hat{\bf \Sigma}$. The eigenvalues (in decreasing magnitude) provide the PC variance and the corresponding unit-length eigenvectors give the corresponding loadings, which form an orthonormal basis in $\mathbb{R}^d$. The PC scores are the inner product of each vector with these loadings (assuming the data are centered).  Thus, the PCA scores are $\vec{x}_i^T\vec{w}_1,\dots,\vec{x}_i^T\vec{w}_d$ for $i=1,\dots,d.$  We can compactly (again assuming our data are centered) express the scores in terms of the original data matrix and the loadings as

\begin{equation}
{\bf Y} = {\bf XW}
\end{equation}

where ${\bf Y}\in \mathbb{R}^{N\times d}$ is a matrix of PC scores and ${\bf W}$ is the orthonormal matrix with columns given by the loadings.

#### Geometric interpretation of PCA

We began with an $N\times d$ data matrix ${\bf X}$ and now we have an $N\times d$ matrix of PCA scores ${\bf Y}$. One may be inclined to ask: where is the dimension reduction?  To answer this question, let's revisit some important features of the scores and loadings.

First, since the loadings $\vec{w}_1,\dots,\vec{w}_d$ are orthonormal vectors in $\mathbb{R}^d$, they naturally form a basis!  The PCA scores are then the coordinates of our data in the basis $\{\vec{w}_1,\dots,\vec{w}_d\}$. Specifically, ${\bf Y}_{ij}$ for $j=1,\dots,d$ are the coordinates for (centered) vector $\vec{x}_i$ since $$\vec{x}_i = {\bf Y}_{i1}\vec{w}_1+\cdots+ {\bf Y}_{id}\vec{w}_d.$$

The dimension reduction comes in from an important observations about the PCA scores (aka coordinate). Consider the sample covariance matrix of the PCA scores, which one can show is diagonal (see exercises)

\begin{equation}
\hat{\bf \Sigma}_Y = 
\begin{bmatrix}
\lambda_1 &  & \\
&\ddots & \\
&& \lambda_d
\end{bmatrix}
\end{equation}


There are two important perspectives on the using PCA scores.  First, the diagonality of the PCA scores indicates that in the new basis, $\{\vec{w}_1,\dots,\vec{w}_d\}$, the coordinates are i) uncorrelated and ii) ordered in decreasing variance.  As a result, if we want to construct a $k < d$ dimensional representation our data that minimizes the loss in variability, we should use the first $k$ PCA scores.  

Secondly, we can use the first $k$ scores to build approximations,
$$\vec{x}_i \approx \hat{x}_i = {\bf Y}_{i1}\vec{w}_1+\cdots + {\bf Y}_{ik} \vec{w}_k, \qquad i=1,\dots, N.$$ How good is this approximation?  Optimal, in a squared loss sense, thanks to the following theorem.

::: {.theorem #pca-opt}
Fix $k<d.$ Given any $k$-dimensional linear subspace of $\mathbb{R}^d$, denoted $\mathcal{V}$, let $\text{proj}_{\mathcal{V}}(\vec{x})$ denote the orthogonal projection of $\vec{x}$ onto $\mathcal{V}$, then $$\frac{1}{N}\sum_{i=1}^N \|\vec{x}_i - \text{proj}_{\mathcal{V}}(\vec{x}_i)\|^2 \ge \lambda_{k+1}+\cdots+\lambda_d$$
where $\lambda_{k+1},\cdots,\lambda_d$ are the last $(d-k)$ principal component variances of $\vec{x}_1,\dots,\vec{x}_N$.  

Furthermore, if $\lambda_k > \lambda_{k+1}$, then $\text{span}\{\vec{w}_1,\dots,\vec{w}_k\}$ is the unique $k$-dimensional linear subspace for which $$\frac{1}{N}\sum_{i=1}^N \|\vec{x}_i - \text{proj}_{\mathcal{V}}(\vec{x}_i)\|^2 = \lambda_{k+1}+\cdots+\lambda_d.$$
:::

A few notes on the above theorem to add some clarity.  First, note that $$\hat{x}_i = {\bf Y}_{i1}\vec{w}_1+\cdots + {\bf Y}_{ik} \vec{w}_k$$ is the orthogonal projection of $\vec{x}_i$ onto $\text{span}\{\vec{w}_1,\dots,\vec{w}_k\}$ so that $$\vec{x}_i - \text{proj}_{\text{span}\{\vec{w}_1,\dots,\vec{w}_k\}}(\vec{x}_i) = {\bf Y}_{i,k+1}\vec{w}_{k+1}+\cdots+{\bf Y}_{id}\vec{w}_d.$$ Using this identity and the properties of the PCA scores allows one to verify the equality at the end of the theorem (see exercises).  

The requirement that $\lambda_k\ge \lambda_{k+1}$ is necessary for uniqueness.  If $\lambda_k = \lambda_{k+1}$ then the subspace $\text{span}\{\vec{w}_1,\dots,\vec{w}_{k-1},\vec{w}_{k+1}\}$ would offer an equally good approximation.  Fortunately, it is rare that two (nonzero) PCA scores are equal in practice so we rarely have to worry about this issue. In short, PCA learns the $k$-dimensional linear subspace which best approximates the original data on average!

In all of the previous analysis, we have been operating under the assumption that our data is centered, e.g. $\bar{x}=\vec{0}$ --  an preprocessign step that made our analysis easier and is done in practice to avoid potential issue with numerical linear albegra. We can recover all of the nice geometric intuition when working with uncentered data by adding the sample mean back to the approximation and shifting our focus to affine subspaces, which look just like linear subspaces which have been translated by some constant shift away from the origin.

::: {.corollary $pca-opt-aff}
Given any $k$-dimensional affine subspace $\mathcal{A}\subset\mathbb{R}^d$
$$\frac{1}{N}\sum_{i=1}^N \|\vec{x}_i - \text{proj}_{\mathcal{A}}(\vec{x}_i)\|^2 \ge \lambda_{k+1}+\cdots+\lambda_d$$ with equality (assuming $\lambda_k>\lambda_{k+1}$) if and only if $\mathcal{A} = \bar{x} + \text{span}\{\vec{w}_1,\dots,\vec{w}_k\}.$
:::

Note the set $\bar{x} + \text{span}\{\vec{w}_1,\dots,\vec{w}_k\}$ is the set of all vectors $$\{\vec{v}\in\mathbb{R}^d:\,\vec{v}-\bar{x} \in \text{span}\{\vec{w}_1,\dots,\vec{w}_k\}\},$$ i.e. the optimal $k$-dimensional hyperplane found after centering if translated by $\hat{x}$.  In this case, the optimal approximation of our data is $$\vec{x}_i \approx \bar{x} + {\bf Y}_{i1}\vec{w}_1+\cdots + {\bf Y}_{ik}\vec{w}_k.$$


### PCA in Practice

#### Choosing the number of components

Now that we have explored the mathematical details of PCA, let us turn to practical guidance when working with real data.  

A natural first question for PCA, is how many principal components should I use?  Like many modeling decisions in statistics there are a range of answers from simple heuristics to advanced technical answers. For PCA, methods based on random matrix theory are quite strong but also beyond the scope of this book. The curious reader is encouraged to see [@ScreeNot] for more information.  Herein, we are going to focus on simpler but more common approaches with an emphasis on inherent trade-offs.  Like many statistical modeling choices, decisions are not always so clear cut, and it's best to understand and reason about decisions in the context of a specific problem rather than following hard and fast rules.

The best known method for choosing an optimal number of principal components is the scree plot [@scree_og], which is based on a visual inspection of a plot of the principal component variances in decreasing order.  Ideally, one would observe an *elbow* delineating a regions where the variances decrease sharply then saturate near a small value.

```{r, fig.cap="Idealized version of the scree plot"}
library("ggrepel")
df <- data.frame(lambda = c(15,10,5,2,1.25,1,1,1,1,1,1,1,1,1,1), comp = 1:15)
elbow_point <- 5
ggplot(df, aes(x=comp, y = lambda)) + geom_point() + xlab("Component, k") + 
  ylab(expression(lambda[k])) + 
  ggtitle("Scree plot") +
  theme_minimal() +
  # Annotate the elbow point
  geom_text_repel(aes(label = ifelse(comp == elbow_point, "elbow", "")), 
                  nudge_y = 5, 
                  arrow = arrow(length = unit(0.015, "npc")))
```
In the preceding figure, we would opt to keep the first four components since the variance appears to saturate beginning with the fifth principal component.  


Another heuristic suggests keeping the number of components needed to explain a specified percentage of the variance, typically 80 or 90%.  Using 80% as a guide would suggest keeping the first 7 principal components (using the first 6 only gets us to 79.2%) based on the related figure below.  
Using the same data as above, we have the following *percentage explained variance* plot below
```{r, fig.cap = "Percentage of explained variance"}
df$varp = cumsum(df$lambda)/sum(df$lambda)
ggplot(df, aes(x=comp, y = varp)) + geom_point() + xlab("Component, k") + 
  ylab("% of explained variance")  
```

Specifically, we have plotted the cumulative sum of the p.c. variances relative to the total variance 
$$\frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^d \lambda_j} = \frac{\sum_{j=1}^k \lambda_j}{tr(\hat{\bf \Sigma}_x)}$$
as a function of the number of components.  

Based on an evaluation of the principal component variances we have two heuristics suggesting different cutoffs (4 vs. 7). Which (if either) is correct? Like many issues in statistics, the answer depends on context.  Perhaps there is a data driven reason to choose 4 or 7 or some other number for that matter.  The important issue is to recognize the inherent trade-off in the choice so that one can make the best decision in the context of the larger data science problem where PCA is being applied.  Choosing a smaller (larger) number of components gives a greater level of dimension reduction at the expense of worse (better) approximation and retention of variability.  

Additionally, the variances alone often are not enough to understand the structure of your data. As we will see in the next section, in some cases you may observe as stark drop in variances that is the result of features of data unrelated to concentration near a lower dimensional hyperplane.  


#### Limitations and vulnerabilities

PCA relies on sample variance of data projected in different direction.  As such, it is susceptible to outliers and/or imbalanced scales which can greatly influence calculations of sample variance.  Furthermore, PCA is optimal at finding the best *linear* subspace as shown in the preceding sections.  However, it cannot find (potentially simpler and more compressed) nonlinear structure in the data.  Let's investigate each of these limitations in the following subsections.

##### Imbalanced scales

As a motivating example, let's consider the [wine dataset](https://archive.ics.uci.edu/ml/index.php) which contains observations of 13 chemical compounds (variables) for 178 different wines (subjects) from three different cultivars.  Below we show the first 10 rows of the data matrix.

```{r, echo = F}
library(HDclassif)
data("wine")
# wine.class <- wine[,1]
# wine <- wine[,-1]
# colnames(wine) <- c("alcohol","malic_acid","ash",
#                     "alcalinity","magnesium","phenols",
#                     "flavanoids","nonflav_phenols","proanthcyanins",
#                     "color","hue","OD280",
#                     "proline")
knitr::kable(head(wine,10))
```

Prior to running PCA, we might be able to guess which coordinates account for the most variability. Proline (on the scale of 1000s) and magnesium (on the scale of 100s) have much larger values than other coordinates in the data matrix.  We can see this observation holds out immediately when looking at the *biplot* summarizing PCA run on the wine data.

```{r}
library(ggbiplot)
out <- prcomp(wine)
ggbiplot(out, scale = 0, varname.color = "red")  +ylim(c(-250,250))
```

In the figure above, the black dots are scatterplots of the first and second principal component score for each sample in our data set, i.e. $\{(y_{i1},y_{i2})\}_{i=1}^N$, whereas the arrows correspond to the contribution of each column of the data matrix to the first and second loadings, i.e. $\{(\vec{w}_1)_j,(\vec{w}_2)_j\}_{j=1}^d$.  The red vector associated with proline  is the only one with a large horizontal component, indicating it is essentially the only coordinate contributing to the first loading.  A similar observation indicates magnesium is the only coordinate contributing to the second loading.  Admittedly, the arrow corresponding to the variables are difficult to see, but we can zoom in closer to the origin where the results are clear.

```{r, echo = F}
ggbiplot(out, scale = 0, varname.color = "red")  +xlim(c(-110,50))
```

A natural way (and generally good idea) to address this issue is to standardize the columns of the data matrix prior to running PCA.  If we use this approach for the wine data, we get the following biplot where there is much more diverse contributions to the first and second loadings from many coordinates.

```{r, echo = F}
out <- prcomp(wine, scale. = TRUE)
library(gridExtra)
grid.arrange(ggplot(data.frame(comp = 1:length(out$sdev), var = out$sdev^2),aes(x=comp,y=var)) + geom_point() + xlab("Variance") + ylab("Component") +ggtitle("Scree plot"),
  ggbiplot(out,scale = 0, varname.color = "red") + ggtitle("Biplot") ,
  nrow = 1)  
```
##### Outliers

The existence of an outlier in a dataset (as in the univariate case) will alter the calculation of a sample covariance. However, a point can be an outlier because it has an extreme value in one **or more** of its coordinates. Unfortunately, standardization will not do much to alter this issue.  Nonethess, the biplot is a helpful method for identifying this issue.  Consider the wine data set, but let us now append a new data vector.  It has the same values as the first row, except let's set the malic acid and ash columns to $10^6$ (these are unrealistic  extreme values only intended to emphasis the effect of an outlier). Below we show the scree plot and biplot resulting from PCA without standardization.

```{r, echo = F, fig.cap="PCA on wine data with an outlier"}
data(wine)
wine <- rbind(wine,wine[1,])
wine[179,2:3] <- 1e6
out <- prcomp(wine, scale. = F)
library(gridExtra)
grid.arrange(ggplot(data.frame(comp = 1:13, var = out$sdev^2),aes(x=comp,y=var)) + geom_point() + xlab("Variance") + ylab("Component") + ggtitle("Scree plot"),
  ggbiplot(out,scale = 0, varname.color = "red") + ylim(c(-1e6,5e5))+ ggtitle("Biplot"),
  nrow = 1) 
```

Here, we see one point has an extreme PC score associated with the outlier, which also accounts for a single large variance.  It would be inappropriate to assume that a single PC component is sufficient to describe the data. Additionally, observe that the first loading is dominated by Malic Acid and Ash, values for which the outlier is extreme.  Standardizing the coordinates can reduce the impact of the outlier but cannot remove it entirely.  In such as case, further investigation to see the specific impact the outlier has on the loadings (and the potential overemphasis on some features) warrants further investigation.

```{r, echo = F, fig.cap="PCA on wine data with outlier after standardization"}
data(wine)
wine <- rbind(wine,wine[1,])
wine[179,2:3] <- 1e6
out <- prcomp(wine, scale. = T)
library(gridExtra)
grid.arrange(ggplot(data.frame(comp = 1:13, var = out$sdev^2),aes(x=comp,y=var)) + geom_point() + xlab("Variance") + ylab("Component")+ ggtitle("Scree plot"),
  ggbiplot(out,scale = 0, varname.color = "red")+ ggtitle("Biplot"),
  nrow = 1) 
```


##### Nonlinear structure

Approximations using PCA are built via linear combinations of the loadings. However, there is not reason to *a priori* expect that the lower dimensional structure of our data is inherently linear.  As such, PCA (and the other linear methods we'll discuss in the remainder of this chapter) are limited in the complexity of structure they can discover in data.  As a final example, consider the following data on a curve in $\mathbb{R}^3$.

```{r, echo = F}
n <- 1e3
th <- runif(n,0,6*pi)
df <- data.frame(x=th*cos(th),y=th*sin(th),z = 0.05*th^2)
plot_ly(data.frame( x = df[,1], y= df[,2], z = df[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 2)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-20, 20)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-20, 20)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(0, 17)))
         )
```

These data reside on a one-dimensional curve, specifically the curve parameterized by the equation $$t \mapsto (t\cos(t),t\sin(t),t^2/20)^T.$$

However, if we inspect the principal component variance, we do not see a clear separation in the first PC variance with the second and third.  There is a jump between the second and third, and plotting the first two scores (biplot) shows the nonlinear structure.

```{r, echo = FALSE}
out <- princomp(df)
grid.arrange(ggplot(data.frame(comp = 1:3, var = out$sdev^2),aes(x=comp,y=var)) + geom_point() + xlab("Variance") + ylab("Component")+ ggtitle("Scree plot") + ylim(c(0,70)),
  ggbiplot(out,scale = 0, varname.color = "red")+ ggtitle("Biplot"),
  nrow = 1) 
```

Ultimately, the fundamental limitation of PCA is a direct byproduct of it's primary strength. PCA discovers best approximating hyperplanes but nothing more complex! 