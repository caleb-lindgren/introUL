## Model-Based Clustering


**Goal**: Similar to traditional clustering, but assumes a probabilistic model that describes data generation for each cluster.

### Key Components

1. **Cluster Densities**: Each cluster is represented by a probability distribution \( f_j \) over data space.
2. **Cluster Probabilities**: Probabilities \( p_j \) represent the likelihood of data belonging to each cluster.

Suppose data \( x \in \mathbb{R}^d \) are generated by first choosing a cluster label \( Z \) with probability \( p_j \), then drawing \( x \) from \( f_j \), independently for each data point.

### Gaussian Mixture Models (GMM)

### Gaussian Mixture Model (GMM) for \( k \) Clusters

For a fixed number of clusters \( k \), we assume that each cluster follows a Gaussian distribution, and we aim to estimate parameters that maximize the model's likelihood.

The probability density function for GMM is:

$$
f(x) = \sum_{j=1}^{k} p_j \phi(x | \mu_j, \Sigma_j)
$$

where:
- \( \phi(x | \mu_j, \Sigma_j) \) is the Gaussian density for cluster \( j \),
- \( p_j \) is the weight of cluster \( j \).

# Expectation-Maximization (EM) Algorithm

1. **E-Step**: Estimate the probability that each data point belongs to a cluster given current parameters.
2. **M-Step**: Maximize the expected log-likelihood with respect to parameters.

### Steps in the EM Algorithm

1. **Initialize** values for means, covariances, and weights for each cluster.
2. **E-Step**: For each data point \( x_i \), calculate the responsibility \( \tau_{ij} \), the probability that \( x_i \) is assigned to cluster \( j \).
3. **M-Step**: Update parameters to maximize the likelihood:
   - Update means \( \mu_j \)
   - Update covariances \( \Sigma_j \)
   - Update weights \( p_j \)
4. **Repeat** until convergence.

### Model Selection

To determine the optimal number of clusters, we often use criteria like **AIC** and **BIC**:
- **AIC (Akaike Information Criterion)**:
  
  $$
  AIC = 2p - 2 \log(L)
  $$

- **BIC (Bayesian Information Criterion)**:
  
  $$
  BIC = p \log(N) - 2 \log(L)
  $$

Lower values of AIC/BIC indicate better models, balancing fit and complexity.

### Example in R: Gaussian Mixture Modeling

```{r, echo = F, message = F}
# Example using the mclust package for GMM in R
# library(mclust)
# data <- matrix(rnorm(100), ncol=2)
# gmm_result <- Mclust(data)
# # Plot classification
# plot(gmm_result)
```


Covariance Structure Choices in GMM
Different choices for covariance matrices affect the flexibility of cluster shapes:

Diagonal: Each cluster has independent dimensions.
Full: Each cluster has its own covariance matrix, allowing for elliptical clusters.
Spherical: Clusters are spherical, all with the same radius.
Choosing Covariance Type
Different choices allow clusters to have various orientations and spreads, which is essential for capturing complex structures in data.

Conclusion
Model-based clustering, especially with GMM, provides a flexible and probabilistic approach to clustering. The EM algorithm facilitates parameter estimation, while criteria like AIC and BIC help determine the optimal number of clusters.