## Model-Based Clustering {#sec-model-based-clustering}
```{r, echo = FALSE, message=FALSE}
library(mclust, quietly = TRUE)
```

Unlike the preceding methods, model-based clustering assumes a probabilistic model for the data generating process. This model implies a likelihood (or posterior if we want to take a fully Bayesian approach) which we can optimize over the model parameters.  Using the optimal parameters, we can construct probability weights for cluster membership (e.g. we assign sample one to cluster one with probability 0.6 and cluster two with probability 0.4) or make a discrete decision by assigning each sample to the cluster for which it has the highest probability of membership.

For now, fix the number of clusters as$K$. Let us turn our attention to a standard two-step approach for generating a data set clustering structure which relies on two key components

1. **Cluster Densities**: Each cluster is associated with a probability density \( f_j:\mathbb{R}^d \to \mapsto[0,\infty) \), for $j=1,\dots,K$. (In the related literature, these densities are often referred to as kernels which should be confused with the kernels we have discussed previously.)
2. **Cluster Probabilities**: Probabilities $\{p_j\}_{j=1}^K represent the *a priori* probability that a sample will be assigned to a given cluster


Now, suppose random vector \( \vec{x} \in \mathbb{R}^d \) is generated by first choosing a cluster label \( Z \) with probability \( p_j \), then conditional on $Z=j$ we sample \( x \) from \( f_j \). This method implies the following joint density of $(\vec{x},z)$
\begin{equation}
p(\vec{x},Z=j) = p_j f_j(\vec{x})
\end{equation}
If we marginalize over $z$, we then obtain the following mixture density for $\vec{x}$,
\begin{equation}
p(\vec{x}) = \sum_{j=1}^K p_jf_j(\vec{x})
\end{equation}

We then repeat this approach independently $N$ times to generate our data set.  Hereafter, we'll use $Z_1,\dots,Z_n \in \{1,\dots,K\}$ to denote the **latent** cluster labels for each observation and continue using the notation $\vec{x}_1,\dots,\vec{x}_N$ for our observed data. Under an independent sampling assumption, we then obtain the following joint distribution for the latent cluster labels and observed data
\begin{equation}
p(\vec{x}_1,Z_1=z_1,\dots,\vec{x}_N,z_N) = \prod_{i=1}^N p_{z_i} f_{z_i}(\vec{x}_i)
(\#eq:GMM-likelihood-with-latents)
\end{equation}
When we turn to likelihood estimation, a slightly different expression of the likelihood will be helpful. We can rewrite \@ref(eq:GMM-likelihood-with-latents) in the following form:
\begin{align}
p(\vec{x}_1,Z_1=z_1,\dots,\vec{x}_N,z_N) = \prod_{i=1}^N p_{z_i} f_j(\vec{x}_i) \\
&= \prod_{i=1}^N\sum_{j=1}^K \mathbb{1}(z_i=j)p_j f_j(\vec{x}_i) \\
&= \prod_{i=1}^N\prod_{j=1}^K \left[p_j f_j(\vec{x}_i)\right]^{\mathbb{1}(z_i=j)} (\#eq:handy-joint-likelihood)
\end{align}
Marginalizing over $Z_1,\dots,Z_N$ we then obtain the following joint density for the observed data
\begin{equation}
p(\vec{x}_1,\dots,\vec{x}_N) = \prod_{i=1}^N\left(\sum_{j=1}^K p_j f_j(\vec{x}_i)\right).
\end{equation}

As in any modeling approach, performance is strongly influenced by the flexibility of the model to capture the observed data.  There are many decisions which one can use to encode beliefs of the inherent structure of the data including the number of clusters, their shape, and the proportion of samples we expect in each cluster.  For remainder of this section, we will focus on Gaussian mixture models (wherein $f_1,\dots,f_K$ are Gaussian densities) which are the most common application of model-based clustering.


### Gaussian Mixture Models (GMM)
For a fixed number of clusters \( k \), we assume that each cluster follows a Gaussian distribution, which are specified through their associated means and covariances. Following the notational convention from Chapter 2, we have $$f_j(\vec{x}) = \frac{1}{(2\pi)^{d/2}|{\bf \Sigma}_j|}\exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu}_j)^T{\bf \Sigma}_j^{-1}(\vec{x}-\vec{\mu}_j)\right).$$
Following the notational convention from chapter 2, we use the shorthand
$$f_j(\vec{x}) = \mathcal{N}\left(\vec{x};
\,\vec{\mu}_j,{\bf \Sigma}_j\right).$$
A Gaussian Mixture model (GMM) with $K$ cluster is thus specified by the associated means, $\{\vec{\mu}\}_{j=1}^K$, covariances matrices $\{{\bf \Sigma}_j\}_{j=1}^K$ and probabilities $p_1,\dots,p_K$.  For brevity, we'll use $$\Phi = \left\{ \{\vec{\mu}\}_{j=1}^K, \{{\bf \Sigma}_j\}_{j=1}^K, \{p_j\}_{j=1}^K \right\}$$
to denote the set of model parameters.  We'll use $p(\vec{x}_1,z_1,\dots,vec{x}_N,z_N \mid \Phi)$ to denote the joint distribution of the latent cluster labels and observations for a given set of parameters $\Phi$. Similarly, we'll use $p(\vec{x}_1,\dots,\vec{x}_N \mid \Phi)$ to denote the marginal density of the observations given parameters $\Phi$. Later, we'll consider fixing $\vec{x}_1,\dots,\vec{x}_N$ and treating $p(\vec{x}_1,\dots,\vec{x}_N\mid\Phi)$ as a likelihood for $\Phi$. First, let's visualize data that can be generated by a GMM to demonstrate what shapes of clusters are possible for GMMs.

::: {.example #gmm name="Samples from a GMM"}
Briefly, we'll demonstrate the type of data one can generate from a Gaussian mixture model.  We focus on a two-dimensional case with $K=3$ clusters.  For the sampling process, we'll use means
$$
\vec{\mu}_1 = \begin{bmatrix} 6 \\ 0 \end{bmatrix}, \,
\vec{\mu}_2 = \begin{bmatrix} 0 \\ 3 \end{bmatrix}, \,
\vec{\mu}_3 = \begin{bmatrix} -3 \\ 0 \end{bmatrix}, \,
$$
and covariances
$$
{\bf \Sigma}_1 = \begin{bmatrix} 1 & 0.5\\ 0.5 & 1 \end{bmatrix}, \,
{\bf \Sigma}_2 = \begin{bmatrix} 0.5 & 0  \\ 0 &0.5 \end{bmatrix}, \,
{\bf \Sigma}_3 = \begin{bmatrix} 5 & -3 \\ -3 & 2\end{bmatrix}, \,
$$
for the Gaussian densities. And we'll use probabilities $\vec{p}=(0.5,0.3,0.2)$ for the cluster label assignment.  Below, we show 1000 samples from this GMM with points color-coded by sampled cluster label $z_i$.

```{r ex-GMM, fig.align = 'center', fig.cap="Samples from a GMM", echo = FALSE}
set.seed(5)
N <- 1e3
p <- c(0.5,0.3,0.2)
mu1 <- c(6,0)
mu2 <- c(0,3)
mu3 <- c(-3,0)
Sigma1 <- matrix(c(1,0.5,0.5,1),nrow = 2)
Sigma2 <- matrix(c(0.5,0,0,0.5), nrow = 2)
Sigma3 <- matrix(c(5,-3,-3,2),nrow = 2)
Z <- sample(c(1,2,3), N, prob = p, replace = TRUE)
X.gmm.ex <- matrix(NA, nrow = N, ncol =2 )
for (j in 1:N){
  X.gmm.ex[j,] <- (Z[j]==1)*mvrnorm(n=1, mu1,Sigma1) +
    (Z[j]==2)*mvrnorm(n=1, mu2,Sigma2) +
    (Z[j]==3)*mvrnorm(n=1, mu3,Sigma3) 
}
plot(X.gmm.ex[,1],X.gmm.ex[,2],col = c("red","blue","black")[Z],
     xlab = "",ylab = "")
```
:::

In the preceding example, the clusters had an elliptical shape which is a distinct variation from $k$-means clustering which presumes spherical clusters. The location and shape/orientation of each cluster is controlled by its mean vector and covariance matrix respectively. As such, GMMs can still generate spherical clusters if the eigenvalues of each covariance matrix are (approximately) equal. This is a natural consequence of the Gaussian distribution and diagonalization of the covariance matrix.  In high dimensions, Gaussian distributions concentrate near the surface of ellipsoids (rather than on the interior) but the general observation regarding the increased flexibility of GMMs over k-means holds true. Ignoring computational demands, one might expect GMMs to outperform k-means in many settings.  

Model flexibility is not the only advantage of mixture models.  Suppose you were asked to assign a sample at the location marked with a purple *x* below.

```{r, echo = FALSE, fig.align='center'}
plot(X.gmm.ex[,1],X.gmm.ex[,2],col = c("red","blue","black")[Z],
     xlab = "",ylab = "")
points(x=2.3,y=-2.2,pch = 4, col = 'purple')
```

Do you expect the *x* was generated from the same density as the points in the red cluster or from the density that generated the point in the black cluster?  In center based clustering, this is a binary choice.  However, it is unclear from the picture so perhaps that most honest answer is that there is a 50/50 chance that it was generated from either (we can likely agree there is vanishingly small chance it belongs to the blue cluster). The option to make such *fuzzy* clustering is a natural consequence of the probabilistic modeling approach behind GMMs.  For a given parameter vector $\Phi$ we can compute a posterior $p(z_i | \vec{x}_i,\Phi)$ reflecting our belief about the clustering labeling.

Ultimately, clustering via GMMs requires estimation of the parameter $\Phi$ for a given model. If we had access to the cluster labels, $z_1,\dots,z_N$ this would be an easy problems. We could simply take sample means and covariances for the samples assigned to cluster $j$ to estimate $\vec{\mu}_j$ and ${\bf \Sigma}_j$ and we could use the proportion of samples given cluster label $j$ as our estimate for $p_j$.  Mathematically, we would use the estimates
\begin{equation}
  \hat{\mu}_j = \frac{\sum_{i=1}^N\mathbb{1}(Z_i=j)\vec{x}_i}{\sum_{i=1}^N\mathbb{1}(Z_i=j)},
  \, \hat{\bf \Sigma}_j = \frac{\sum_{i=1}^N\mathbb{1}(Z_i=j)(\vec{x}_i-\hat{\mu_j})(\vec{x}_i-\hat{\mu_j})^T}{\sum_{i=1}^N\mathbb{1}(Z_i=j)},
  \, \hat{p}_j = \frac{\sum_{i=1}^N\mathbb{1}(z_i=j)}{N}

  (\#eq:simple_gmm_estimates)
\end{equation}
where $\mathbb{1}(z_i=j)$ is the indicator function taking value one if $z_i=j$ and zero otherwise.  

Unfortunately, we do not have access to $z_1,\dots,z_N$. (If we did, there would be no need to estimate the clusters.) So we need an algorithm for optimizing $p(\vec{x}_1,\dots,\vec{x}_N \mid \Phi)$ over $\Phi$.  One approach is to use a gradient based method to optimize the log-likelihood
\begin{equation}
\log p(\vec{x}_1,\dots,\vec{x}_N\mid \Phi) = \sum_{i=1}^N\log\left(\sum_{j=1}^N p_j\mathcal{N}(\vec{x}_i;\, \vec{\mu}_j,{\bf \Sigma}_j)\right).
\end{equation}
However, this method easily suffers from computation limitations (underflow) and additional care is required to enforce constraints on the parameters: the covariance matrices $\{{\bf \Sigma}_j\}_{j=1}^K$ must be positive definite and the probabilities $p_1,\dots,p_K$ must be non-negative and sum to one.  Instead, we'll use the powerful Expectation-Maximization (EM) algorithm which was named in the 1977 paper by Dempster, Laird, and Rubin [@Dempster1977]. In addition to proving convergence to a local maxima in our GMM setting, that paper also discusses many use cases for EM beyond mixture models.  

### Expectation-Maximization (EM) Algorithm

EM can be opaque when one is first exposed to the method.  We'll discuss the mathematical details shortly. For now, let's discuss the algorithm from a high level. It begins with an initial guess for $\Phi$.  The EM algorithm is comprised of two aptly named steps.

1. **E-Step**: We use our current guess for the model parameters to estimate the cluster labels $z_1,\dots,z_N$. These estimates are probabilistic in the sense that we compute $p(z_i\mid \vec{x}_i,\Phi)$ rather than discrete choices of the labels. 
2. **M-Step**: Maximize the expected log-likelihood with respect to parameters, $\Phi$, using the estimated label probabilities. In this step, we modify the estimates in \eqref(eq:simple_gmm_estimates) to reflect our uncertainty in the cluster labels.

The EM algorithm then proceeds by iterating the E and M steps until convergence to a local maximum is achieved.  Let's explore these steps in greater detail for GMMs. Let $\Phi^{(t)}$ to denote the estimate of $\Phi$ after $t$ iterations of the E and M steps. We'll use the superscript $^{(t)}$ above individual parameters as well.

In the E-step, we want to find $p(z_i\mid \vec{x}_i, \Phi^{(t)})$ which follows from an application of Bayes' formula
\begin{align*}
p(z_i=j\mid \vec{x}_i, \Phi^{(t)}) & = \frac{p(\vec{x}_i,z_i=j\mid \Phi^{(t)})}{p(\vec{x}_i\mid \Phi^{(t)})} \\
&= \frac{p(\vec{x}_i \mid z_i=j,\Phi^{(t)})p(z_i=j\mid \Phi^{(t)})}{p(\vec{x}_i\mid \Phi^{(t)})} \\
&= \frac{p_j^{(t)} \mathcal{N}(\vec{x}_i; \vec{\mu}^{(t)}_j,{\bf \Sigma}^{(t)}_j)}{\sum_{\ell=1}^Kp_\ell^{(t)} \mathcal{N}(\vec{x}_i; \vec{\mu}^{(t)}_\ell,{\bf \Sigma}^{(t)}_\ell)}
\end{align*}
We'll adopt the notation $$\gamma_{ij}^{(t)} = \frac{p_j^{(t)} \mathcal{N}(\vec{x}_i; \vec{\mu}^{(t)}_j,{\bf \Sigma}^{(t)}_j)}{\sum_{\ell=1}^Kp_\ell^{(t)} \mathcal{N}(\vec{x}_i; \vec{\mu}^{(t)}_\ell,{\bf \Sigma}^{(t)}_\ell)}$$ to compress our notation.  Importantly, the quantities $\gamma_{ij}^{(t)}$ give a conditional distribution of $z_1,\dots,z_N$ given $\vec{x}_1,\dots,\vec{x}_N$ and $\Phi^{(t)}$

Now, let's turn to the handy form of the likelihood of the observations and the latent clusters labels in \@ref(eq:handy-joint-likelihood) which has associated log-likelihood
$$\log p(\vec{x}_1,z_1,\dots,\vec{x}_N,z_N \mid \Phi) = \sum_{i=1}^N\sum_{j=1}^K \mathbb{1}(z_i =j) \log\left( p_j \mathcal{N}(\vec{x}_i; \vec{\mu}_j, {\bf \Sigma}_j)\right).$$ 
Treating $\vec{x}_1,\dots,\vec{x}_N$ and $\Phi$ as fixed, we take the expectation of the above expression w.r.t. the cluster label probabilities $\gamma_{ij}^{(t)}$. From the linearity of expectation, we have 
\begin{align*}
Q(\Phi\mid \Phi^{(t)}) &= E_{z_1,\dots,z_N \mid \vec{x}_1,\dots,\vec{x}_N,\Phi^{(t)}}\left[ \log p(\vec{x}_1,z_1,\dots,\vec{x}_N,z_N \mid \Phi) \right] \\
&= \sum_{i=1}^N\sum_{j=1}^K \gamma_{ij}^{(t)} \log\left( p_j \mathcal{N}(\vec{x}_i; \vec{\mu}_j, {\bf \Sigma}_j)\right) \\
&= \sum_{i=1}^N\sum_{j=1}^K \gamma_{ij}^{(t)} \left[\log p_j -\frac{d}{2}\log(2\pi) - \frac{1}{2}\log |{\bf \Sigma}_j| - \frac{1}{2}(\vec{x}_i - \vec{\mu}_j)^T{\bf \Sigma}_j^{-1}(\vec{x}_i - \vec{\mu}_j)     \right]
\end{align*}
The indicators $\mathbb{1}(z_i=j)$ are replaced with the quantities $\gamma_{ij}^{(t)}$ when we take the expectation! Once we have computed $Q(\Phi\mid \Phi^{(t)})$ the E-step is complete.

For the $M$-step, we then optimize $Q(\Phi\mid\Phi^{(t)})$ over $\Phi$ so that $$\Phi^{(t+1)} = \text{argmax}_{\Phi}\,Q(\Phi\mid \Phi^{(t)}).$$ For a GMM, there are closed form expressions for the optimal values of parameters.  One can compute them directly using partial derivatives (and enforcing the constraints). We omit the calculation here and cite the result instead

\begin{align*}
p_j^{(t+1)} &= \frac{\sum_{i=1}^N \gamma_{ij}^{(t)}}{\sum_{i=1}^N\sum_{\ell=1}^K \gamma_{i\ell}^{(t)}} = \frac{\sum_{i=1}^N\gamma_{ij}^{(t)}}{N} \\
\vec{\mu}_j^{(t+1)} &= \frac{\sum_{i=1}^N \gamma_{ij}^{(t)}\vec{x}_i}{\sum_{i=1}^N \gamma_{ij}^{(t)}} \\
{\bf \Sigma}_j^{(t+1)} &= \frac{\sum_{i=1}^N \gamma_{ij}^{(t)}(\vec{x}_i-\vec{\mu}_j^{(t+1)})(\vec{x}_i-\vec{\mu}_j^{(t+1)})^T}{\sum_{i=1}^N \gamma_{ij}^{(t)}} 
\end{align*}

The above quantities resemble the simple estimates from \@ref(eq:simple_gmm_estimates). Rather than taking average using the known labels, we take weighted estimates using the relative probability of vectors have associated cluster labels!  

Once we have computed $\Phi^{(t+1)}$ when then return to the E-step and continuing iterating until convergence.  Convergence of the EM algorithm to a local maximum follows directly from Jensen's inequality [@Dempster1977].  Since the algorithm is only guaranteed to converge to a local maximum, one should typically investigate multiple initial conditions then select the parameter value among all optima that attains the highest log-likelihood $p(\vec{x}_1,\dots,\vec{x}_N\mid \Phi)$. 

Given a final estimate $\Phi^*$ we can now determine a clustering of the data. Let $$\gamma_{ij}^* = \frac{p_j^{*} \mathcal{N}(\vec{x}_i; \vec{\mu}^*_j,{\bf \Sigma}^{*}_j)}{\sum_{\ell=1}^Kp_\ell^* \mathcal{N}(\vec{x}_i; \vec{\mu}^*_\ell,{\bf \Sigma}^*_\ell)}$$ denote our estimate cluster label probabilities for each observation given parameter $\Phi^*$.  Each $\gamma_{ij}^*$ reflects our belief that observation $i$ was generated by the $j$th cluster density. To make a discrete clustering, we then assign observation $\vec{x}_i$ to the cluster maximizing $\gamma_{ij}^*$.


### Model Selection and Parameter Constraints

The EM-algorithm provides a method for a finding the optimal parameters and subsequent clustering for a fixed number of clusters $K$.  If we wish to estimate the number of clusters, we can use the maximized log-likelihood as a measure of the goodness of fit to the data. Let $$\log L_k^* = \log p(\vec{x}_1,\dots,\vec{x}_N\mid \Phi^*)$$ denote the maximum log-likelihood assuming $k$ clusters.  Larger values of $\log L_k^*$ reflect a better fit to the observed data so it natural to expect $\log L_k^*$ to increase as $k$ increases.  To avoid overfitting, we use the Bayesian Information Criterion (BIC). Let $\mathcal{P}_k$ denote the number of parameters in a GMM for $k$ clusters. BIC is defined as 

$$BIC_k = 2 \log L_k^* - 2 \log \mathcal{P}_k$$

which balances goodness of fit with model complexity. Under this framework, we select the value of $k$ which maximizes BIC. (Note, some sources define BIC as the negative of our formula above and select $k$ opt to minimize. Be mindful of the convention when using pre-existing packages!) 

We can use the BIC approach to investigate additional constraints on the model parameters. Recall from the \@ref(ex:gmm), the shape of clusters is controlled by the covariance matrices.  By applying addition constraints to ${\bf \Sigma}_1,\dots,{\bf \Sigma}_K$ we can reduce the number of model parameters.  The standard convention for covariance constraints comes from Banfield and Raftery (1993) who observed that every covariance matrix can be decomposed via its eigenvalue decomposition $${\bf \Sigma}_j = \lambda_j {\bf W}_j{\bf \Lambda}_j{\bf W}_j$$
where $\lambda_j$ is a scalar parameter controlling the volume of the cluster,  ${\bf W}_j$ is an orthonormal matrix controlling the orientation of the cluster, and ${\bf \Lambda}_j$ is a diagonal matrix with diagonal entries in the interval $(0,1]$ giving the relative lengths of the semi-major axes of the ellipsoid [@banfield1993].  We can apply constraints on one or more of the corresponding components of the covariance matrices summarized in the table below

```{r, echo = FALSE}
tab <- data.frame(
  Equal = c("$$\\lambda=\\lambda_1=\\dots=\\lambda_K$$",
         "$$\\Lambda = \\Lambda_1 = \\dots = \\Lambda_K$$",
         "$$W=W_1=\\dots W_K$$"),
  Variable = c("$$\\lambda_1,\\dots,\\lambda_K$$ can differ",
          "$$ \\Lambda_1,\\dots, \\Lambda_K$$ can differ",
          "$$ W_1,\\dots W_K$$ can differ"),
  Identity = c("Not applicable",
         "$$\\Lambda_1=\\dots \\Lambda_K = I$$",
         "$$W_1=\\dots W_K = I$$")
)
row.names(tab) <- c("Equal","Variable","Identity")
knitr::kable(t(tab), format = "markdown", booktabs = TRUE, escape = FALSE,
             col.names = c("Volume","Shape","Orientation"),
             row.names = TRUE)

```

We then use triplets of \underline{E}qual, \underline{V}ariable, and \underline{I}dentity as shorthand for covariance constraints. For example, a GMM with covariance constraint EVE indicates covariances with equal volume, varying shape, and equal direction so that $${\bf \Sigma}_j = \lambda {\bf W}{\bf \Lambda}_j{\bf W}^T.$$  This results in a total of $2\times 3\times 3 -4=14$ covariance options. We have subtracted 4 since some triplets coincide such as EII, EIE, and EIV.

For each covariance model, we can count the associated number of parameters and compute BIC accordingly. As a result, we have quantitative method for choosing the number of clusters and the optimal covariance model (for a given number of clusters). GMMs (with the help of the EM algorithm for fitting) provide a self-contained clustering framework with easy to interpret model selection criteria. However, there are inherent limitations which should be addressed.

### Weakneses and Limitations

Unlike center-based methods, GMMs allow for a greater level of flexibility in the shapes of clusters (ellipses vs spheres). However, ellipsoids may still be inadequate choices a cluster supported on or near a nonlinear manifold.  For example, in the **smile** data below.

::: {.example name="GMMs fail to capture complex nonlinear structure"}

Consider the following data in $\mathbb{R}^2$ which exhibitis four clusters upon visual inspection

```{r, echo = FALSE}
smile <- read.csv("../data/smile.csv")[,-1]
plot(smile[,1],smile[,2],
     xlab = "", ylab ="",
     xaxt = 'n',yaxt = 'n')
```

The eyes and nose are each compact and approximately elliptical in shape, but the mouth is concentrated around a nonlinear parabolic curve. Applying the GMM model above for a range $K=2,\dots,15$ clusters gives the following BIC curves

```{r, fig.align='center', fig.cap="BIC curves for GMMs applied to the smile data", echo = FALSE}
# smile.kmm <- kmeans(smile, centers =1:10)
smile.gmm <- Mclust(smile, G = 2:15, verbose = FALSE)
plot(smile.gmm, "BIC")
```

Using BIC, the optimal GMM one should select contains nine clusters with EVV covariance structure.  In the associated clustering shown below, the eyes are appropriately clustered, but the nose is split into two separate clusters. Most egregiously though, we can see the mouth was broken apart into five thin elliptic pieces reflecting the inherent limitations of the cluster shapes that can be identified by GMMs.  

```{r}
plot(smile.gmm, what = "classification",
     xaxt='n',yaxt='n',
     xlab = '', ylab ='')
```

:::


Given this limitation, there has been considerable work dedicated to improving model-based clustering including the design of more expressive mixture densities [@Karlis2009;@OHagan2016;@Dang2023} and methods which produce more parsimonious clustering arrangements by iteratively merging nearby clusters [@Dombowsky2025].
