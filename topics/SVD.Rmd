## Singular Value Decomposition {#sec-svd}

### Low-rank approximations 
In the next two subsections, we are going to focus on *low-rank* matrix approximation methods in which we try to approximate our data matrix ${\bf X}$ using a low-rank alternative.  In the language of dimension reduction, the idea is to approximate each data with a linear combonation of a small number ($k< d$ of latent feature vectors.  Briefly, let's discuss how this idea works in the case of PCA.


In PCA, the loadings provide a data-driven orthonormal basis $\vec{w}_1,\dots,\vec{w}_d$ which allow us to compute the PCA scores from the centered data. In matrix notation, this scores are given by $$\underbrace{{\bf Y}}_{\text{PCA scores}} = \underbrace{({\bf HX})}_{\text{centered data matrix}} \times \underbrace{{\bf W}}_{\text{loadings}}.$$

The matrix ${\bf W}$ is orthonormal allowing us to write $${\bf HX} = {\bf YW }^T.$$ The $ith$ row of the preceding matrix equality reads
$$(\vec{x}_i - \bar{x})^T = \sum_{j=1}^d {\bf Y}_{ij} \vec{w}_j^T.$$  From the PCA notes, an approximation using the first $k$ loadings
$$(\vec{x}_i - \bar{x})^T \approx \sum_{j=1}^k {\bf Y}_{ij} \vec{w}_j^T$$
minimizes the average squared Euclidean distance over all vectors.  In matrix notation, the approximation over all vectors decomposes as the product of an $N\times k$ matrix and a $k\times d$ matrix as follows. 
$${\bf HX} \approx \underbrace{\begin{bmatrix}{\bf Y}_{11} & \dots & {\bf Y}_{1k} \\
\vdots & & \vdots \\
\vdots & & \vdots \\
{\bf Y}_{N1} & \dots & {\bf Y}_{Nk}\end{bmatrix}}_{N\times k} \underbrace{\begin{bmatrix}\vec{w}_1^T \\ \vdots \\ \vec{w}_k^T \vphantom{\vdots} \end{bmatrix}\\}_{k\times d}.$$

Due to the properties of the scores and loadings, the approximation is a rank $k$ matrix.  In the following sections, we'll seek similar decompositions of our data matrix.

### SVD and Low Rank Approximations

The standard problem for low rank matrix approximations is to solve the following problem. Given a matrix ${\bf X}\in\mathbb{R}^{N\times d}$ and a chosen rank $k$, we want:
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{equation}
\argmin_{{\bf Z}\in\mathbb{R}^{N\times d} \\ \text{rank}({\bf Z}) = k} \|{\bf X} - {\bf Z}\|_F^2 = \argmin_{{\bf Z}\in\mathbb{R}^{N\times d} \\ \text{rank}({\bf Z}) = k} \left(\sum_{ij}({\bf X}_{ij} - {\bf Z}_{ij})^2 \right) = \argmin_{{\bf Z}\in\mathbb{R}^{N\times d} \\ \text{rank}({\bf Z}) = k} \left(\sum_i \|\vec{x}_i-\vec{z}_i\|^2\right)
\end{equation}
where $\vec{z}_1^T, \dots,\vec{z}_N^T$ denote the rows of ${\bf Z}$.

Solving this constrained minimization problem may appear difficult, but the answer is obtainable directly from the SVD of ${\bf X}$ due to the following theorem.

::: {.theorem #svd-frob name="Best Rank $k$ Approximation"}
Suppose matrix ${\bf X}\in\mathbb{R}^{N\times d}$ has singular value decomposition $${\bf X} = {\bf US V}^T$$ with singular values $$\sigma_1\ge\dots \ge \sigma_{\min\{N,d\}}.$$  Then 
1) For any rank $k$ matrix ${\bf Z}\in\mathbb{R}^{N\times d}$, $$\|{\bf X}-{\bf Z}\|_F \ge \sigma_{k+1}^2 + \dots + \sigma_{\min\{N,d\}}^2$$
2) The rank $k$ matrix attained by keeping the first $k$ left singular vectors, right singular vectors, and singular values of the SVD of ${\bf X}$ attains this minimum.  Specifically, if $\vec{u}_1,\dots,\vec{u}_k$ are the first $k$ left singular vectors and $\vec{v}_1,\dots,\vec{v}_k$ are the first $k$ right singular vectors then 
\begin{equation}
\argmin_{{\bf Z}\in\mathbb{R}^{N\times d} \\ \text{rank}({\bf Z}) = k} \|{\bf X} - {\bf Z}\|_F^2 = 
\begin{bmatrix}&& \\ \vec{u}_1 & \dots & \vec{u}_k \\ && \end{bmatrix}
\begin{bmatrix}\sigma_1 & &  \\
 & \ddots &  \\
 &  & \sigma_k \end{bmatrix}
\begin{bmatrix}
&\vec{v}_1^T & \\ & \vdots& \\ &\vec{v}_k^T& \end{bmatrix}
\end{equation}
:::

There are several important implications of this theorem.  First, the direct result indicates that computing the SVD of ${\bf X}$ immediately allows us to compute the best approximation under Frobenius loss for a specified rank $k$.  In practice, the full SVD is not required since we will typically consider the case where $k <\min\{N,d\}$.  There is a another implication as well.  In cases where a specific choice of $k$ is not clear, the singular values of ${\bf X}$ provide  a method to comparing different choices of $k$.  Akin to the scree plot, we can plot the (squared) singular values to look for clear separation or alternatively, plot the ratio $$\frac{\sum_{j=1}^k\sigma_j^2}{\sum_{j=1}^{\min\{N,d\}}\sigma_j^2}$$ as a function of $k$ to understand the relative error for a specific choice of $k$.  

For a given choice of $k$, we now approximate our original data by linear combination of the right singular vectors $\vec{v}_1,\dots,\vec{v}_k$.  The approximations are $$\vec{x}_i \approx \vec{z}_i = \sum_{j=1}^k \sigma_j{\bf U}_{ij} \vec{v}_j$$.

### Connections with PCA

Suppose that we were to compute the full SVD of the centered data matrix $${\bf HX}= {\bf USV}^T.$$ We can express the sample covariance matrix of the original data using the SVD as
\begin{equation}
\hat{\Sigma}_X = \frac{1}{N} ({\bf HX})^T ({\bf HX}) = \frac{1}{N} {\bf VS}^T{\bf U}^T{\bf U SV} ^T = {\bf V}\left(\frac{1}{N} {\bf S}^T{\bf S}\right) {\bf V}^T.
\end{equation}
The matrix $\frac{1}{N}{\bf S}^T {\bf S} \in \mathbb{R}^{d\times d}$ is diagonal with entries $\sigma_1^2/N \ge \dots \ge \sigma_d^2/N.$ Furthermore, the matrix ${\bf V}$ is orthonormal.  Thus, from the SVD of ${\bf HX}$ we can immediately compute the spectral decomposition of $\hat{\Sigma}_X$ to attain the principal component variances and loadings.  In fact, the principal component loadings are the right singular vectors of ${\bf HX}$ whereas the principal component variances are the squared singular values divided by $N$, e.g. $\lambda_j = \sigma_j^2/N$. Using this observation,
$${\bf HX} = {\bf USV}^T \rightarrow {\bf HXV} = {\bf US}$$
from which we may conclude the principal component scores are equal to ${\bf US}.$  This connection is the basis for most numerical implementation of PCA since it is more both faster and more numerically stable  to compute the SVD of ${\bf HX}$ than to compute both $\hat{\Sigma}_X$ and its eigendecomposition!  Thus, computing the best rank $k$ approximation to a centered data matrix is equivalent to the best approximation of the centered data using the first $k$ PC scores.    

However, using the SVD to compute a low rank approximation to a *non-centered* data matrix will give a different result than PCA since the SVD of ${\bf HX}$ will be different than the SVD of ${\bf X}$.  Unlike PCA, which decomposes variability in directions relative to the center of the data, SVD learns an orthonormal basis which decomposes variability relative to the origin.  Only when the data is centered (so its mean is the origin) do SVD and PCA coincide.  Nonetheless, SVD has similar weaknesses to PCA including a sensitivity to scaling and outliers and an inability to detect nonlinear structure.

SVD can provide one final note of insight regarding PCA. Suppose that $N < d$, which is to say that we have fewer samples than the dimensionality of our data.  After centering, the matrix ${\bf HX}$ will have rank most $N-1$.  (Centering reduces the maximum possible rank from $N$ to $N-1$).  The SVD of ${\bf HX}$ will have at most $d-1$ non-zero singular values. Thus, $\hat{\Sigma}_X$ will have at most $N-1$ non-zero PC variances and we can conclude that our data reside on a hyperplane of dimension $N-1$ (possibly lower if ${\bf HX}$ has rank less than $N-1$).  Since $N-1 < d$, we are guaranteed to find a lower-dimensional representation of our data!  However, this conclusion should be viewed cautiously.  Should additional samples be drawn, can we conclude that they would also be constrained to the same hyperplane learned using the first $N$ samples?


### Recommender Systems

SVD may also be applied to association rule learning which can identify similar items in a datasets based on partial observations.  As a motivating example, consider the case where we have a dataset of user provided ratings of products, which could be items purchased, songs listed to, or movies watched.  In this case, ${\bf X}_{ij}$ indicates user $i$s rating of item $j$.  Typically, most of the entries of ${\bf X}$ will be NA since users have likely interacted with a small number of items.  Using a variant of SVD, a simple recommendation system proceeds in two steps. First, we can impute the missing ratings. We can then use this result to infer similar movies which can be used for recommendation.

#### Imputation

Let ${\bf X}\in\mathbb{R}^{N\times d}$ be the data matrix of user ratings with rows corresponding to user and columns to items and $$\mathcal{I} =\{ij \, \text{ s.t. } {\bf X}_{ij} \ne NA\}$$ be the set of all indices of ${\bf X}$ for which we have observed ratings.  For any approximating matrix $\tilde{\bf X}\in\mathbb{R}^{N\times d}$, we may define a `Frobenius'-like error as $$\mathbb{L}({\bf X},\tilde{\bf X}) = \sum_{ij \in \mathcal{I}} ({\bf X}_{ij}-\tilde{\bf X}_{ij})^2$$
which is the sum-squared error over all observations.  Using this definition of loss, here's a simple algorithm for imputing the missing entries of ${\bf X}$ using low rank approximations of a pre-specified rank $k$.

0) We initialize the matrix $\tilde{\bf X}$ of imputed entries by taking $$\tilde{X}_{ij}= \begin{cases} X_{ij} & ij \in \mathcal{I} \\ 0 & ij \ne \mathcal{I} \end{cases}.$$ Now let's use the SVD of $\tilde{\bf X}$ to compute a rank $k$ approximation, ${\bf X}^{(k)}$. Update our imputed matrix $\tilde{\bf X}= \tilde{\bf X}^{(k)}$ and compute the error $$\ell = \mathbb{L}({\bf X},\tilde{\bf X})$$.  The low-rank approximation will distort all entries of $\tilde{\bf X}$ so that $\ell > 0$. We now repeat the following two steps.

1) Overwrite the entries of $\tilde{\bf X}$ corresponding to observations in ${\bf X}$, e.g. for all $ij \in \mathcal{I}$, set $\tilde{\bf X}_{ij} = {\bf X}_{ij}$. The entries corresponding to missing observations generated by the low-rank approximation are kept unchanged.  Now recompute the SVD of $\tilde{\bf X}$ to find the rank $k$ approximating matrix $\tilde{\bf X}^{(k)}$. Update our imputed matrix using the low-rank approximation so that $\tilde{\bf X} = {\bf X}^{(k)}$ and recompute the error $\ell^* = \mathbb{L}({\bf X},\tilde{\bf X}).$

2) If $\ell^* < \ell$ and $|\ell - \ell^*|/\ell > \epsilon$ then set $\ell = \ell^*$  and return to step (1).  Else stop the algorithm and we use matrix  $\tilde{\bf X}$ as our matrix of imputed values.


In summary, after initialization, we are continually overwriting the entries of our matrix of imputed values corresponding to observations then applyin a low-rank approximation.  We stop the algorithm when the error stops decreasing or when the relative decrease in error is less than a specified threshhold $\epsilon.$ In addition to the rank $k$ and the stopping threshhold $\epsilon$ there is one other important `tuning' parameter, the initialization.  In the brief description above, we used a standard of 0, but one could also use the average of all entries in the corresponding column (average item rating) or row (average rating given by a user) or some other specification.  Many more complicated recommendation systems include user and item specific initializations and adjustments but still imploy a low rank approximation somewhere in their deployment.

Below, we should an application of this algorithm to the (https://grouplens.org/datasets/movielens/10m/)[Movielens 10m dataset] containing 10 million ratings from 72,000 viewers for 10,000 movies. In this implementation, we use the Fast Truncated Singular Value Decompositio

```{r pull_movielens, echo = FALSE, message=FALSE}
library("tidyverse")
library("data.table")
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t",
                             readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>%
  mutate(movieId = as.numeric(movieId),
         title = as.character(title),
         genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
rm(movies,ratings,dl)
ratings <- as.matrix(pivot_wider(movielens, 
                             id_cols = "userId", 
                             names_from = "movieId", 
                             values_from = "rating"))
```

```{r movielens_impute}
library("irlba")
initialize <- function(mat){
# get column means ignoring NAs
ave.rat <- colMeans(mat,na.rm = TRUE)
# fill NAs by average movie rating
for(j in 1:ncol(mat)){
  mat[is.na(mat[,j]),j] <- ave.rat[j]
}
return(mat)
}

maxim <- function(mat,k){
# temp <- svd(mat)
temp<- irlba(mat, nv = k)
return(list(U = temp$u[,1:k],
            D = temp$d[1:k],
            V = temp$v[,1:k],
            mat.hat = temp$u[,1:k] %*% diag(temp$d[1:k]) %*% t(temp$v[,1:k])))
}

recommender <- function(mat, num_steps, k){
# initialize loss function tracking
loss <- rep(NA,num_steps)
# run EM algorithm and save loss
ind.known <- !is.na(mat)
mat2 <- initialize(mat)
for (j in 1:num_steps){
  mat2 <- maxim(mat2,k)$mat.hat
  loss[j] <- sum((mat2[ind.known] - mat[ind.known])^2)
  mat2[ind.known] <- mat[ind.known]
}
return(list(loss= loss, fit = mat2))
}

k <- 3
temp <- recommender(ratings,num_steps = 200, k = k)
plot(temp$loss, xlab = "Step", ylab = expression(ell))
```

#### Recommendation
