[["index.html", "An Introduction to Unsupervised Learning Chapter 1 Introduction ", " An Introduction to Unsupervised Learning Alex Young Version 0 Chapter 1 Introduction "],["prerequisites.html", "1.1 Prerequisites", " 1.1 Prerequisites "],["probability-review.html", "Chapter 2 Probability Review ", " Chapter 2 Probability Review "],["important-notation.html", "2.1 Important notation", " 2.1 Important notation Throughout this text, we will be working with vectors and matrices quite often so we begin with a bit of notation and a few important conventions we will adopt hereafter. We’ll use notation \\(\\vec{x}\\in\\mathbb{R}^d\\) to denote a \\(d\\)-dimensional vector. Importantly, we adopt the convention that vectors are columns vectors by default so that \\[\\vec{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}\\] where \\(x_1,\\dots,x_d\\) are the entries or coordinates of vector \\(\\vec{x}\\). Row vectors are then the transpose of column vectors so that \\(\\vec{x}^T = (x_1,\\dots, x_d)\\). When needed we’ll let \\(\\vec{0}\\) denote a vector of all zeros and \\(\\vec{1}\\) a vector of all ones with the dimensionality defined implicitly, e.g. if \\(\\vec{x}\\in\\mathbb{R}^d\\) then in the expression \\(\\vec{x} + \\vec{1}\\), you may interpret \\(\\vec{1}\\in\\mathbb{R}^d\\) so the summation is well defined. Matrices will be denoted in bold so that \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\) denotes an \\(m\\times n\\) matrix with real entries. Subscripts are read as row,column so that \\({\\bf A}_{ij}\\) is the entry of \\({\\bf A}\\) in the \\(i\\)th row and \\(j\\)th column. A superscript \\(T\\) denotes the transpose of a matrix. For a square matrix \\({\\bf B}\\in \\mathbb{R}^{n\\times n}\\), we use notation \\(Tr({\\bf B})\\) to denote the trace of \\({\\bf B}\\) and \\(det({\\bf B}) = |{\\bf B}|\\) to denotes its determinant. Using this above notation, we may also define the inner product and outer product of two vectors. For vectors \\(\\vec{x}\\) and \\(\\vec{y}\\), the inner product or dot product of \\(\\vec{x}\\) and \\(\\vec{y}\\) is the scalar \\(\\vec{x}^T \\vec{y} = \\sum_{i=1}^d x_i y_i\\). Alternatively, we may also consider the outer product \\(\\vec{x}\\vec{y}^T\\) which is a matrix such that \\((\\vec{x} \\vec{y}^T)_{ij} = x_i y_j.\\) For the inner product to be well defined \\(\\vec{x}\\) and \\(\\vec{y}\\) must have the same dimension. This is not the case for the outer product. If \\(\\vec{x}\\in\\mathbb{R}^m\\) and \\(\\vec{y}\\in\\mathbb{R}^n\\) then \\(\\vec{x} \\vec{y}^T \\in \\mathbb{R}^{m\\times n}.\\) If we view a \\(d\\)-dimensional vector as a \\(d\\times 1\\) matrix, then both of these algebraic computations are completely consistent with standard matrix multiplication which we will revisit near the end of this chapter. Let \\(f:\\mathbb{R}^d\\to \\mathbb{R}\\) be a function of \\(d\\)-dimensional vector \\(\\vec{x}\\). Then we define the gradient of \\(f\\) with respect to \\(\\vec{x}\\), denoted \\(\\nabla f\\), to be the \\(d\\)-dimensional vector of partial deriviates of \\(f\\) with respect to the coordinates of \\(\\vec{x}\\) so that \\[\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{ \\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_d} \\end{bmatrix}.\\] The Hessian matrix of \\(f\\) with respect to \\(\\vec{x}\\), denoted as \\(\\mathcal{H}f\\), is the \\(d\\times d\\) matrix of second order partial derivatives of \\(f\\) with respect to the coordinates of \\(\\vec{x}\\) so that \\((\\mathcal{H}f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\). If we are considering a function of multiple vector valued variables, e.g. \\(f(\\vec{x},\\vec{y},\\vec{z})\\) then we use \\(\\nabla_{\\vec{x}}\\) to denote the gradient of \\(f\\) w.r.t. the vector \\(\\vec{x}\\) only. "],["random-vectors-in-mathbbrd.html", "2.2 Random vectors in \\(\\mathbb{R}^d\\)", " 2.2 Random vectors in \\(\\mathbb{R}^d\\) Throughout this text, we will consider independent, identically distributed (iid) samples of a \\(d\\)-dimensional random vector \\(\\vec{x} = (x_1,\\dots,x_d)^T\\). Each coordinate \\(x_i\\) is a random variable and we may view the distribution of the random vector \\(\\vec{x}\\) as the joint distribution of all of its coordinates. The cumulative distribution function of \\(\\vec{x}\\) is then \\[F(\\vec{x} \\le \\vec{x}^o\\,) = P(\\vec{x} \\le \\vec{x}^o\\,) = P(x_1\\le x_1^o, \\dots ,x_d \\le x_d^o\\,).\\] We’ll largely consider continuous entries so we can rewrite the above in terms of the joint density, \\(f: \\mathbb{R}^d \\to [0,\\infty)\\), of \\(\\vec{x}\\) such that \\[F(\\vec{x} \\le \\vec{x}^o\\,) = \\int_{-\\infty}^{x_1^o}\\dots\\int_{-\\infty}^{x_d^o} f(x_1,\\dots,x_d) dx_d\\dots dx_1.\\] To simplify notation, we’ll often write the above as \\[\\int_{-\\infty}^{x_1^o}\\dots\\int_{-\\infty}^{x_d^o} f(x_1,\\dots,x_d) dx_d\\dots dx_1 = \\int_{-\\infty}^{\\vec{x}^0} f(\\vec{x})d\\vec{x}.\\] In the case that \\(\\vec{x}\\) may only take one of a countable set of outcomes, one can replace the integral above with a corresponding summation. Generally speaking we will be considering data drawn from an unknown distribution. However, considering known cases which we can analyze and sample from is often helpful to study how different algorithms perform. With this idea in mind, let’s define a few different distributions which we will revisit throughout this chapter as examples. In each case, we will also provide scatterplots of independent samples from these distributions so that you can visualize the distributions more directly. Definition 2.1 (Multivariate Gaussian Distribution) The multivariate Gaussian distribution in \\(\\mathbb{R}^d\\) with mean \\(\\vec{\\mu} \\in \\mathbb{R}^d\\) and symmetric positive definite covariance matrix \\({\\bf \\Sigma} \\in \\mathbb{R}^{d\\times d}\\) is the random \\(d\\)-dimensional vector \\(\\vec{x}\\) with probability density function \\[f(\\vec{x}) = \\frac{1}{(2\\pi)^{d/2} det({\\bf \\Sigma})^{1/2}}\\exp\\left(-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T{\\bf \\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})\\right).\\] We use shorthand \\(\\vec{x}\\sim \\mathcal{N}(\\vec{\\mu},{\\bf \\Sigma})\\) to indicate \\(\\vec{x}\\) follows this distribution. The Multivariate Gaussian distribution is also often called the Multivariate Normal (MVN) distribution. For example of the MVN, first consider the two-dimensional case with \\(\\vec{\\mu} = \\vec{0}\\) and \\[{\\bf \\Sigma} = \\begin{bmatrix}1 &amp; p \\\\ p &amp; 1\\end{bmatrix}.\\] Below, we show scatterplots of \\(1000\\) independent samples from this distribution for three different values of \\(p.\\) We will also refer to a collection of points in \\(\\mathbb{R}^d\\) as a point cloud. For an examples in \\(\\mathbb{R}^3\\) we again consider case where \\(\\vec{\\mu}=0\\) and let \\[{\\bf \\Sigma} = \\begin{bmatrix}1 &amp; p &amp; p^2 \\\\ p &amp; 1 &amp;p \\\\ p^2 &amp;p &amp; 1 \\end{bmatrix}.\\] In the preceding examples, different choices of \\(p\\), hence different covariance matrices, resulted in point clouds with different orientations and shapes. Later, we’ll discuss how we can determine the shape and orientation from the covariance matrix with the aid of linear algebra. What about changes to \\(\\vec{\\mu}\\)? Changing \\(\\vec{\\mu}\\) translates the point cloud. If in the preceding examples, we had taken \\(\\vec{\\mu}=\\vec{1}\\) the scatterplots would have had the same shape and orientation, but they would have been tranlated by a shift of \\(\\vec{1}.\\) Definition 2.2 (Multivariate t Distribution) The multivariate t-distribution on \\(\\mathbb{R}^d\\) with location vector \\(\\vec{\\mu}\\in\\mathbb{R}^d\\), positive definite scale matrix \\({\\bf \\Sigma}\\in \\mathbb{R}^{d\\times d}\\) and degrees of freedom \\(\\nu\\) has density \\[f(\\vec{x}) = \\frac{\\Gamma\\left(\\frac{\\nu+d}{2}\\right)}{\\Gamma(\\nu/2)\\nu^{d/2}\\pi^{d/2}|{\\bf \\Sigma}|^{1/2}}\\left[1 + \\frac{1}{\\nu}(\\vec{x}-\\vec{\\mu})^T{\\bf \\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})\\right]^{-(\\nu +d)/2}.\\] We use shorthand \\(\\vec{x}\\sim t_\\nu(\\vec{\\mu},{\\bf \\Sigma})\\) to indicate \\(\\vec{x}\\) follows this distribution. We’ll only consider a three dimensional case where the location, which determines the mode of the distribution, is \\(\\vec{0}\\) and the scale is the identity matrix. As in the Gaussian case, changing \\(\\vec{\\mu}\\) translates the point cloud and different values of \\({\\bf \\Sigma}\\) give point clouds with different shapes. The remaining parameter to consider here is the degrees of freedom, \\(\\nu\\), which controls how spread out the samples can be. We show results for three different choices of the degrees of freedom. For smaller degrees of freedom, there are more points which are far from the mode at \\(\\vec{0}\\). "],["expectation-mean-and-covariance.html", "2.3 Expectation, Mean, and Covariance", " 2.3 Expectation, Mean, and Covariance As in the one-dimensional case, the cumulative distribution function determines the distribution of the random vector, and using the density we may establish a few important quantities which will appear often throughout this text. The first is the mean or expected value of the random vector which is the vector of expected values of each entry so that \\[\\begin{equation} E[\\vec{x}] = \\int_{\\mathbb{R}^d}\\vec{x} f(\\vec{x})d\\vec{x} = \\begin{bmatrix} E[x_1] \\\\ \\vdots \\\\ E[x_d] \\end{bmatrix} = \\begin{bmatrix} \\int_{\\mathbb{R}^d} x_1 f(\\vec{x})d\\vec{x} \\\\ \\vdots \\\\ \\int_{\\mathbb{R}^d} x_d f(\\vec{x})d\\vec{x} \\end{bmatrix} \\tag{2.1} \\end{equation}\\] where \\[\\int_{\\mathbb{R}^d} x_i f(\\vec{x})d\\vec{x} = \\int_{-\\infty}^\\infty \\dots \\int_{-\\infty}^\\infty x_i f(x_1,\\dots,x_d)dx_1 \\dots dx_d.\\] Note, we are assuming each of the integrals in (2.1) is well defined, which is a convention we adhere to throughout this text. Often, we’ll often use \\(\\vec{\\mu}\\) to denote the mean vector. When we are considering more than multiple random vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) we will add a corresponding subscript \\(\\vec{\\mu}_\\vec{x}\\) to denote the corresponding mean of \\(\\vec{x}\\). The linearity of expectation for univariate random vectors holds here as well. If \\(\\vec{x}\\in\\mathbb{R}^d\\) is a random vector, \\({\\bf A}\\in\\mathbb{R}^{k\\times d}\\) is a matrix of constant entries, and \\(\\vec{b}\\in\\mathbb{R}^k\\) is a vector of constant entries then \\[E[{\\bf A}\\vec{x} + \\vec{b}] = {\\bf A}\\vec{\\mu} + \\vec{b}.\\] Importantly, for non-squared matrices \\({\\bf A}\\) then mean of \\({\\bf A}\\vec{x}\\) will be of a different dimension than \\(\\vec{x}.\\) In general, the coordinates of a random vector will not be independent. To quantify the pairwise dependence, we could consider the covariance \\[Cov(x_i,x_j) = E[(x_i - \\mu_i)(x_j -\\mu_j)] = \\int_\\mathbb{R}^d (x_i-\\mu_i)(x_j-\\mu_j) f(\\vec{x})d\\vec{x}\\] for \\(1\\le i,j \\le d\\). In the case \\(i=j\\), this simplifies to \\(Cov(x_i,x_i) = Var(x_i)\\). Importantly, we do not want to consider each all of the pairwise covariance separately. Instead, we can organize them as a \\(d\\times d\\) matrix \\({\\bf \\Sigma}\\) with entries \\({\\bf \\Sigma}_{ij} = Cov(x_i,x_j)\\). Hereafter, we will refer to \\({\\bf \\Sigma}\\) as the covariance matrix of \\(\\vec{x}.\\) When we are considering multiple random vectors we will use subscripts so that \\({\\bf \\Sigma}_{\\vec{x}}\\) and \\({\\bf \\Sigma}_{\\vec{y}}\\) denote the covariance matrices or random vectors \\({\\vec{x}}\\) and \\({\\vec{y}}\\) respectively. Following the notational conventions, it follows that \\(\\vec{x} - E[\\vec{x}] = \\vec{x} - \\vec{\\mu} \\in \\mathbb{R}^d\\) so that the outer product of \\(\\vec{x} - \\vec{\\mu}\\) with itself is the \\(d\\times d\\) matrix with entries \\[[(\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T]_{ij} = (x_i-\\mu_i)(x_j-\\mu_j)\\] so that we may more compactly write \\[\\begin{equation} \\text{Var}(\\vec{x}) = E\\left[(\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T\\right] \\tag{2.2} \\end{equation}\\] where we interpret the expectation operation as applying to each entry of the matrix \\((\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T\\). This looks very similar to the univariate case save that we must be mindful of the multidimensional nature of our random vector. In fact with some algebra, we have the following alternative formula for the covariance matrix \\[{\\bf \\Sigma} = E[\\vec{x}\\,\\vec{x}^T] -\\vec{\\mu}\\vec{\\mu}^T\\] which is again reminiscent of the univariate case. Showing this result is left as a short exercise. One brief note to avoid confusion. Other texts refer to \\(\\text{Var}(\\vec{x})\\) as the variance matrix or variance-covariance matrix. Herein, we use the term covariance matrix for \\(\\text{Var}(\\vec{x}).\\) Recall the univariate case, \\[\\text{Var}(aX+b) = a^2\\text{Var}(X)\\] for constants \\(a\\) and \\(b\\) and (one-dimensional) random variable \\(X\\). Similar to the univariate case, there is a formula for the covariance of an affine mapping of a random vector, but the specific form requires us to be mindful of the matrix structure of the covariance matrix. For random vector \\(\\vec{x}\\in\\mathbb{R}^d\\), constant matrix \\({\\bf A}\\in\\mathbb{R}^{k\\times d}\\) and constant vector \\(\\vec{b}\\in\\mathbb{R}^k\\), it follows (see exercises) that \\[{\\bf \\Sigma}_{{\\bf A}\\vec{x}+\\vec{b}} = {\\bf A \\Sigma A}^T.\\] Importantly, note that \\({\\bf A \\Sigma A}^T\\) is a \\(k\\times k\\) matrix which is consistent with the fact that \\({\\bf A}\\vec{y}+\\vec{b}\\) is a \\(k\\)-dimensional vector. Example 2.1 (Mean and Covariance of MVN) If \\(\\vec{x} \\sim \\mathcal{N}(\\vec{\\mu}, {\\bf \\Sigma})\\) then \\(E[\\vec{x}] = \\vec{\\mu}\\) and \\(\\text{Var}(\\vec{x}) = {\\bf \\Sigma}\\) Example 2.2 (Mean and Covariance of Multivariate t-distribution) Let \\(\\vec{x} \\sim t_\\nu(\\vec{\\mu}, {\\bf \\Sigma})\\). If \\(\\nu &gt; 1\\) then \\(E[\\vec{x}] = \\vec{\\mu}\\); otherwise the mean does not exist. If \\(\\nu &gt; 2\\), then \\(\\text{Var}(\\vec{x}) = \\frac{\\nu}{\\nu-2}{\\bf \\Sigma}\\); otherwise, the covariance matrix does not exist. Verifying these examples is left to the exercises and rely on multivariate change of variables which are not covered here. 2.3.1 Sample Mean and Sample Covariance In many cases, we’ll consider a collection of \\(N\\) \\(iid\\) vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N \\in \\mathbb{R}\\). Again, subscripts are used here, but importantly when accompanied by the \\(\\vec{\\cdot}\\) sign a subscript does not refer to a specific coordinate of a vector but rather one vector in a set. Given iid observations \\(\\vec{x}_1,\\dots,\\vec{x}_N\\), we will use sample averages to estimate the expectation and covariance of the data generating distribution. We’ll use bars to denote sample averages so that \\(\\bar{x}\\) denotes the sample mean and \\(\\bar{\\bf \\Sigma}\\) the sample covariance. In this case, we have \\[\\begin{equation} \\bar{x} = \\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i. \\tag{2.3} \\end{equation}\\] Similarly, we define the sample covariance matrix to be \\[\\begin{equation} {\\bf \\bar{\\Sigma}} = \\frac{1}{N} \\sum_{i=1}^N (\\vec{x}_i - \\bar{x})(\\vec{x}_i - \\bar{X})^T = \\left(\\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i\\vec{x}_i^T\\right) - \\bar{x}\\bar{x}^T \\tag{2.4} \\end{equation}\\] In (2.4), dividing by \\(N\\) rather than \\(N-1\\) yields biased estimates of the terms of the sample covariance matrix. However, the final formula in (2.4) more directly matches the corresponding term in the definition of the covariance matrix. Had we used a factor of \\(1/(N-1)\\) instead, we would have \\[\\frac{1}{N-1}\\sum_{i=1}^N (\\vec{x}_i - \\bar{X})(\\vec{x}_i - \\bar{X})^T = \\left(\\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i\\vec{x}_i^T\\right) - \\frac{N}{N-1}\\bar{x}\\bar{x}^T\\] which is slightly more cumbersome. In the examples we will consider, \\(N\\) will typically be large enough so that the numerical difference is small. As such, we will opt for algebraically convenient definition form of @(eq:def-sample-covariance) as our definition of the sample covariance matrix. Alternatively, we can view the sample mean and sample covariance as the mean and covariance (using expectation rather than averages) of the empirical distribution from a collection of samples \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) defined below. Definition 2.3 (Empirical Distribution) Given a finite set of points \\(\\mathcal{X}=\\{\\vec{x}_1,\\dots,\\vec{x}_N\\} \\subset \\mathbb{R}^d\\), we say that random vector \\(\\vec{z}\\) follows the empirical distribution from data \\(\\mathcal{X}\\) if \\[P(\\vec{z} = \\vec{x}_i) = \\frac{1}{N}, \\quad i = 1,\\dots, N\\] and is zero otherwise. If \\(\\vec{z}\\) follows the empirical distribution on a set of \\(N\\) points \\(\\mathcal{X}= \\{\\vec{x}_1,\\dots,\\vec{x}_N\\}\\), then the expectation and covariance matrix of \\(\\vec{z}\\) are equivalent to the sample mean and sample covariance for data \\(\\vec{x}_1,\\dots,\\vec{x}_N.\\) 2.3.2 The Data Matrix Both (2.3) and (2.4) involve summations. Working with sums will prove cumbersome, so briefly let us introduce a more compact method for representing these expressions. Hereafter, we will organize the vectors \\(\\vec{x}_1,\\dots, \\vec{x}_N\\) into a data matrix \\[{\\bf X} = \\begin{bmatrix} \\vec{x}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T\\end{bmatrix} \\in \\mathbb{R}^{N\\times d}.\\] In this setup, \\({\\bf X}_{ij}\\) is the \\(j\\)th coordinate of \\(\\vec{x}_i\\), or equivalently, the \\(j\\)th measurement taken from the \\(i\\)th subject. Thus, rows of \\({\\bf X}\\) index subjects (realizations of the random vector) whereas columns index common measurements across all subjects. Using the data matrix, we can forgo the summation notation giving the following formulas for the sample mean \\[\\begin{equation} \\bar{x} = \\frac{1}{N} {\\bf X}^T \\vec{1} \\tag{2.5} \\end{equation}\\] and the sample covariace matrix \\[\\begin{equation} {\\bf \\bar{\\Sigma}} = \\frac{1}{N} ({\\bf HX})^T {\\bf HX} = \\frac{1}{N} {\\bf X}^T {\\bf H X} \\tag{2.6} \\end{equation}\\] where \\({\\bf H} = {\\bf I} - \\frac{1}{N} \\vec{1} \\vec{1}^T \\in \\mathbb{R}^{N\\times N}\\) is known as the centering matrix. We have used the fact that \\({\\bf H}\\) is symmetric and idempotent, e.g. \\({\\bf H}^2 = {\\bf H}\\) which is left as a exercise. The vector \\(\\vec{1}\\) is the \\(N\\)-dimensional vector with 1 in each entry and \\({\\bf I}\\) is the \\(N\\times N\\) identity matrix. One can show (see exercises) that the matrix-vector and matrix-matrix multiplication implicitly handles the summations in (2.3) and (2.4). To conclude this section, we compare the sample mean and covariance matrix computed from random draws from the \\(MVN\\) distribution. Example 2.3 (Draws from the MVN) We draw \\(N=100\\) samples from the \\(\\mathcal{N}(\\vec{0}, {\\bf \\Sigma})\\) distribution where \\[{\\bf \\Sigma}=\\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp;4 &amp; 0 \\\\ 0&amp;0&amp;9\\end{bmatrix}\\]. N &lt;- 100 X &lt;- mvrnorm(n=N, mu = rep(0,3), Sigma = c(1,4,9)*diag(3)) To compute the sample mean and sample covariance, we implement (2.3) and (2.4). xbar &lt;- (1/N) * t(X) %*% rep(1,N) H &lt;- diag(N) - (1/N)*matrix(1,nrow = N, ncol = N) S &lt;- (1/N) * t(H %*% X) %*% (H %*% X) The results are shown below (rounded to three decimal places) \\[\\bar{x} = \\begin{bmatrix} 0.008 \\\\ 0.078 \\\\ -0.239 \\end{bmatrix} \\qquad \\text{and} \\qquad \\bar{\\bf \\Sigma} = \\begin{bmatrix} 1.148 &amp; 0.216 &amp; 0.121 \\\\ 0.216 &amp; 4.181 &amp; 0.087 \\\\ 0.121 &amp; 0.087 &amp; 9.366 \\end{bmatrix}\\] which are closer to the true values. If we increase the sample size to \\(N=10^4\\) samples, we get estimates which are closer to the true values (shown below). \\[\\bar{x} = \\begin{bmatrix} 0.001 \\\\ 0.02 \\\\ 0.051 \\end{bmatrix} \\qquad \\text{and} \\qquad \\bar{\\bf \\Sigma} = \\begin{bmatrix} 0.991 &amp; -0.02 &amp; -0.04 \\\\ -0.02 &amp; 3.997 &amp; 0.095 \\\\ -0.04 &amp; 0.095 &amp; 8.992 \\end{bmatrix}\\] "],["linear-algebra.html", "2.4 Linear Algebra", " 2.4 Linear Algebra 2.4.1 Assumed Background This text assumes familiarity with definitions from a standard undergraduate course in linear algebra including but not limited to linear spaces, subspaces, spans and bases, and matrix multiplication. However, we have elected to provide review of some of the most commonly used ideas in the methods we’ll cover in the following subsections. For a more thorough treatment of linear algebra, please see REFERENCES 2.4.2 Interpretations of Matrix Multiplication Throughout this text, comfort with common calculations in linear algebra will be very important. Herein, we assume the reader has some exposure to these materials at an undergraduate level including the summation of vectors or matrices. The familiarity with matrix-vector and matrix-matrix multiplication will play a central role as we have already seen in the case of the data matrix formulation of the sample mean and sample covariance. However, rote familiarity with computation will not be sufficient to build intuition for the methods we’ll discuss. As such, we’ll begin with a review of a few important ways one can view matrix-vector (and matrix-matrix) multiplication which will be helpful later. Those who feel comfortable with the myriad interpretations of matrix multiplication in terms of linear combinations of the rows and columns may skip to the next section. Suppose we have matrix \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\) and vector \\(\\vec{x}\\in\\mathbb{R}^n\\). If we let \\(\\vec{a}_1^T,\\dots, \\vec{a}_M^T\\in\\mathbb{R}^n\\) denote the rows of \\({\\bf A}\\), then the most commonly cited formula for computing \\({\\bf A}\\vec{x}\\) is \\[{\\bf A}\\vec{x} = \\begin{bmatrix} \\vec{a}_1^T \\\\ \\vdots \\\\ \\vec{a}_m^T\\end{bmatrix} \\vec{x} = \\begin{bmatrix}\\vec{a}_1^T \\vec{x} \\\\ \\vdots \\\\ \\vec{a}_m^T\\vec{x}\\end{bmatrix}\\] wherein we take the inner product of the rows of \\({\\bf A}\\) with vector \\(\\vec{x}\\). We can expand this definition to matrix-matrix multiplication. If \\({\\bf B}\\in\\mathbb{R}^{n\\times k}\\) has columns \\(\\vec{b}_1,\\dots,\\vec{b}_k\\) then \\[({\\bf AB})_{ij} = \\vec{a}_i^T\\vec{b}_j\\] where we take the inner product of the \\(i\\)th row of \\({\\bf A}\\) with the \\(j\\)th column of \\({\\bf B}\\) to get the \\(ij\\)th entry of \\({\\bf AB}.\\) This is perfectly reasonable method of computation, but alternative perspectives are helpful, particularly when we consider different factorization of the data matrix in later chapters.. Returning to \\({\\bf A}\\vec{x}\\), suppose now that \\({\\bf A}\\) has columns \\(\\vec{\\alpha}_1,\\dots,\\vec{\\alpha}_n\\) and \\(\\vec{x} = (x_1,\\dots,x_n)^T\\), then we may view \\({\\bf A}\\vec{x}\\) as a linear combination of the columns of \\({\\bf A}\\) so that \\[{\\bf A}\\vec{x} = \\begin{bmatrix}\\vec{\\alpha}_1 \\,| &amp; \\cdots &amp;|\\, \\vec{\\alpha}_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1\\vec{\\alpha}_1 + \\dots + x_n \\vec{\\alpha}_n = \\sum_{j=1}^n x_j\\vec{\\alpha}_j.\\] Here, we added vertical columns between the vectors \\(\\vec{\\alpha}_i\\) to make clear that \\(\\begin{bmatrix}\\vec{\\alpha}_1 \\,| &amp; \\cdots &amp;|\\, \\vec{\\alpha}_n \\end{bmatrix}\\) is a matrix. We can extend this perspective to see that the columns of \\({\\bf AB}\\) are comprised of different linear combinations of the columns of \\({\\bf A}\\). Specifically, the \\(j\\) column of \\({\\bf AB}\\) is a linear combination of the columns of \\({\\bf A}\\) using the entries in the \\(j\\)th column of \\({\\bf B}\\). More specifically, the \\(j\\)th column of \\({\\bf AB}\\) is the linear combination \\[\\sum_{i=1}^n {\\bf B}_{ij}\\vec{\\alpha}_{i}.\\] Our final observations follows by taking these insights on linear combinations of columns and transposing the entire operation. What can we say about the rows of \\({\\bf AB}\\)? We can rewrite \\({\\bf AB} = ({\\bf B}^T{\\bf A}^T)^T\\). The columns of \\({\\bf B}^T{\\bf A}^T\\) are linear combinations of the columns of \\({\\bf B}^T\\). Since the columns of \\({\\bf B}^T\\) are the rows of \\({\\bf B}\\), it follows that the rows of \\({\\bf AB} = ({\\bf B}^T{\\bf A}^T)^T\\) are linear combinations of the rows of \\({\\bf B}\\) with weights given by the entries in each row of \\({\\bf A}\\) respectively. In mathematical notation, the \\(i\\)th row of \\({\\bf AB}\\) is \\[\\sum_{j=1}^n {\\bf A}_{ij} \\vec{\\beta}_j^T\\] where \\(\\vec{\\beta}_1^T,\\dots, \\vec{\\beta}_n^T\\) are the rows of \\({\\bf B}.\\) 2.4.3 Norms and Distances Throughout this text, we will use \\(\\| \\cdot \\|\\) to denote the usual Euclidean (or \\(\\ell_2\\)) norm, which for a vector, \\(\\vec{x} = (x_1,\\dots,x_d)\\in\\mathbb{R}^d\\), is \\[\\|\\vec{x}\\| = \\left(\\sum_{j=1}^d x_j^2 \\right)^{1/2}.\\] We may then define the Euclidean distance between two \\(d\\)-dimension vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) to be \\[\\|\\vec{x}-\\vec{y}\\| = \\left(\\sum_{j=1}^d (x_j - y_j)^2\\right)^{1/2}.\\] Euclidean distance is the most commonly used notion of distance (or norm or metric) between two vectors, but it is far from the only option. We can consider the general \\(\\ell_p\\) norm \\[\\|\\vec{x}\\|_p = \\left(\\sum_{j=1}^d x_j^p\\right)^{1/p}\\] which coincides with the Euclidean norm for \\(p=2\\). Two other special cases include \\(p=1\\) also known as the Manhattan distance and \\(p = \\infty\\) also known as the sup-norm \\[\\|\\vec{x}\\|_\\infty = \\max_{j=1,\\dots, d} |x_j|.\\] We can also extend this notions of vector norms to a measure of the norm of a matrix. Two important cases are the \\(\\ell_2\\) norm of a matrix and the Frobenius norm. For matrix \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\), this norm is \\[\\begin{equation} \\|{\\bf A}\\| = \\sup_{\\vec{x}\\in\\mathbb{R}^n \\text{ s.t. } \\vec{x}\\ne \\vec{0}} \\frac{\\|{\\bf A}\\vec{x}\\|}{\\|\\vec{x}\\|} = \\sup_{\\vec{x}\\in\\mathbb{R}^n \\text{ s.t. }\\|\\vec{x}\\|=1} \\|{\\bf A}\\vec{x}\\|. \\end{equation}\\] You can interpret \\(\\|{\\bf A}\\|\\) as the largest relative change in the Euclidean length of a vector after it is multiplied by \\({\\bf A}\\). The Frobenius extends the algebraic definition of a matrix to a matrix. For \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\), its Frobenius norm is \\[\\begin{equation} \\|{\\bf A}\\|_F = \\left(\\sum_{i=1}^m\\sum_{j=1}^n {\\bf A}_{ij}^2\\right)^{1/2}. \\end{equation}\\] The \\(\\ell_2\\) distance between two matrices is then \\(\\|{\\bf A}-{\\bf B}\\|\\) and the Frobenius distance between two matrices is \\(\\|{\\bf A} - {\\bf B}\\|\\) where both \\({\\bf A}\\) and \\({\\bf B}\\) have the same number of rows and columns. 2.4.4 Important properties A few additional definitions that we will use throughout the text are provided below without examples. Definition 2.4 (Symmetric Matric) A matrix \\({\\bf A}\\in \\mathbb{R}^{d\\times d}\\) is symmetric if \\({\\bf A} = {\\bf A}^T.\\) ::: {.definition #def-eigen name = “Eigenvectors and Eigenvalues”} Let \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). If there is a scalar \\(\\lambda\\) and vector \\(\\vec{x}\\ne \\vec{0}\\) such that \\({\\bf A}\\vec{x} = \\lambda \\vec{x}\\) then we say \\(\\lambda\\) is an eigenvalue of \\({\\bf A}\\) with associated eigenvector \\(\\vec{x}.\\) ::: Definition 2.5 (Positive definite) Let \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). If \\(\\vec{x}^T{\\bf A}\\vec{x} &gt;0\\) for all \\(\\vec{x}\\ne \\vec{0}\\) then we say \\({\\bf A}\\) is positive definite. If instead, \\(\\vec{x}^T{\\bf A}\\vec{x} \\ge 0\\) for all \\(\\vec{x}\\ne \\vec{0}\\) we say that \\({\\bf A}\\) is positive semi-definite. 2.4.5 Matrix Factorizations Two different matrix factorization will arise many times throughout the text. The first, which is commonly presented in linear algebra courses, is the spectral decomposition of a square matrix which is also known as diagonalization or eigenvalue decomposition. Herein, we assume familiarity with eigenvalues and eigenvectors. The second factorization is the singular value decomposition. In the subsequent subsections, we briefly discuss these two factorizations, their geometric interpretation, and some notation that will typically be used in each case. 2.4.5.1 Eigenvalue Decomposition We begin with the eigenvalue decomposition of a square matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). As you may recall, \\({\\bf A}\\) will have a set of \\(d\\) eigenvalues \\(\\lambda_1,\\dots, \\lambda_d\\) (which may include repeated values) and associated eigenvectors. A number, \\(\\lambda\\), may be repeated in the list of eigenvalues, and the number of times is called the algebraic multiplicity of \\(\\lambda\\). Each eigenvalue has a least one eigenvector. In cases where the eigenvalue has algebraic multiplicity greater than one, we refer to its geometric multiplicity as the number of linearly independent eigenvectors associated with the eigenvalue. The algebraic multiplicity is always greater than or equal to the geometric multiplicity. However, this is not always the case, and when this occurs, the matrix cannot be diagonalized. Fortunately, we will largely be dealing with symmetric matrices for which diagonalization is guaranteed by the following theorem. Theorem 2.1 (Spectral Decomposition Theorem for Symmetric Matrices) Any symmetric matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\) can be written as \\[{\\bf A} = {\\bf U\\Lambda U}^T\\] where \\({\\bf U}\\in\\mathbb{R}^{d\\times d}\\) is an orthonormal matrix and \\({\\bf \\Lambda}\\) is a diagonal matrix \\[{\\bf \\Lambda} = \\begin{bmatrix} \\lambda_1 &amp; 0&amp;0 \\\\ 0&amp; \\ddots &amp;0 \\\\ 0&amp;0 &amp;\\lambda_d\\end{bmatrix}\\] where the scalars \\(\\lambda_1,\\dots,\\lambda_d \\in \\mathbb{R}\\) are the eigenvalues of \\({\\bf A}\\) and the corresponding columns of \\({\\bf U}\\) are their associated eigenvectors. By convention, we will always assume the eigenvalues are in decreasing order so that \\(\\lambda_1\\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d\\). The most common types of symmetric matrices that we will encounter are covariance matrices. In those cases, the spectral decomposition of the covariance can provide some helpful insight about the shape of the associated probability density. We demonstrate this idea graphically in the following examples using the MVN. Example 2.4 (Level curves of MVN in) Consider NOTES: Add example of Gaussian densities in \\(\\mathbb{R}^2\\) with level curves and eigenvectors/values. 2.4.5.2 Singular Value Decomposition The spectral theorem is limited in that it requires a matrix to be both square and symmetric. When focusing on data matrices \\({\\bf X}\\in\\mathbb{R}^{N\\times d}\\) both assumptions are extremely unlikely to be satisfied, and we will need a more flexible class of methods. This idea is explored in much greater detail in Chapter 4. For now, we briefly introduce the Singular Value Decomposition and how this factorization provides some insight on the geometric structure of matrix-vector multiplication. Definition 2.6 (The Singular Value Decomposition (SVD)) Let \\({\\bf A} \\in \\mathbb{R}^{m\\times n}\\) be a rank \\(k\\) matrix. Then \\({\\bf A}\\) may be factored as \\[\\begin{equation} {\\bf A} = \\tilde{\\bf U}\\tilde{\\bf S}\\tilde{\\bf V}^T \\tag{2.7} \\end{equation}\\] where \\(\\tilde{\\bf U}\\in\\mathbb{R}^{m\\times k}\\) and \\(\\tilde{\\bf V}\\in\\mathbb{R}^{k\\times n}\\) have orthonormal columns and \\(\\tilde{\\bf S}\\in\\mathbb{R}^{k\\times k}\\) is a diagonal matrix with real entries \\(\\sigma_1 \\ge \\dots \\sigma_k \\ge 0\\) along the diagonal. We refer to (2.7) as the reduced singular value decomposition of \\({\\bf A}.\\) The columns of \\(\\tilde{\\bf U}\\) (\\(\\tilde{\\bf V}\\)) are called the left (right) singular vectors of \\({\\bf A}\\) and the scalars \\(\\sigma_1\\ge \\dots\\ge \\sigma_k\\) are referred to as the singular values of \\({\\bf A}.\\) Let \\(\\vec{u}_1,\\dots,\\vec{u}_k\\in\\mathbb{R}^m\\) be the columns of \\(\\tilde{\\bf U}\\). When \\(m &gt;k\\), we may find additional vectors \\(\\vec{u}_{k+1},\\dots,\\vec{u}_m\\) such that \\(\\{\\vec{u}_1,\\dots,\\vec{u}_m\\}\\) are an orthonormal basis for \\(\\mathbb{R}^m\\). Similarly, we can apply the same reasoning to \\(\\tilde{\\bf V}\\) to find an orthonormal basis of \\(\\mathbb{R}^n\\) with the first \\(k\\) such vectors corresponding to the right singular vectors of \\({\\bf A}\\). The (full) singular value decomposition of \\({\\bf A}\\) is \\[\\begin{equation} {\\bf A} = {\\bf US\\bf V}^T \\tag{2.8} \\end{equation}\\] where \\[{\\bf U} = \\begin{bmatrix} \\vec{u}_1 &amp; \\dots &amp; \\vec{u}_m\\end{bmatrix}\\in\\mathbb{R}^{m\\times m} \\qquad {\\bf V} = \\begin{bmatrix} \\vec{v}_1 &amp; \\dots &amp; \\vec{v}_n\\end{bmatrix}\\in\\mathbb{R}^{n\\times n}\\] are orthonormal matrices. The matrix \\({\\bf S}\\in\\mathbb{R}^{m\\times n}\\) is formed by taking \\(\\tilde{S}\\) and padding it with zero along the bottom and right so that it is \\(m\\times n\\). When considering matrix muliplication \\({\\bf A}\\vec{x}\\), the full SVD of \\({\\bf A}\\) is helpful for decomposition this matrix multiplication into three more interpretable steps. 2.4.6 Symmetry, Positive Definiteness, and Matrix Powers "],["exercises.html", "2.5 Exercises", " 2.5 Exercises Let \\(f(\\vec{x}) = \\vec{x}^T {\\bf A}\\vec{x}\\) for a vector \\(\\vec{x}\\in \\mathbb{R}^d\\) and a matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\) which is constant. Give expressions for \\(\\nabla f\\) and \\(\\mathcal{H} f\\) using only matrices and vectors (no summation notation is allowed). Given a random vector \\(\\vec{x}\\) with mean \\(\\vec{\\mu}\\) and covariance matrix \\({\\bf \\Sigma} = E[(\\vec{x}-\\vec{\\mu})(\\vec{x}-\\vec{\\mu})^T\\,]\\) verify the identity \\({\\bf \\Sigma} = E[\\,\\vec{x}\\,\\vec{x}^T\\,] - \\vec{\\mu} \\,\\vec{\\mu}^T\\). Suppose \\(\\vec{x}\\in \\mathbb{R}^n\\) is a random vector with covariance \\({\\bf \\Sigma}\\in\\mathbb{R}^{n\\times n}\\), and let \\({\\bf A} \\in \\mathbb{R}^{m\\times n}\\) be a matrix with constant entries (non-random). Show that the covariance matrix of \\({\\bf A}\\vec{x}\\) is \\({\\bf A\\Sigma A}^T.\\) Show that the \\(N\\times N\\) centering matrix \\({\\bf H} = {\\bf I} - \\frac{1}{N}\\mathbb{1}\\mathbb{1}^T\\) is idempotent, i.e. \\({\\bf H}^2 = {\\bf H}.\\) Consider the data matrix \\[{\\bf X}=\\begin{bmatrix}\\vec{x}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T\\end{bmatrix}.\\] Show that \\[{\\bf HX} = \\begin{bmatrix} \\vec{x}_1^T -\\bar{x}^T \\\\ \\vdots \\\\ \\vec{x}_N^T - \\bar{x}^T\\end{bmatrix}\\] where \\({\\bf H}\\) is the centering matrix and \\(\\bar{x}\\) is the sample mean of vectors \\(\\vec{x}_1,\\dots, \\vec{x}_N.\\) Let \\({\\bf A}\\in \\mathbb{R}^{m\\times n}\\). Explain why \\({\\bf A A}^T\\) and \\({\\bf A}^T{\\bf A}\\) are diagonalizable and positive semi-definite. Give expressions for the singular vectors and singular values of \\({\\bf A}\\) in terms of the eigenvectors and eigenvalues of \\({\\bf A A}^T\\) and \\({\\bf A}^T{\\bf A}\\). "],["central-goals-and-assumptions.html", "Chapter 3 Central goals and assumptions ", " Chapter 3 Central goals and assumptions "],["dimension-reduction-and-manifold-learning.html", "3.1 Dimension reduction and manifold learning", " 3.1 Dimension reduction and manifold learning "],["clustering.html", "3.2 Clustering", " 3.2 Clustering "],["generating-synthetic-data.html", "3.3 Generating synthetic data", " 3.3 Generating synthetic data "],["exercises-1.html", "3.4 Exercises", " 3.4 Exercises "],["linear-methods.html", "Chapter 4 Linear Methods", " Chapter 4 Linear Methods "],["sec-pca.html", "4.1 Principal Component Analysis", " 4.1 Principal Component Analysis 4.1.1 Derivation 1: Iterative Projections We begin with a data matrix \\[{\\bf X} = \\begin{bmatrix} \\vec{x}_1^T\\\\ \\vdots \\\\\\vec{x}_N^T\\end{bmatrix} \\in\\mathbb{R}^{N\\times d}.\\] Let’s begin with an example of dimension reduction where we’ll seek to replace each vector \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) with corresponding scalars \\(y_1,\\dots,y_N\\) which preserve as much of the variability between these vectors as possible. To formalize this idea, let’s introduce a few assumptions. First, we’ll assume the data \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) are centered. This is not a requirement, but it will simplify the analysis later. We’ll discuss how to account for this centering step later, but for now assume \\(\\bar{x} = \\vec{0}\\) so that \\({\\bf HX} = {\\bf X}\\). More importantly, let’s assume that each \\(y_i\\) is derived in the same way. Specifically, let \\(y_i = \\vec{x}_i^T \\vec{w}\\) for some common vector \\(\\vec{w}\\). Thus, we can view each one-dimensional representation as a dot product of the corresponding observed vector with the same vector \\(\\vec{w}.\\) We can compactly write this expression as \\[\\vec{y} = \\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_n \\end{bmatrix}=\\begin{bmatrix}\\vec{x}_1^T \\vec{w} \\\\ \\vdots \\\\ \\vec{x}_N^T \\vec{w}\\end{bmatrix} = {\\bf X} \\vec{w}.\\] How do we choose \\(\\vec{w}\\)? We would like differences in the scalars \\(y_1,\\dots,y_N\\) to reflect differences in the vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) so having \\(y_1,\\dots,y_N\\) spread out is a natural goal. Thus, if \\(\\vec{x}_i\\) and \\(\\vec{x}_j\\) are far apart then so will \\(y_i\\) and \\(y_j\\). To do this, we’ll try to maximize the sample variance of the \\(y\\)’s. The sample variance \\[\\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y})^2 = \\frac{1}{N}\\sum_{i=1}^N(\\vec{x}_i^T \\vec{w} - \\bar{y})^2\\] will depend on our choice of \\(\\vec{w}\\). In the previous expression, \\[\\bar{y} = \\frac{1}{N} y_i = \\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i^T \\vec{w} = \\frac{1}{N}\\vec{1}^T{\\bf X}\\vec{w}\\] is the sample mean of \\(y_1,\\dots,y_N.\\) Importantly, since we have assumed that \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) are centered, it follows that \\(\\bar{y}=0\\) and the sample variance of \\(y_1,\\dots,y_N\\) simplifies to \\[\\frac{1}{N}\\sum_{i=1}^N(\\vec{x}_i^T \\vec{w})^2 = \\frac{1}{N}\\sum_{i=1}^N y_i^2 = \\frac{1}{N} \\|y\\|^2 = \\frac{1}{N}\\vec{y}^T\\vec{y}.\\] We can write the above expression more compactly. Using the identity \\(\\vec{y} = {\\bf X}\\vec{w}\\), we want to choose \\(\\vec{w}\\) to maximize \\[\\frac{1}{N}\\vec{y}^T\\vec{y} = \\frac{1}{N}({\\bf X}\\vec{w})^T{\\bf X}\\vec{w} = \\frac{1}{N}\\vec{w}^T{\\bf X}^T{\\bf X}\\vec{w} = \\vec{w}^T\\left(\\frac{{\\bf X}^T{\\bf X}}{N}\\right)\\vec{w}.\\] Since we have assumed that \\({\\bf X}\\) is centered it follows that \\({\\bf X}^T{\\bf X}/N\\) is the sample covariance matrix \\(\\hat{\\bf \\Sigma}\\)! Thus, we want to make \\(\\vec{w}^T\\hat{\\bf \\Sigma} \\vec{w}\\) as large as possible. Naturally, we could increase the entries in \\(\\vec{w}\\) and increase the above expression without bound. To make the maximization problem well posed, we will restrict \\(\\vec{w}\\) to be unit-length under the Euclidean norm so that \\(\\|\\vec{w}\\|=1.\\) We now have a constrained optimization problem which gives rise to the first principal component loading. Definition 4.1 (First PCA Loading and Scores) The first principal component loading is the vector \\(\\vec{w}_1\\) solving the constrained optimization problem \\[\\begin{equation} \\begin{split} \\text{Maximize } &amp;\\vec{w}^T \\hat{\\bf \\Sigma}\\vec{w} \\\\ \\text{subject to constraint } &amp;\\|\\vec{w}\\|=1. \\end{split} \\end{equation}\\] The first principal component scores are the scalars \\(y_i = \\vec{x}_i^T\\vec{w}_1\\) for \\(i=1,\\dots, N\\). To find the first PCA loading we can make use of Lagrange multipliers (see exercises) to show that \\(\\vec{w}_1\\) must also satisfy the equation \\[\\hat{\\bf \\Sigma}\\vec{w}_1 = \\lambda \\vec{w}_1\\] where \\(\\lambda\\) is the Lagrange multiplier. From this expression, we can conclude that the first principal component loading is the unit length eigenvector associated with the largest eigenvalue of the sample covariance matrix \\(\\hat{\\bf \\Sigma}\\) and that the Lagrange multiplier \\(\\lambda\\) is the largest eigenvalue of \\(\\hat{\\bf \\Sigma}\\). In this case, we refer to \\(\\lambda\\) as the first principal component variance. 4.1.1.1 Geometric Interpretation of \\(\\vec{w}_1\\) Since \\(\\|\\vec{w}_1\\| = 1\\) we may interpret this vector as specifying a direction in \\(\\mathbb{R}^d\\). Additionally, we can decompose each of our samples into two pieces: one pointing in the direction specified by \\(\\vec{w}_1\\) and a second portion perpendicular to this direction. Thus, we may write \\[\\vec{x}_i = \\underbrace{\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1}_{parallel} + \\underbrace{(\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1)}_{perpendicular}.\\] By the Pythagorean theorem, \\[\\begin{align*} \\|\\vec{x}_i\\|^2 &amp;= \\| \\vec{w}_1 \\vec{x}_i^T\\vec{w}_1 \\|^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\\\ &amp;= (\\vec{w}_1^T\\vec{x}_i)^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\\\ &amp;= y_i^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\end{align*}\\] for \\(i=1,\\dots,N\\). Averaging over all of samples gives the expression \\[\\frac{1}{N}\\sum_{i=1}^N\\|\\vec{x}_i\\|^2 = \\frac{1}{N}\\sum_{i=1}^N y_i^2 +\\frac{1}{N}\\sum_{i=1}^N \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2.\\] The left-hand side of the above expression is fixed for a given set of data, whereas the first term on the right side is exactly what we sought to maximize when finding the first principal component loading. This quantity is the average squared length of the projection of each sample onto the direction \\(\\vec{w}_1\\). As such, we can view the first principal component loading as the direction in which \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) most greatly varies. Let’s turn to an example in \\(\\mathbb{R}^3\\) to view this. Example 4.1 (Computing the First PCA Loading and Scores) Below, we show a scatterplot of \\(N=1000\\) random points in \\(\\mathbb{R}^3.\\) Notice the oblong shape of the cloud of points. Rotating this image, it is clear that the data varies more in certain directions than in others. We begin by centering the data data &lt;- scale(data, center = TRUE, scale = FALSE) # subtracts mean from each column which appears the same as the previous figure except centered around the origin. To find the principal component scores and loading, let us calculate of the sample covariance matrix. We can use the largest eigenvalue to find the first PCA variance. Its associated eigenvector (unit length) will be the first loading. The sample covariance \\[ \\hat{\\Sigma} = \\begin{bmatrix} 12.06&amp;7.51&amp;3.85 \\\\ 7.51&amp;9.76&amp;9.52 \\\\ 3.85&amp;9.52&amp;15.72 \\\\ \\end{bmatrix} \\] has largest eigenvalue \\(\\lambda = 26.75\\) and associated eigenvector \\((\\)-0.47, -0.58, -0.66\\()^T\\). We can conclude by plotting the first PCA scores below plot(y[,1], rep(0,N), ylab = &#39;&#39;, xlab = &#39;First PCA scores&#39;, main = &#39;&#39;) 4.1.1.2 Additional Principal Components The first PCA loading provides information about the direction in which are data most greatly vary, but it is quite possible that there are still other directions wherein our data still exhibits a lot of variability. In fact, the notion of a first principal component loading, scores, and variance suggests the existence of a second, third, etc. collection of these quantities. To explore these quantities, let’s proceed as follows For each datum, we can remove its component in the direction of \\(\\vec{w}_1\\), and focus on the projection onto the orthogonal complement of \\(\\vec{w}_1\\). Let \\[\\vec{x}_i^{(1)} = \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 = \\vec{x}_i - \\vec{w}_1 y_i\\] denote the portion of \\(\\vec{x}_i\\) which is orthogonal to \\(\\vec{w}_1\\). Here, the superscript \\(^{(1)}\\) indicates we have removed portion of the vector in the direction of the first loading. We can organize the orthogonal components into a new data matrix \\[{\\bf X}^{(1)} = \\begin{bmatrix} \\left(\\vec{x}_1^{(1)}\\right)^T \\\\ \\vdots \\\\ \\left(\\vec{x}_N^{(1)}\\right)^T \\end{bmatrix} = \\begin{bmatrix} \\vec{x}_1^T - \\vec{x}_1^T\\vec{w}_1\\vec{w}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T - \\vec{x}_N^T\\vec{w}_1\\vec{w}_1^T \\end{bmatrix} = {\\bf X} - {\\bf X}\\vec{w}_1\\vec{w}_1^T.\\] Now let’s apply PCA to the updated data matrix \\({\\bf X}^{(1)}\\) from which we get the second principal component loading, denoted \\(\\vec{w}_2\\), the second principal component scores, and the second principal component variance. One can show that the data matrix \\({\\bf X}^{(1)}\\) is centered so that its sample covariance matrix is \\(\\hat{\\bf \\Sigma}^{(1)} = \\frac{1}{N}({\\bf X}^{(1)})^T{\\bf X}^{(1)}.\\) Thus, the second PCA loading, \\(\\vec{w}_2\\), is a unit eigenvector associated with the largest eigenvalue of \\({\\bf \\Sigma}^{(1)}\\). This eigenvalue is the 2nd PCA variance and the 2nd PCA score of \\(\\vec{x}_i\\) is given by the inner product of \\(\\vec{x}_i^{(1)}\\) with \\(\\vec{w}_2\\). Here is one crucial observation. The vector \\(\\vec{w}_2\\) gives the direction of greatest variability of the vectors \\(\\vec{x}_1^{(1)},\\dots,\\vec{x}_N^{(1)}.\\) For each of these vectors we have removed the component in the direction of \\(\\vec{w}_1\\). Thus, \\(\\vec{x}_1^{(1)},\\dots,\\vec{x}_N^{(1)}\\) do not vary at all in the \\(\\vec{w}_1\\) direction. What can we say about \\(\\vec{w}_2\\)? Naturally, it must be perpendicular to \\(\\vec{w}_1\\)! We need not stop at the second PCA loading, scores, and variance. We could remove components in the direction of \\(\\vec{w}_2\\) and apply PCA to the vectors \\[\\begin{align*} \\vec{x}_i^{(2)} &amp;= \\vec{x}_i^{(1)} - \\vec{w}_2 (\\vec{x}_i^{(1)})^T\\vec{w}_2\\\\ &amp;= \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 - \\vec{w}_2(\\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1)^T\\vec{w}_2\\\\ &amp;= \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 - \\vec{w}_2\\vec{x}_i^T\\vec{w}_2 + \\vec{w}_2\\vec{w_1}^T\\vec{x}_i\\underbrace{\\vec{w}_1^T\\vec{w}_2}_{=0} \\end{align*}\\] to obtain a third loading, variance, and set of scores. We can continue repeating this argument \\(d\\) times for our \\(d\\)-dimensional data until we arrive at a set of \\(d\\) unit vectors \\(\\vec{w}_1,\\dots,\\vec{w}_d\\) which are the \\(d\\) PCA loadings. To review, we then have \\(d\\) PCA loadings \\(\\vec{w}_1,\\dots,\\vec{w}_d\\) each with an associated PCA variance \\(\\lambda_1,\\dots,\\lambda_d\\). For each sample, we also have an associated set of PCA scores \\(\\vec{x}_i^T\\vec{w}_1,\\dots,\\vec{x}_i^T\\vec{w}_d\\) which we can organize into a large matrix \\[{\\bf Y} = \\begin{bmatrix} \\vec{x}_1^T\\vec{w}_1 &amp; \\dots &amp; \\vec{x}_1^T\\vec{w} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\vec{x}_N^T\\vec{w} &amp; \\dots &amp; \\vec{x}_N^T\\vec{w}_d \\end{bmatrix} = {\\bf X}\\begin{bmatrix} \\vec{w}_1 \\,| \\dots \\,|\\,\\vec{w}_d\\end{bmatrix}.\\] We formalize this idea in the following Lemma. Lemma 4.1 (Eigenvalues of Covariance Matrix of Projected Data) Suppose vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N\\in\\mathbb{R}^d\\) are centered and have sample covariance matrix \\(\\hat{\\bf \\Sigma}\\). Let \\(\\lambda_1 \\ge \\dots\\ge\\lambda_d \\ge 0\\) denote the eigenvalues of \\(\\hat{\\bf \\Sigma}\\) with associated eigenvectors \\(\\vec{w}_1,\\dots,\\vec{w}_d.\\) Not let \\(\\vec{x}^{(k)}_i = \\vec{x}_i - \\sum_{j=1}^k \\vec{w}_j\\vec{x}_i^T\\vec{w}_j\\) denote the portion of vector \\(\\vec{x}_i\\) which is orthogonal to each of the first \\(k\\) PCA loadings. If \\(\\hat{\\bf \\Sigma}^{(k)}\\) denotes the sample covariance matrix of these vectors it follows that \\(\\hat{\\bf \\Sigma}^{(k)}\\) has eigenvalues \\(\\underbrace{0,\\dots,0}_{k}\\), \\(\\lambda_{k+1}\\ge \\dots \\ge \\lambda_d\\ge 0\\) with associated eigenvectors \\[\\underbrace{\\vec{w}_1,\\dots,\\vec{w}_k,}_{\\text{each with eigenvalue 0}} \\vec{w}_{k+1},\\dots,\\vec{w}_d.\\] Verifying this Lemma is left as an exercise. For each loading, we have a corresponding set of PCA scores and PCA variances. Importantly, since are data are \\(d\\)-dimensional and our loadings are \\(d\\)-mutually orthogonal unit vectors, they define a new basis in \\(\\mathbb{R}^d\\). Let’s continue our example above to see this process in the case where \\(d=3\\). Example 4.2 (Computing the Remaining PCA Loadings and Scores) We can continue this process by also removing, from each vector, its component in the direction of \\(\\vec{w}_2.\\) Finally, we can plot the data in the basis defined by the loadings. This is equivalent to a scatterplot of the scores. plot_ly(data.frame( x = y[,1], y= y[,2], z = y[,3]), x = ~x, y = ~y, z = ~z, marker = list(size = 5)) %&gt;% add_markers() %&gt;% layout(scene = list( xaxis = list(title = &quot;y&lt;sub&gt;1&lt;/sub&gt;&quot;), yaxis = list(title = &quot;y&lt;sub&gt;2&lt;/sub&gt;&quot;), zaxis = list(title = &quot;y&lt;sub&gt;3&lt;/sub&gt;&quot;)), title = &quot;Principal Component Scores&quot; ) In this basis, the data forms an ellipsoid with principal axis directed along the \\(y_1,\\,y_2, \\text{ and }y_3\\) axes. This is a result of the uncorrelated nature of principal component scores. 4.1.2 Derivation 2: Optimal Linear Subspace "],["singular-value-decomposition-1.html", "4.2 Singular Value Decomposition", " 4.2 Singular Value Decomposition yay "],["nonnegative-matrix-factorization.html", "4.3 Nonnegative Matrix Factorization", " 4.3 Nonnegative Matrix Factorization We will continue with the usual setting focusing on an \\(N\\times d\\) data matrix \\({\\bf X}\\). However, we will consider the additional assumption that each entry of the data matrix is non-negative which is a natural feature of many experimental data sets. As before, our goal is to find a low-rank matrix \\(\\hat{\\bf X}\\) which is as close to possible to \\({\\bf X}\\) as possible. How do we measure closness? Here are a few common choices. Frobenius norm \\(\\|{\\bf X}-\\hat{\\bf X}\\|_F.\\) Divergence \\(D({\\bf X} \\| \\hat{\\bf X}) = \\sum_{i=1}^N\\sum_{j=1}^d \\left[{\\bf X}_{ij} \\log \\frac{{\\bf X}_{ij}}{\\hat{\\bf X}_{ij}} + \\hat{\\bf X}_{ij} - {\\bf X}_{ij}\\right]\\) IS Divergence Without any restrictions on \\(\\hat{\\bf X}\\) our previous analysis using SVD provides the answer when we consider the Frobenius norm of the \\(ell_2\\) norm of the difference between \\({\\bf X}\\) and \\(\\hat{\\bf X}.\\) "],["sec-mds.html", "4.4 Multidimensional Scaling", " 4.4 Multidimensional Scaling Introduction and Settings Multidimensional scaling (MDS) is a broad name for a myriad of different methods which are designed to handle a common problem. Suppose there are \\(N\\) objects of interest in a dataset. Examples include a set of geographic locations or cells from mass spectrometry or census blocks, etc. Importantly, we do not need actual data corresponding to each object. In MDS, we instead require a measure the distance or dissimilarity between each pair of objects. We can organize these distances into a matrix \\({\\bf \\Delta}\\in\\mathbb{R}^{N\\times N}\\), with \\({\\bf \\Delta}_{rs}\\) representing the distance/dissimilarity between objects \\(r\\) and \\(s\\). Thus, \\({\\bf \\Delta}_{rr}=0\\) and for now, we may assume that \\({\\bf \\Delta}_{sr} = {\\bf \\Delta}_{rs}\\). The matrix \\({\\bf \\Delta}\\) is often called a Distance or Dissimilarity matrix. In practice, we may construct a distance matrix from observations, but for the purposes of MDS we only require \\({\\bf \\Delta}\\). The primary goal of MDS is to find a set of lower-dimensional points \\(\\vec{y}_1, \\dots, \\vec{y}_N \\in \\mathbb{R}^{t}\\) corresponding to each of the \\(N\\) objects such that the distance between \\(\\vec{y}_r\\) and \\(\\vec{y}_s\\) is close to \\({\\bf \\Delta}_{rs}\\). There are numerous different notions of distance one use in \\(\\mathbb{R}^t\\). Additionally, we may not know the notion of distance/dissimilarity used when computing \\({\\bf \\Delta}\\) or if the dissimilarity corresponds to any well defined notion of distance. Euclidean distance is a common choice for the distance of the lower-dimensional vectors. Fixing this choice still leaves many open questions. Are the original distances Euclidean? If so, can we determine the dimensionality of the original data? MDS also serves as a data visualization method. In this case, \\(t\\) is typically chosen to be either two or three, and the \\(N\\) points in the two-dimensional (or three-dimensional) representation may be plotted so that one can visualize the relationships between data. Before turning to the most common methods of MDS, let’s view a few examples using distances between cities. Example 4.3 (MDS applied to distances between cities) The eurodist R package provides air travel distances between 21 cities in Europe and 10 cities in the US. For the moment, we will focus on the 10 US cities with names and distances given in the following tables Atlanta Chicago Denver Houston LosAngeles Miami NewYork SanFrancisco Seattle Washington.DC Atlanta 0 587 1212 701 1936 604 748 2139 2182 543 Chicago 587 0 920 940 1745 1188 713 1858 1737 597 Denver 1212 920 0 879 831 1726 1631 949 1021 1494 Houston 701 940 879 0 1374 968 1420 1645 1891 1220 LosAngeles 1936 1745 831 1374 0 2339 2451 347 959 2300 Miami 604 1188 1726 968 2339 0 1092 2594 2734 923 NewYork 748 713 1631 1420 2451 1092 0 2571 2408 205 SanFrancisco 2139 1858 949 1645 347 2594 2571 0 678 2442 Seattle 2182 1737 1021 1891 959 2734 2408 678 0 2329 Washington.DC 543 597 1494 1220 2300 923 205 2442 2329 0 After conducting classical MDS (a method of MDS, details will be discussed later), we acquire the plot below. The plot is consistent with the geographical relationships between the cities. Miami and Seattle are the farthest apart in our plot. NY and D.C. are quite close on the plot, which is also true for Los Angeles and San Francisco. Actually, if we rotate the above plot 180 degrees, a crude US map appears impressively. Again, let’s try MDS on the 21 European cities, again applying some reflections of the map given by MDS to help it comply to the conventional orientation of a European map. &gt;&gt;&gt;&gt;&gt;&gt;&gt; fe6c00855e493ca3aa72864411530b42ede78222 Athens Barcelona Brussels Calais Cherbourg Cologne Copenhagen Geneva Gibraltar Hamburg Hook of Holland Lisbon Lyons Madrid Marseilles Milan Munich Paris Rome Stockholm Vienna Athens 0 3313 2963 3175 3339 2762 3276 2610 4485 2977 3030 4532 2753 3949 2865 2282 2179 3000 817 3927 1991 Barcelona 3313 0 1318 1326 1294 1498 2218 803 1172 2018 1490 1305 645 636 521 1014 1365 1033 1460 2868 1802 Brussels 2963 1318 0 204 583 206 966 677 2256 597 172 2084 690 1558 1011 925 747 285 1511 1616 1175 Calais 3175 1326 204 0 460 409 1136 747 2224 714 330 2052 739 1550 1059 1077 977 280 1662 1786 1381 Cherbourg 3339 1294 583 460 0 785 1545 853 2047 1115 731 1827 789 1347 1101 1209 1160 340 1794 2196 1588 Cologne 2762 1498 206 409 785 0 760 1662 2436 460 269 2290 714 1764 1035 911 583 465 1497 1403 937 Copenhagen 3276 2218 966 1136 1545 760 0 1418 3196 460 269 2971 1458 2498 1778 1537 1104 1176 2050 650 1455 Geneva 2610 803 677 747 853 1662 1418 0 1975 1118 895 1936 158 1439 425 328 591 513 995 2068 1019 Gibraltar 4485 1172 2256 2224 2047 2436 3196 1975 0 2897 2428 676 1817 698 1693 2185 2565 1971 2631 3886 2974 Hamburg 2977 2018 597 714 1115 460 460 1118 2897 0 550 2671 1159 2198 1479 1238 805 877 1751 949 1155 Hook of Holland 3030 1490 172 330 731 269 269 895 2428 550 0 2280 863 1730 1183 1098 851 457 1683 1500 1205 Lisbon 4532 1305 2084 2052 1827 2290 2971 1936 676 2671 2280 0 1178 668 1762 2250 2507 1799 2700 3231 2937 Lyons 2753 645 690 739 789 714 1458 158 1817 1159 863 1178 0 1281 320 328 724 471 1048 2108 1157 Madrid 3949 636 1558 1550 1347 1764 2498 1439 698 2198 1730 668 1281 0 1157 1724 2010 1273 2097 3188 2409 Marseilles 2865 521 1011 1059 1101 1035 1778 425 1693 1479 1183 1762 320 1157 0 618 1109 792 1011 2428 1363 Milan 2282 1014 925 1077 1209 911 1537 328 2185 1238 1098 2250 328 1724 618 0 331 856 586 2187 898 Munich 2179 1365 747 977 1160 583 1104 591 2565 805 851 2507 724 2010 1109 331 0 821 946 1754 428 Paris 3000 1033 285 280 340 465 1176 513 1971 877 457 1799 471 1273 792 856 821 0 1476 1827 1249 Rome 817 1460 1511 1662 1794 1497 2050 995 2631 1751 1683 2700 1048 2097 1011 586 946 1476 0 2707 1209 Stockholm 3927 2868 1616 1786 2196 1403 650 2068 3886 949 1500 3231 2108 3188 2428 2187 1754 1827 2707 0 2105 Vienna 1991 1802 1175 1381 1588 937 1455 1019 2974 1155 1205 2937 1157 2409 1363 898 428 1249 1209 2105 0 The above plot reconstructs the European map quite well. Gibraltar, Lisbon and Madrid are in the south-west corner, the two North European cities Stockholm and Copenhagen are in the north end, and Athens is in the south-west corner. Finally, let’s consider the 18 representative global cities. Their pairwise flight lengths (geodesic distance) are shown in the table below. As we can see, the geodesic distances between the three Southern-Hemisphere cities: Rio, Cape Town, Melbourne and other Northern-Hemisphere cities are generally large (almost all over 10,000 kilometers) Beijing Cape Town Hong Kong Honolulu London Melbourne Mexico City Montreal Moscow New Delhi New York Paris Rio Rome S.F. Singapore Stockholm Tokyo Beijing 0 12947 1972 8171 8160 9093 12478 10490 5809 2788 11012 8236 17325 8144 9524 4465 6725 2104 Cape Town 12947 0 11867 18562 9635 10388 13703 12744 10101 9284 12551 9307 6075 8417 16487 9671 10334 14737 Hong Kong 1972 11867 0 8945 9646 7392 14155 12462 7158 3770 12984 9650 17710 9300 11121 2575 8243 2893 Honolulu 8171 18562 8945 0 11653 8862 6098 7915 11342 11930 7996 11988 13343 12936 3857 10824 11059 6208 London 8160 9635 9646 11653 0 16902 8947 5240 2506 6724 5586 341 9254 1434 8640 10860 1436 9585 Melbourne 9093 10388 7392 8862 16902 0 13557 16730 14418 10192 16671 16793 13227 15987 12644 6050 15593 8159 Mexico City 12478 13703 14155 6098 8947 13557 0 3728 10740 14679 3362 9213 7669 10260 3038 16623 9603 11319 Montreal 10490 12744 12462 7915 5240 16730 3728 0 7077 11286 533 5522 8175 6601 4092 14816 5900 10409 Moscow 5809 10101 7158 11342 2506 14418 10740 7077 0 4349 7530 2492 11529 2378 9469 8426 1231 7502 New Delhi 2788 9284 3770 11930 6724 10192 14679 11286 4349 0 11779 6601 14080 5929 12380 4142 5579 5857 New York 11012 12551 12984 7996 5586 16671 3362 533 7530 11779 0 5851 7729 6907 4140 15349 6336 10870 Paris 8236 9307 9650 11988 341 16793 9213 5522 2492 6601 5851 0 9146 1108 8975 10743 1546 9738 Rio 17325 6075 17710 13343 9254 13227 7669 8175 11529 14080 7729 9146 0 9181 10647 15740 10682 18557 Rome 8144 8417 9300 12936 1434 15987 10260 6601 2378 5929 6907 1108 9181 0 10071 10030 1977 9881 S.F. 9524 16487 11121 3857 8640 12644 3038 4092 9469 12380 4140 8975 10647 10071 0 13598 8644 8284 Singapore 4465 9671 2575 10824 10860 6050 16623 14816 8426 4142 15349 10743 15740 10030 13598 0 9646 5317 Stockholm 6725 10334 8243 11059 1436 15593 9603 5900 1231 5579 6336 1546 10682 1977 8644 9646 0 8193 Tokyo 2104 14737 2893 6208 9585 8159 11319 10409 7502 5857 10870 9738 18557 9881 8284 5317 8193 0 The three-dimensional visualization result of classical MDS is shown above. You can rotate and magnify it on your laptop. The blue points represent Asian cities, the black points represent European cities, and the red points represent North American cities. If you inspect the plot clearly, you may notice that the cities appear to be constrained to the surface of a sphere, which complies to the true scenario. In each of the examples, classical MDS was quite successful in generating maps which reflected the geographical configuration with continental or global maps which were (after some reflections/rotations) consistent with conventional maps. fe6c00855e493ca3aa72864411530b42ede78222 4.4.1 Key features of MDS Beyond demonstrating the capacity of MDS to recover meaningful visualization, we also gain two insights from the examples above. The pairwise distance between two objects need not be Euclidean. In the above example, they are actually great circle distance. Actually, based on the type of distance metric and the specific recovery method of the Proximity Matrix, MDS can be divided into three major types: Classical Scaling; Metric MDS; and Non-Metric MDS. Classical Scaling and Metric MDS generally require that the input data is a true distance metric, while Non-metric MDS is usually used when the input data doesn’t satisfy the properties of a true distance metric or when the relationships are ordinal (i.e., we only know which distances are larger, but not by how much). To add, Classical MDS operates by applying an eigen-decomposition to the “double-centered” dissimilarity matrix, while Metric MDS and Non-Metric MDS may employ iterative optimization methods to better accommodate non-linear relationships in data structure. As we can tell from the recovery of US map and European map, the configurations of \\(\\tilde{y}_1, \\tilde{y}_2, \\dots, \\tilde{y}_N\\) are not unique, as we can rotate or flip the map. Actually, if \\(\\tilde{y}_1, \\tilde{y}_2, \\dots, \\tilde{y}_N \\in \\mathbb{R}^{t^{\\prime}}\\) is considered as the optimal solution, given any vector \\(\\vec{b} \\in \\mathbb{R}^{t^{\\prime}}\\) and orthogonal matrix \\(A \\in \\mathbb{R}^{t^{\\prime} \\times t^{\\prime}}\\), \\(||(A \\tilde{y}_r + \\vec{b} - (A \\tilde{y}_s + \\vec{b})|| = ||\\tilde{y}_r - \\tilde{y}_s||\\). Rotation, Reflection or Translation don’t alter the pairwise distances. So \\(A \\tilde{y}_1 + \\vec{b}, A \\tilde{y}_2 + \\vec{b}, \\dots, A \\tilde{y}_N + \\vec{b}\\) is also an optimal solution. 4.4.2 Classical Scaling Before going deep into Classical Scaling, we first need to introduce some important definitions. 4.4.2.1 Definitions: A matrix \\(\\Delta \\in \\mathbb{R}^{N \\times N}\\) is called a distance matrix if it possesses the following properties: Symmetry \\(\\Delta\\) is symmetric, meaning that: \\[\\Delta_{rs} = \\Delta_{sr} \\quad \\text{for all } r \\text{ and } s\\] This implies that the distance between points \\(r\\) and \\(s\\) is the same as the distance between points \\(s\\) and \\(r\\). Zero Diagonal The diagonal entries of the matrix represent the distance of a point to itself, and are thus all zeros: \\[\\Delta_{rr} \\equiv 0 \\quad \\text{for all } 1 \\leq r \\leq N\\] Non-negativity All distances are non-negative: \\[\\Delta_{rs} \\geq 0\\] Triangle Inequality The matrix respects the triangle inequality: \\[\\Delta_{rs} \\leq \\Delta_{rt} + \\Delta_{ts}\\] This property ensures that the direct distance between two points \\(r\\) and \\(s\\) is never greater than the sum of the distances from \\(r\\) to a third point \\(t\\) and from \\(t\\) to \\(s\\). Further, the distance matrix \\(\\Delta \\in \\mathbb{R}^{N \\times N}\\) is a Euclidean distance matrix if there exists a configuration \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N\\) s.t. \\(\\Delta_{rs}\\) represents the Euclidean distance between points \\(r\\) and \\(s\\), i.e., \\(||\\vec{y}_r-\\vec{y}_s||=\\Delta_{rs} \\forall r,s\\). For configuration \\(\\vec{y}_i, \\vec{y}_j \\in \\mathbb{R}^t\\), the Euclidean distance between point \\(r\\) and point \\(s\\) is defined as \\(\\delta_{ij}=||\\vec{y}_i - \\vec{y}_j||= \\left\\{\\sum_{k=1}^t \\left(y_{i k}-x_{j k}\\right)^2\\right\\}^{1/2}\\). Remember to take the square root Prerequisite for Classical Scaling: Strictly speaking, we require that the Proximity Matrix \\(\\Delta\\) is a Euclidean distance matrix. Furthermore, it is assumed that the configurations \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N\\) are centered. In other words, \\(\\sum_{i=1}^N \\vec{y}_i = \\vec{0}\\). This step eliminates the influence of location. 4.4.2.2 Recover Coordinates It may sound difficult to recover the configuration \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N\\) from merely the Euclidean distance matrix \\(\\Delta\\) at the first glance. However, the situation becomes clearer after some analysis. \\(\\Delta^2_{rs}\\) can be expressed as \\(\\Delta^2_{rs} = ||\\vec{y}_r||^2 + ||\\vec{y}_s||^2 - 2 \\vec{y}_r^T \\vec{y}_s\\). It’s not difficult to observe that both terms are very easy to be expressed with the original configuration: \\(Y = \\left(\\begin{array}{c} \\vec{y}_1^{\\top} \\\\ \\vec{y}_2^{\\top} \\\\ \\vdots \\\\ \\dot{y}_N^{\\top} \\end{array}\\right)\\). The entries of the inner product matrix, i.e. \\(B = YY^T \\in \\mathbb{R}^{N \\times N}\\) are able to express the Euclidean distance matrix \\(\\Delta\\). 1) Computing the inner product matrix \\(B\\) The inner product term can be represented as: \\(B_{ij} = \\vec{y}_r^T \\vec{y}_s\\), while the norms of \\(\\vec{y}_r\\) and \\(\\vec{y}_s\\) can be represented by the entry of \\(B\\), i.e. \\(B_{ii} = ||\\vec{y}_i||^2\\). As a result, \\(\\Delta^2_{ij} = B_{ii} + B_{jj} - 2 B_{ij}\\). Key Observation Here, we successfully express the entries of \\(\\Delta\\) (known) with entries of \\(B\\) (unknown). However, our goal is to find a way to express entries of \\(B\\) (unknown) with entries of \\(\\Delta\\). Intuitively, we want to cancel out \\(B_{ij}\\) terms in the expression. Considering that \\(\\sum_{i=1}^N \\vec{y}_i = \\vec{0}\\), we sum both sides of the equation over the index \\(i\\). We can get the following expression: (Because \\(\\sum_{i=1}^N B_{ij} = \\vec{y}_j^T (\\sum_{i=1}^N \\vec{y}_i) = 0\\)) \\[\\sum_{i=1}^N \\Delta_{ij}^2=\\operatorname{tr}(B) + N B_{ii}\\] Similarly, we can also sum both sides of the equation over index \\(j\\), and get the expression: \\[\\sum_{j=1}^N \\Delta_{ij}^2=\\operatorname{tr}(B) + N B_{jj}\\] We successfully eliminate all the off-diagonal terms of \\(B\\) through the above steps. Now, we want to take a step further. Sum both sides of the equations over both indexes \\(i\\) and \\(j\\). We acquire the following expression: \\[\\sum_{i=1}^N \\sum_{j=1}^N \\Delta_{ij}^2 = 2N \\operatorname{tr}(B)\\] Now we can solve the entries of \\(\\Delta\\) using the entries of \\(B\\) through a backward calculation. From the last equation, we get \\[\\operatorname{tr}(B) = \\frac{1}{2N} \\sum_{i=1}^N \\sum_{j=1}^N \\Delta_{ij}^2\\] Then substitute the above expression into above formulas, we get the expression of the diagonal entries of \\(B\\): \\[B_{ii} = \\frac{1}{N} (\\sum_{j=1}^{N} \\Delta_{ij}^2 - \\operatorname{tr} (B))\\] After that, we can finally get the off-diagonal entries of \\(B\\): \\[\\begin{aligned} B_{ij} &amp; = \\frac{1}{2} (B_{ii} + B_{jj} - \\Delta_{ij}^2) \\\\ &amp; = -\\frac{1}{2} \\Delta_{ij}^2 + \\frac{1}{N} \\sum_{i=1}^N \\Delta_{ij}^2 + \\frac{1}{N} \\sum_{j=1}^N \\Delta_{ij}^2-\\frac{1}{2 N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\Delta_{ij}^2 \\end{aligned}\\] Actually, we may also express the inner product matrix \\(B\\) in a matrix form, which is what we do in real data computation. Here \\(A \\in \\mathbb{R}^{N \\times N}, A_{ij} = \\Delta^2_{ij} \\quad \\text{for} \\; \\forall \\; 1 \\leq i,j \\leq N\\), \\(H = \\mathbb{I}_N - \\frac{1}{N} \\mathbb{1}_N \\mathbb{1}_N^T\\). \\[B = H A H\\] 2) Recover the coordinates using inner product matrix \\(B\\) Both diagonal and off-diagonal entries of the inner product matrix \\(B\\) has been shown. We assumed that \\(B\\) = \\(YY^T\\), so \\(B\\) is symmetric and positive semi-definite (all eigenvalues are non-negative), with \\(t\\) positive eigenvalues and \\(N-t\\) ‘zero’ eigenvalues. Our intuition is to apply SVD to \\(B\\) in order to recover the configuration \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N \\in \\mathbb{R}^t\\). \\[\\begin{align} B &amp; = \\left(\\vec{u}_1\\left|\\vec{u}_2\\right| \\ldots \\mid \\vec{u}_t\\right) \\begin{pmatrix} \\lambda_1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\lambda_t \\end{pmatrix} \\left(\\begin{array}{c} \\vec{u}_1^{\\top} \\\\ \\vec{u}_2^{\\top} \\\\ \\vdots \\\\ \\vec{u}_t^{\\top} \\end{array}\\right) \\\\ &amp; = \\tilde{u} \\begin{pmatrix} \\lambda^{1/2}_1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\lambda^{1/2}_t \\end{pmatrix} \\begin{pmatrix} \\lambda^{1/2}_1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\lambda^{1/2}_t \\end{pmatrix} \\tilde{u}^T \\\\ &amp; = (\\tilde{u} \\Lambda^{1/2}) (\\tilde{u} \\Lambda^{1/2})^T \\end{align}\\] Let \\(Y=\\tilde{u} \\Lambda^{1/2}\\), use rows of \\(\\tilde{u} \\Lambda^{1/2}\\) as \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N\\). It satisfies every entry of the inner product matrix \\(B\\), as well as all pairwise distances \\(\\Delta_{ij}\\). Dealing with real data case In real data case, sometimes the Euclidean condition is not met. As in the previous globe map example, the distance matrix actually computes the geodesic distance instead of the Euclidean distance. Under this circumstance, the “inner product” matrix \\(B = HAH\\) (we put a quotation mark here because \\(B\\) is no longer the inner product matrix) is not necessarily positive semi-definite. Let’s put a numeric example here to illustrate this point. Suppose we have a proximity matrix \\(\\Delta\\): Given the matrix: \\[ \\Delta = \\begin{pmatrix} 0 &amp; 1 &amp; 3 \\\\ 1 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\end{pmatrix} \\] ## [,1] [,2] [,3] ## [1,] 0 1 3 ## [2,] 1 0 1 ## [3,] 3 1 0 Obviously, matrix \\(\\Delta\\) does not satisfy Triangle Inequality, so it’s not a Euclidean distance matrix (even not a distance matrix actually) Compute matrix \\(A\\) as: \\[ A = -\\frac{1}{2} \\Delta^2 \\] ## [,1] [,2] [,3] ## [1,] 0.0 -0.5 -4.5 ## [2,] -0.5 0.0 -0.5 ## [3,] -4.5 -0.5 0.0 Compute matrix \\(B\\) using: \\[ H = I - \\frac{1}{n} \\mathbf{11}^T \\] Where \\(I\\) is the identity matrix and \\(n\\) is the number of rows (or columns) in \\(\\Delta\\). Then: \\[ B = H A H \\] ## [,1] [,2] [,3] ## [1,] 2.1111111111111116045436 0.2777777777777779011359 -2.3888888888888892836349 ## [2,] 0.2777777777777779011359 -0.5555555555555554692049 0.2777777777777778456247 ## [3,] -2.3888888888888892836349 0.2777777777777779011359 2.1111111111111116045436 Finally, perform an eigen-decomposition on matrix \\(B\\). ## [1] 4.500000000000000888178e+00 -7.771561172376095782965e-16 -8.333333333333334813631e-01 ## [,1] [,2] [,3] ## [1,] 7.071067811865475727373e-01 0.5773502691896253979920 -0.4082482904638635168304 ## [2,] 1.110223024625156540424e-16 0.5773502691896261751481 0.8164965809277253683263 ## [3,] -7.071067811865475727373e-01 0.5773502691896256200366 -0.4082482904638631282523 Here, we have a negative eigenvalue \\(-\\frac{5}{6}\\). This is because the original proximity matrix \\(\\Delta\\) is not a distance matrix. Handle Negative Eigenvalues non-symetric issue: There are cases where the proximity matrix \\(\\Delta\\) is not symmetric (though it rarely happens in classical scaling scenario). Usually we set \\(\\Delta \\leftarrow \\Delta + \\Delta^T\\). In that way, we manually make \\(\\Delta\\) symmetric. When there exist some negative eigenvalues in the inner product matrix \\(B\\), we usually have two options to deal with it. Inflate the original proximity matrix \\(\\Delta\\) by a small constant factor \\(c\\), i.e., \\(\\Delta_{ij} \\leftarrow \\Delta_{ij} + c, \\; \\text{if} \\; i \\neq j\\). In this way, we can deal with the violence of Triangular Inequality. If there exist several negative eigenvalues with small absolute value (compared to the largest several positive eigenvalues), and there are more positive eigenvalues than our prior estimation (the dimension of the original configuration), we may just pick the largest \\(t\\) eigenvalues and eliminate the rest. Global City Distance case revisit We can also consider the previous global city distance matrix example. Plot the scree plot of the inner product matrix. We find that the first three eigenvalues are much larger than the rest, so we assume that the dimension of the original configuration is 3, which also complies to our knowledge about global map. 4.4.2.3 Duality of PCA and Classical Scaling You may have already observed that Classical Scaling is actually equivalent to PCA to some extent. Cox states that “there is a duality between a principals components analysis and classical MDS where dissimilarities are given by Euclidean distance”[1]. Given the matrix expression of the original configuration \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times k}\\). Recall that PCA is attained by finding the eigen-vectors of the covariance matrix \\(\\frac{1}{N-1} (HX)^T (HX)\\), where \\(H\\) is the centering matrix. Suppose the \\(k\\) eigen-vectors are \\(\\vec{w}_1, \\vec{w}_2, \\dots, \\vec{w}_k\\), and the corresponding eigenvalues are \\(\\mu_1, \\mu_2, \\dots,\\mu_k\\). Then \\(HX=YW^T\\), where \\(W=(\\vec{w}_1 | \\vec{w}_2 | \\dots | \\vec{w}_N)\\) represents PC Loadings, and \\(Y\\) represents PC Scores. While MDS is attained by first converting \\(X\\) into distance matrix, here, Euclidean distance. In classical MDS algorithm, we convert the Euclidean distance matrix into the inner product matrix \\(B=(HX)(HX)^T\\). then we find the eigen-vectors of \\(B\\) — call the eigen-vectors \\(\\vec{\\nu}_1, \\vec{\\nu}_2, \\dots, \\vec{\\nu}_N\\), and eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_N\\). Recall what you have learnt in Linear Algebra. The eigenvalues of \\(XX^T\\) are the same as those for \\(X^TX\\), together with an extra n-p zero eigenvalues. So, for \\(i &lt; p\\), \\(\\mu_i = \\lambda_i\\). So the first \\(t\\) PC scores give the t-dimensional configuration for Classical Scaling. Choosing the number of dimensionality \\(t\\) of the original configuration is equivalent to choosing the number of principal components to keep Furthermore, we have shown that \\(HX=YW^T\\), then \\(B=(HX)(HX)^T=YW^TWY^T=YY^T\\), this means PC scores are the exact solutions for Classical Scaling! As a result, classical MDS has the same strengths and weaknesses of PCA. 4.4.3 Metric MDS 4.4.4 Nonmetric MDS References "],["kernels-and-nonlinearity.html", "Chapter 5 Kernels and Nonlinearity ", " Chapter 5 Kernels and Nonlinearity "],["exercises-2.html", "5.1 Exercises", " 5.1 Exercises "],["manifold-learning.html", "Chapter 6 Manifold Learning ", " Chapter 6 Manifold Learning "],["background.html", "6.1 Background", " 6.1 Background In the previous sections, we have focused on methods which seek to approximate our data through a linear combination of feature vectors. As we have seen, the resulting approximations live on linear (or affine) subspaces in the case of PCA and SVD and positive spans or convex combinations in the case of NMF. While our data may exhibit some low-dimensional structure, there is no practical reason to expect such behavior to be inherently linear. In the resulting sections, we will explore methods which consider nonlinear structure and assume the data reside on or near a manifold. Such methods are referred to as nonlinear dimension reduction or manifold learning. Critical to this discussion is the notion of a manifold. Definition 6.1 (Informal Definition of a Manifold) A manifold is a (topological) space which locally resembles Euclidean space. Each point on a \\(k\\)-dimensional manifold has a neighborhood that can be mapped continuously to \\(\\mathbb{R}^k\\). To guide your intuition, think of a manifold as a smooth, possibly curved surface. Here are a few examples. Example 6.1 (Examples of Manifolds) Add line, sphere, plane, and S And here is an example of something which isn’t a manifold. Example 6.2 (Non-manifold) figure 8 Much more could be said about the mathematical foundations of manifolds which are far beyond the scope of this book. For those interested in the such details consider checking out REFERENCES HERE. We will appeal to a more intuitive understanding of manifolds and when necessary provide informal, descriptive “definitions” of important concepts. For now, let’s turn to the standard manifold assumption which is common to this area of unsupervised learning. 6.1.1 Data on a manifold In the simplest setting, we will assume there are points \\(\\vec{z}_1,\\dots,\\vec{z}_N\\in A \\subset \\mathbb{R}^k\\) which are iid random samples. These points are (nonlinearly) mapped into a higher dimensional space \\(\\mathbb{R}^d\\) by a smooth map \\(\\Psi\\) giving data \\(\\vec{x}_i = \\Psi(\\vec{z}_i)\\) for \\(i=1,\\dots,N.\\) Hereafter, we refer to \\(\\Psi\\) as the manifold map. In this setting, we are only given \\(\\vec{x}_1,\\dots,\\vec{x}_N\\), and we want to recover the lower-dimensional \\(\\vec{z}_1,\\dots,\\vec{z}_N\\). If possible, we would also like recover \\(\\Psi\\) and \\(\\Psi^{-1}\\) and in the most ideal case, the sampling distribution that generated the lower-dimensional coordinates \\(\\vec{z}_1,\\dots,\\vec{z}_N\\). Example 6.3 (Mapping to the Swiss Roll) Let \\(A = (\\pi/2,9\\pi/2)\\times (0,15)\\). We define the map \\(\\Psi:A\\to \\mathbb{R}^3\\) as follows \\[\\Psi(\\vec{z}) = \\Psi(z_1,z_2) = \\begin{bmatrix} z_1\\sin(z_1) \\\\ z_1\\cos(z_1) \\\\ z_2 \\end{bmatrix}\\] Below we show \\(N=10^4\\) samples which are drawn uniformly from \\(A\\). We then show the resulting observations after applying map \\(\\Psi\\) to each sample. We may also consider the more complicated case where the observations are corrupted by additive noise. In this setting, the typical assumption is that the noise follows after the manifold map so that our data are \\[\\vec{x}_i = \\Psi(\\vec{z}_i) + \\vec{\\epsilon}_i, \\qquad i = 1,\\dots, N\\] for some noise vectors \\(\\{\\vec{\\epsilon}_i\\}_{i=1,\\dots,N}.\\) Example 6.4 (Swiss Roll with Additive Gaussian Noise) Here, we perturb the observations in the preceding example with additive \\(\\mathcal{N}(\\vec{0},0.1{\\bf I})\\) noise. In addition to the goals in the noiseless case, we may also add the goal of learning the noiseless version of the data which reside on a manifold. However, there are a number of practical issues to this setup. First, the dimension, \\(k\\), of the original lower-dimensional points is typically unknown. Similar to previous methods, we could pick a value of \\(k\\) with the goal of visualization, base our choice off of prior knowledge, or run our algorithms different choices of \\(k\\) and compare the results. More advanced methods for estimating the true value of \\(k\\) are an open area of research (REFERENCES NEEDED). There is also a issue with the uniqueness problem statement. Given only the high dimensional observations, there is no way we could identify the original lower-dimensional points without more information. In fact, one could find an unlimited sources of equally suitable results. Here is the issue. Let \\(\\Phi:\\mathbb{R}^k\\to\\mathbb{R}^k\\) be some invertible function. As an example, you could think of \\(\\Phi\\) as defining a translation, reflection, rotation, or some composition of these operations. If our original observed data are \\(\\vec{x}_i = \\Psi(\\vec{z}_i)\\), our manifold learning algorithm could instead infer that the manifold map is \\(\\Psi \\circ \\Phi^{-1}\\) and the lower-dimensional points are \\(\\Phi(\\vec{z}_i)\\). This is a perfectly reasonable result since \\((\\Psi\\circ \\Phi^{-1}\\circ)\\Phi(\\vec{z}_i) = \\Psi(\\vec{z}_i)= \\vec{x}_i\\) for \\(i=1,\\dots,N\\), which is the only result we require. Without additional information, there is little we could do to address this issue. For the purposes of visualization, however, we will typically be most interested in the relationship between the lower-dimensional points rather than their specific location or orientation. As such, we need not be concerned about a manifold learning algorithm that provides a translated or rotated representation of \\(\\vec{z}_1,\\dots,\\vec{z}_N.\\) More complicated transformations of the lower-dimensional coordinates are of greater concern and may be addressed through additional assumptions about the manifold map \\(\\Psi.\\) In the following sections, we will review a small collection of different methods which address the manifold learning problem. This collection is by no means exhaustive so we provide a small list with associated references to conclude the chapter. "],["isometric-feature-map-isomap.html", "6.2 Isometric Feature Map (ISOMAP)", " 6.2 Isometric Feature Map (ISOMAP) 6.2.1 Introduction The first manifold learning method we are going to cover is the Isometric Feature Map (ISOMAP), originally published by Tenenbaum, de Silva, and Langford in 2000 [2]. As suggested by the name, we will see that the assumption of isometry is central to this method. ISOMAP combines the major algorithmic features of PCA and MDS — computational efficiency, global optimality, and asymptotic convergence guarantees. Thanks to these extraordinary features, ISOMAP is capable of learning a broad class of nonlinear manifolds. 6.2.2 Key Definitions Different notions of pointwise distance Prior to discussing the ISOMAP algorithm, let’s briefly discuss the notion of isometry through an example which motivates different notions of distance between two points. Example 6.5 (Distance between points on a Helix) Consider the helix map \\(\\Psi:\\mathbb{R}\\to\\mathbb{R}^3\\) given by the formula \\[\\begin{equation} \\Psi(t) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}}\\cos(t) \\\\ \\frac{1}{\\sqrt{2}}\\sin(t) \\\\ \\frac{1}{\\sqrt{2}}t \\\\ \\end{bmatrix} \\end{equation}\\] Below, we show the result of applying the Helix map to each point in the interval \\((0,25)\\). Let’s focus on two points \\(\\vec{x}_1 = \\Psi(2\\pi)= (1/\\sqrt{2},0,\\sqrt{2}\\pi)^T\\) and \\(\\vec{x}_2 = \\Psi(4\\pi)=(1/\\sqrt{2},0,2\\sqrt{2}\\pi)^T\\) in particular which are shown as large black dots in the figure below. There are a few different ways we could measure the distance between the two black points. The first approach would be to ignore the helix (manifold) structure viewing them as vectors in \\(\\mathbb{R}^3\\) and directly measure their Euclidean distance which gives \\[\\|\\vec{x}_1 - \\vec{x}_2\\| = \\sqrt{2}\\pi.\\] However, we also know that these points are images of the one-dimensional coordinate \\(z_1 = 2\\pi\\) and \\(z_2 = 4\\pi\\) respectively. Thus, we could also consider the Euclidean distance of the lower-dimemsional coordinates which is \\(|2\\pi - 4\\pi| = 2\\pi\\), which notably differs from the Euclidean distance. A third option is to return to the three-dimensional representation but to also account for the manifold structure when considering distance. Recall Euclidean distance gives the length of the shortest, straightline path connecting the two points. Instead, let’s restrict ourselves to only those paths which stay on the helix (manifold). You may correctly conclude that the curve starting at \\(\\Psi(2\\pi)\\), rotating up the helix one rotation, and ending at \\(\\Psi(4\\pi)\\) is the shortest such path. Fortunately, computing arc-length is relatively friendly in this example since \\(\\Psi\\) already parameterizes the path connecting these two points. The arc-length is then \\[\\int_{2\\pi}^{4\\pi} \\left\\|\\frac{d\\Psi}{dt}\\right\\| dt = \\int_{2\\pi}^{4\\pi} dt = 2\\pi.\\] Jumping slightly ahead, we then say the manifold distance between \\(\\Psi(2\\pi)\\) and \\(\\Psi(4\\pi)\\) is \\(2\\pi\\). Importantly, the manifold distance coincides exactly with the Euclidean distance between the lower-dimensional coordinates. In fact, for any two points, \\(s\\) and \\(t\\), on the real line their Euclidean distance, \\(|s-t|\\) will be the same as the manifold distance between \\(\\Psi(s)\\) and \\(\\Psi(t)\\). Thus, the helix map \\(\\Psi\\) above serves as our first example of an isometric (distance preserving) map. We may generalize this idea to any smooth manifold to define a new notion of distance. Given a manifold \\(\\mathcal{M}\\), we define the manifold distance function \\(d_\\mathcal{M} : \\mathcal{M} \\times \\mathcal{M} \\to [0,\\infty)\\) as follows Definition of Manifold Distance ::: {.definition #def-manifold-dist name=“Manifold Distance Function”} Given two points \\(\\vec{x}\\) and \\(\\vec{y}\\) on a smooth manifold, \\(\\mathcal{M}\\), let \\(\\Gamma(\\vec{x},\\vec{y})\\) be the set of all piecewise smooth curves connecting \\(\\vec{x}\\) and \\(\\vec{y}\\) constrained to stay on \\(\\mathcal{M}\\). Then, we define the manifold distance to be \\[\\begin{equation} d_\\mathcal{M}(\\vec{x},\\vec{y}) = \\inf_{\\gamma \\in \\Gamma(\\vec{x},\\vec{y})} L(\\gamma) \\tag{6.1} \\end{equation}\\] where \\(L(\\gamma)\\) is the arclength of \\(\\gamma.\\) ::: As we reviewed above, the helix example with the arclength formula is one example of a manifold and distance function. Additional examples of a manifold and manifold distance include, Euclidean space \\(\\mathbb{R}^d\\) where standard Euclidean distance gives the manifold distance. The sphere in \\(\\mathbb{R}^3\\) which is a two-dimensional manifold. Its manifold distance is also called the Great Circle Distance. We may now define the notion of isometry which is a central assumption of ISOMAP. Definition of Isometry ::: {.definition #def-isometry name=“Isometry”} Let \\(\\mathcal{M}_1\\) be a manifold with distance function \\(d_{\\mathcal{M}_1}\\) and let \\(\\mathcal{M}_2\\) be a second manifold with distance function \\(d_{\\mathcal{M}_2}\\). The mapping \\(\\Psi:\\mathcal{M}_1 \\mapsto \\mathcal{M}_2\\) is an isometry if \\[d_{\\mathcal{M}_1}(x,y) = d_{\\mathcal{M}_2}\\left(\\Psi(\\vec{x}),\\Psi(\\vec{y})\\right) \\qquad \\text{ for all } \\vec{x},\\vec{y}\\in \\mathcal{M}_1.\\] ::: For the purposes of ISOMAP, we will think of \\(\\mathcal{M}_1\\) as some subset of a \\(\\mathbb{R}^k\\) for \\(k\\) small where we measure distances using the Euclidean norm. Then \\(\\mathcal{M}_2\\) will be a \\(k\\)-dimensional manifold in \\(\\mathbb{R}^d\\) containing our data. Our first assumption is that the manifold mapping \\(\\Psi\\) is an isometry. Unfortunately, in practice we do not know the manifold nor will we have a method for parameterizing curves on the manifold to compute distances. 6.2.3 Algorithm Instead, ISOMAP makes use of a data-driven approach to estimate the manifold distance between points following a three-step procedure. 1) Construct Weighted Neighborhood Graph: MDS uses Euclidean distance to measure pairwise distance between points \\(\\vec{x}_i\\) and \\(\\vec{x}_j\\) (data points in space \\(\\mathcal{M}_2\\)), while ISOMAP uses the geodesic distance in order to reveal the underlying manifold structure. However, when the data points in the high dimensional space \\(\\mathcal{M}_2\\) have a manifold structure, usually the Euclidean pairwise distance is quite different from their pairwise geodesic distance. Fortunately, for small distances on a smoothly embedded manifold, the geodesic path between two close-by points lies nearly flat in the ambient space. So, the length of this path will be very close to the straight line (Euclidean) distance between those points in the ambient space. The key intuition is that as the density of data points on the manifold increases (i.e., points get closer and closer), the straight line segment in the ambient space connecting two neighboring points becomes a better and better approximation of the shortest path between those points on the manifold. In the limit of the density going to infinity, these distances converge. Let’s elucidate this concept with two illustrative examples. Firstly, imagine a two-dimensional surface, like a Swiss Roll, situated within a three-dimensional space. For an ant journeying across the Swiss Roll, the vast size difference means its immediate surroundings appear flat. From its perspective, the distance between its consecutive steps closely mirrors the distance a human might measure (Euclidean distance) – both virtually equating to the roll’s geodesic distance. For a larger-scale analogy, think of Earth. Suppose extraterrestrial beings possessed technology allowing them to traverse straight through Earth’s crust and mantle, thus following the shortest Euclidean path. Their journey from Los Angeles to New York might save them hundreds of miles compared to humans. However, when moving between closer landmarks, such as the Science Center to the Smith Center, their advantage diminishes. As a result, when it comes to the measurement of geodesic distance, it is reasonable to only look at those data points that are close to each other. First, calculate all the pairwise Euclidean distance \\(d_{ij}=||\\vec{x}_i - \\vec{x}_j||_2\\), then determine which points are neighbors on the manifold by connecting each point to Either (i) All points that lie within a ball of radius \\(\\epsilon\\) of that point; OR (ii) all points which are K-nearest neighbors with it. (Two different criteria, \\(K\\) and \\(\\epsilon\\) are tuning parameters) According to this rule, a weighted neighborhood graph \\(G = G(V,E)\\) can be built. The set of vertices (data points in space \\(\\mathcal{M}_2\\)): \\(V = \\{\\vec{x}_1, \\dots , \\vec{x}_N\\}\\) are the input data points, and the set of edges \\(E = \\{e_{ij}\\}\\) indicate neighborhood relationships between the points. \\(e_{ij} = d_{ij}\\) if (i) \\(||\\vec{x}_i - \\vec{x}_j||_2 \\leq \\epsilon\\); OR (ii) \\(\\vec{x}_j\\) is one of the K-nearest neighbors of \\(\\vec{x}_i\\), otherwise \\(e_{ij} = \\infty\\). Sometimes, the tuning of \\(\\epsilon\\) (or \\(K\\)) is quite decisive in the output of ISOMAP, we will explain this later with a simulation example. 2) Compute graph distances In this step, we want to estimate the unknown true geodesic distances \\(\\{d^{\\mathcal{M}}_{ij}\\}\\) between all pairs of points with the help of the neighborhood graph \\(G\\) we have just built. We use the graph distances \\(\\{d^{\\mathcal{G}}_{ij}\\}\\)— the shortest distances between all pairs of points in the graph \\(G\\) to estimate \\(\\{d^{\\mathcal{M}}_{ij}\\}\\). For \\(\\vec{x}_i\\) and \\(\\vec{x}_j\\) that are not connected to each other, we try to find the shortest path that goes along the connected points on the graph. Following this particular sequence of neighbor-to-neighbor links, the sum of all the link weights along the path is defined as \\(\\{d^{\\mathcal{G}}_{ij}\\}\\). In other words, we use a number of short Euclidean distances (representing the local structure of the manifold) to approximate the geodesic distance \\(\\{d^{\\mathcal{M}}_{ij}\\}\\). This path finding step is usually done by Floyd-Warshall algorithm, which iteratively tries all transit points \\(k\\) and find those that \\(\\tilde{d}_{ik} + \\tilde{d}_{kj} &lt; \\tilde{d}_{ij}\\), and updates \\(\\tilde{d}_{ij} = \\tilde{d}_{ik} + \\tilde{d}_{kj}\\) for all possible combination of \\(i,j\\). The algorithm works best in dense neighboring graph scenario, with a computational complexity of \\(O(n^3)\\). The theoretical guarantee of this graph distance computation method is given by Bernstein et, al.[3] one year after they first proposed ISOMAP in their previous paper. They show that asymptotically (as \\(n \\rightarrow \\infty\\)), the estimate \\(d^{\\mathcal{G}}\\) converges to \\(d^{\\mathcal{M}}\\) as long as the data points are sampled from a probability distribution that is supported by the entire manifold, and the manifold itself is flat. The distance matrix \\(\\Delta\\) can be expressed as: \\[\\Delta_{ij} = d^{\\mathcal{G}}_{ij}\\] Simulation Example Here we provide a randomly generated Neighborhood Graph for six data points, it uses the K-nearest neighbor criteria (can easily tell this since the matrix is not symmetric, \\(K=2\\)) # Define the matrix matrix &lt;- matrix(c( 0, 3, 4, Inf, Inf, Inf, 7, 0, Inf, 2, Inf, Inf, 6, Inf,0, Inf, 7, Inf, Inf, 5, Inf, 0, Inf, 10, Inf, Inf,8, Inf, 0, 13, Inf, Inf,Inf, 9, 14, 0 ), byrow = TRUE, nrow = 6) print(matrix) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 3 4 Inf Inf Inf ## [2,] 7 0 Inf 2 Inf Inf ## [3,] 6 Inf 0 Inf 7 Inf ## [4,] Inf 5 Inf 0 Inf 10 ## [5,] Inf Inf 8 Inf 0 13 ## [6,] Inf Inf Inf 9 14 0 Shown below is the implementation of Floyd-Warshall algorithm in R. As you can see from the three for loops, its computation complexity is \\(O(n^3)\\). # Adjusting the matrix to set d_ij and d_ji to the smaller value n &lt;- dim(matrix)[1] for (i in 1:n) { for (j in 1:n) { if (i != j &amp;&amp; is.finite(matrix[i, j]) &amp;&amp; is.finite(matrix[j, i])) { min_val &lt;- min(matrix[i, j], matrix[j, i]) matrix[i, j] &lt;- min_val matrix[j, i] &lt;- min_val } } } # Floyd-Warshall Algorithm floyd_warshall &lt;- function(mat) { n &lt;- dim(mat)[1] dist &lt;- mat for (k in 1:n) { for (i in 1:n) { for (j in 1:n) { dist[i, j] &lt;- min(dist[i, j], dist[i, k] + dist[k, j]) } } } return(dist) } # Get the result result &lt;- floyd_warshall(matrix) # Print the result print(result) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 3 4 5 11 14 ## [2,] 3 0 7 2 14 11 ## [3,] 4 7 0 9 7 18 ## [4,] 5 2 9 0 16 9 ## [5,] 11 14 7 16 0 13 ## [6,] 14 11 18 9 13 0 3) Applying MDS to \\(\\Delta\\) As mentioned before, ISOMAP can be viewed as the application of classical MDS in non-linear case. As a result, the reconstruction of \\(\\{\\vec{z}_i\\}\\) in the \\(k\\) dimensional \\(\\mathcal{M}_1\\) follows similar steps as that of classical MDS. The main goal is to preserve the geodesic distance of the manifold in \\(\\mathcal{M}_2\\) as much as possible. Without any additional information, there are infinite \\(\\{\\vec{z}_i\\}\\) that can be viewed as the optimal solution. For some invertible function \\(\\Phi:\\mathbb{R}^k\\to\\mathbb{R}^k\\), a new manifold mapping \\(\\Psi \\circ \\Phi^{-1}\\) can be constructed. \\(\\vec{x}_i = \\Psi \\circ \\Phi^{-1} (\\Phi(\\vec{z}_i))\\), which proofs that \\(\\{\\Phi(\\vec{z}_i)\\}\\) is equivalent to \\(\\{\\vec{z}_i\\}\\) when it comes to the reconstruction of the lower dimensional configuration. Without loss of generality, we assume that \\(\\{\\vec{z}_i\\}\\) are actually centered. So the distance matrix of \\(\\{\\vec{z}_i\\}\\) can be expressed as \\(B=Z^T Z\\), so that \\(B_{ii}=||z_i||^2_2\\) and \\(B_{ij}={z_i}^T z_j\\). The embedding vectors \\(\\{\\hat{z}_{i}\\}\\) (estimate of points in lower dimensional feature space \\(\\mathcal{M}_1\\)) are chosen in order to minimize the objective function: \\[(\\sum ||\\vec{z}_i - \\vec{z}_j||_2 - \\Delta_{ij})^2\\] Following the same procedure explained in classical MDS chapter, we can compute each entry of \\(B\\): \\[B_{ij}= -\\frac{1}{2} \\Delta^2_{ij} + \\frac{1}{d} \\sum^{d}_{i=1} \\Delta^2_{ij} + \\frac{1}{d} \\sum^{d}_{j=1} \\Delta^2_{ij} - \\frac{1}{2d^2} \\sum^{d}_{i=1} \\sum^{d}_{j=1} \\Delta^2_{ij}\\] To express it in matrix form, it is actually, \\(B = - \\frac{1}{2} H \\Delta H\\), where \\(H = I_n - \\frac{1}{n} \\mathbb{1} \\mathbb{1}^T\\). The next step is just a PCA problem. Implement eigen decomposition on matrix B, \\(B=U \\Lambda U^T= (\\Lambda^{1/2} U)^T (\\Lambda^{1/2} U)\\), then arrange the singular value in descending order, find the first \\(k^{\\prime}\\) ones. We acquire \\(\\Lambda_{k^{\\prime}}\\) and \\(U_{k^{\\prime}}\\). \\[(\\hat{z}_1 | \\hat{z}_2 | \\dots | \\hat{z}_N) = \\Lambda_{k^{\\prime}} U_{k^{\\prime}}\\] Since we don’t know the dimension of the underlying feature space, here \\(k^{\\prime}\\) is a tuning parameter. Usually, we use a scree plot (\\(k^{\\prime}\\) against the sum of the omitted eigenvalues ) and find the elbow point. 6.2.4 Limitations of ISOMAP Though ISOMAP is a powerful manifold learning method that works well under most circumstances. It still has some limitations in certain scenarios. If the noises \\(\\{\\epsilon_i\\}\\) is not negligible, then ISOMAP may fail to identify the manifold. Also, ISOMAP is quite sensitive to the tuning parameters. To alleviate the negative impact, it’s highly suggested to start with a relatively small \\(\\epsilon\\) or \\(K\\), and increase them gradually. Here we use the Swiss Roll Example to help explain this point. # generate Swiss roll-shaped data S &lt;- rep(0,2000) Swiss &lt;- matrix(NA, nrow = 2000, ncol = 3) for( n in 1:2000){ s &lt;- runif(1, min = 3*pi/2, max = 9*pi/2) t &lt;- runif(1, min = 0, max = 15) S[n] &lt;- s Swiss[n, ] &lt;- c( s*cos(s), t, s*sin(s) ) } par(mfrow = c(2,2)) # K = 20 scatterplot3d(Swiss, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S ), xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(embed(Swiss, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 20)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 20&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S) ) # K = 50 scatterplot3d(Swiss, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S ), xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(embed(Swiss, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 50)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 50&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S) ) Obviously, ISOMAP performs well when \\(K\\) is small. However, as \\(K\\) increases, the algorithm no longer recovers the lower dimensional feature space. Because the distance between nearby arms is not large, some points from different arms are considered as “close”. The pairwise geodesic distances between these points is approximated by their Euclidean distance, which is not correct, of course. When data points are sparse in certain areas or directions of the manifold, the integrity of the learned manifold structure can be compromised. The following example will clarify this notion: Consider the Swiss Roll as an illustration. This time, however, the Swiss Roll is made sparse along its horizontal direction. To the naked eye, this sparse region seems to have minimal impact on the overall manifold. Naturally, we still perceive it as a Swiss Roll. set.seed(42) S &lt;- rep(0,2000) Swiss_sparse &lt;- matrix(NA, nrow = 2000, ncol = 3) for( n in 1:2000){ s &lt;- runif(1, min = 3*pi/2, max = 9*pi/2) t &lt;- runif(1, min = 0, max = 60) S[n] &lt;- s Swiss_sparse[n, ] &lt;- c( s*cos(s), t, s*sin(s) ) } # # # Manually create a sparse region # mask &lt;- Swiss[,1] &gt; 2 &amp; Swiss[,1] &lt; 8 &amp; Swiss[,2] &gt; 10 &amp; Swiss[,2] &lt; 15 # Swiss_sparse &lt;- Swiss[!mask, ] # S &lt;- S[!mask] scatterplot3d(Swiss_sparse, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S), main = &quot;Swiss Roll with Sparse Region&quot;, xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) However, when using ISOMAP to recover the lower-dimensional feature space, complications arise, even when adjusting various tuning parameters. set.seed(77) par(mfrow = c(2,2)) # K = 10 plot(embed(Swiss_sparse, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 10)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 10&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) # k = 20 plot(embed(Swiss_sparse, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 20)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 20&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) # K = 50 plot(embed(Swiss_sparse, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 50)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 50&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) # K = 100 plot(embed(Swiss_sparse, &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 100)@data@data, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K = 100&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) One of the two major assumptions of ISOMAP is the convexity of the manifold, that is to say, if the manifold contains many holes and concave margins, then the result of ISOMAP will probably be not ideal. Here we use the example of a folded washer manifold to illustrate this point. It is obvious that the folded washer is concave in its four ends. No matter how we tune the parameter \\(K\\), ISOMAP always fails to recover the lower dimensional feature. # generate washer in 2d N &lt;- 1e5 washer &lt;- matrix(NA, nrow =N, ncol = 2) for (n in 1:N){ r &lt;- runif(1,min=5,max = 10); a = runif(1,min = 0, max = 2*pi); washer[n,] &lt;- r*c(cos(a),sin(a)) + c(20,0) } # generate folded washer in 3D washer3 &lt;- cbind(washer, washer[,2]^2) # generate rolled washer in 3D washer.swiss &lt;- washer3 for (n in 1:dim(washer3)[1] ){ washer.swiss[n,] &lt;- c( washer[n,1]*cos(washer[n,1]), washer[n,2], washer[n,1]*sin(washer[n,1] )) } N &lt;- 2000 par(mfrow = c(2,2)) # K = 10 scatterplot3d(washer3[1:N,], color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), washer[1:N,1] ), xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(embed(washer3[1:N,], &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 10)@data@data %*% matrix(c(0,1,1,0),nrow = 2), xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=10&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), washer[,1]) ) # K = 20 plot(embed(washer3[1:N,], &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 20)@data@data %*% matrix(c(0,1,1,0),nrow = 2), xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=20&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), washer[,1]) ) # K = 50 plot(embed(washer3[1:N,], &quot;Isomap&quot;, .mute = c(&quot;message&quot;, &quot;output&quot;), ndim =2, knn = 50)@data@data %*% matrix(c(0,1,1,0),nrow = 2), xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=50&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), washer[,1]) ) References "],["locally-linear-embeddings-lles.html", "6.3 Locally Linear Embeddings (LLEs)", " 6.3 Locally Linear Embeddings (LLEs) 6.3.1 Introduction Locally linear embedding (LLE) is an unsupervised learning algorithm first introduced in 2000 by Sam T. Roweis and Lawrence K. Saul [4]. In the original four-page paper, the two authors introduced the LLE algorithm and demonstrated its effectiveness in dimensional reduction, manifold learning, and in handling real-world high-dimensional data. Unlike clustering methods for local dimensional reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text. Thanks to its great mathematical properties and relatively low computing cost (compared to other manifold learning methods, like ISOMAP), LLE quickly became attractive to researchers after its emergence due to its ability to deal with large amounts of high dimensional data and its non-iterative way of finding the embeddings [5]. Compared to ISOMAP and some other previous manifold learning methods, LLE is computationally simpler and can give useful results on a broader range of manifolds{@think_globally}. For dimensional reduction, most methods introduced before LLE need to estimate pairwise distances between even two remote data points, no matter it is the simple Euclidean distance (classical MDS) or more sophisticated manifold distance (ISOMAP). The underlying main idea of these methods is actually finding a configuration that recovers the pairwise distances of original data points as much as possible. LLE, however, is quite different from these previous methods as it recovers global non-linear structure from locally linear fits. 6.3.2 Algorithm LLE algorithm is actually built on very simple geometric intuitions. As explained in MDS Part, if we consider a small enough region on a manifold in \\(D\\) dimensional space, in most cases, it can be regarded as a \\(d\\) dimensional hyperplane (\\(d \\ll D\\)). LLE also makes use of this intuition and assumes that the whole manifold consist of numerous \\(d\\)-dimensional patches that have been stitched together. Assuming that there exists sufficient data (data points are compact), it is reasonable to expect that each data point and its neighbors lie on or close to a locally linear patch of the manifold. Following this idea, LLE approximates each data point by a weighted linear combination of its neighbors and proceeds to find a lower-dimensional configuration of data points so that the linear approximations of all data points are best preserved. Specifically speaking, LLE algorithm consists of three steps. The initial step involves selecting a certain number of each data point’s nearest neighbors based on Euclidean distance. Following this, the second step calculates the optimal reconstruction weights for each point using its nearest neighbors. The final step carries out the embedding while maintaining the local geometry depicted by the reconstruction weights. 6.3.2.1 Construct Neighborhood Graph This step is actually very similar to that of ISOMAP. The process of finding neighbors in LLE is typically conducted using grouping methods like k-nearest neighbors (KNN) or selecting neighbors within a fixed radius ball (\\(\\epsilon\\)-neighborhoods), based on the Euclidean distance for each data point, in the provided data set. The KNN method is predominantly utilized for its straightforwardness and ease of implementation. The following explanations are based on KNN method. Denote \\(N\\) data points in original \\(D\\) dimensional space as \\(\\vec{x}_1, \\vec{x}_2, \\dots, \\vec{x}_N \\in \\mathbb{R}^D\\). For a point \\(\\vec{x}_i, \\quad 1 \\leq i \\leq N\\), its neighbor set is defined as \\(N_i^k \\subseteq \\{1, 2, 3, \\dots, i-1, i+1, \\dots, N \\}\\), where \\(N_i^k\\) can also be called as the indices of \\(k\\) nearest neighbors of \\(\\vec{x}_i\\). The tuning parameter \\(k\\) is chosen small enough so that the patch around \\(\\vec{x}_i\\) is flat. However, \\(k\\) should also be strictly larger than \\(d\\) so as to let the algorithm work. As we can tell from these, LLE works well only if data points are dense and hopefully evenly distributed, which will be explained in detail in later examples. The parameter tuning of the appropriate number of neighbors, \\(k\\), faces challenges of complexity, non-linearity, and diversity of high-dimensional input samples. A larger \\(k\\) value might cause the algorithm to overlook or even lose the local nonlinear features on the manifold. This issue is exacerbated as neighbor selection, typically based on Euclidean distance, can result in distant neighbors when considering the intrinsic geometry of the data, akin to a short circuit. Conversely, an overly small \\(k\\) value may lead the LLE algorithm to fragment the continuous manifold into isolated local pieces, losing global characteristics. 6.3.2.2 Reconsruct with Linear Weights As put before, we try to reconstruct each \\(\\vec{x}_i\\) using an almost convex weighted combination of its neighbors. The respective weights of all its neighbors \\(\\vec{x}_j, \\; j \\neq i\\) for each \\(\\vec{x}_i\\) is quite essential in the later reconstruction of the underlying intrinsic configuration, as we consider these weights to remain invariant before and after mapping. To explain it in mathematical formulas, the approximate of \\(\\vec{x}_i\\): \\(\\tilde{x}_i\\) is defined as \\(\\tilde{x}_i = \\sum_{j=1}^N w_{ij} \\vec{x}_j\\). There are two constraints for this formula: First, \\(w_{ij} \\equiv 0\\), if \\(j \\notin N_i^k\\) (consistent with the assumption of \\(k\\) nearest neighbors); Second, the sum of weights for each \\(\\vec{x}_i\\) is always zero, i.e., \\(\\sum_{j=1}^N w_{ij}=1\\). Then, the problem of finding the optimal \\(w_{ij}, \\; 1 \\leq i,j \\leq N\\) is equivalent to solving the following constrained Least Squares problem for \\(\\forall 1 \\leq i \\leq N\\): (1) \\[ \\begin{aligned} &amp; \\min \\left\\| \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} \\vec{x}_j\\right\\|^2 \\\\ &amp; \\text { s.t. } \\quad \\sum_{j \\in N_i^k} w_{i j}=1 . \\end{aligned} \\] It is worth noting that the weights can be negative theoretically, though in practice, we don’t expect that to happen. Invariance to Rotation, Rescaling and Transaction Define \\(\\epsilon(w) = \\sum_{i=1}^N \\left\\| \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} \\vec{x}_j\\right\\|^2\\), which is the cost function. \\(\\epsilon(w)\\) is unchanged by rotation or rescaling by common factor Actually \\(\\sum_{i=1}^N \\left\\| a \\text{U} \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} a \\text{U} \\vec{x}_j\\right\\|^2 = a^2 \\epsilon(w)\\), where \\(a\\) is a non-zero scaler and \\(\\text{U}\\) is an orthonormal matrix. \\(\\epsilon(w)\\) is unchanged by transactions Thanks to the constraint that \\(\\sum_{j=1}^N w_{ij}=1\\), for any transaction \\(\\vec{x}_i \\rightarrow \\vec{x}_i + \\vec{y}\\), the cost function does not change. \\[\\sum_{i=1}^N \\left\\| (\\vec{x}_i + \\vec{y}) -\\sum_{j \\in N_i^k} w_{i j} (\\vec{x}_j + \\vec{y}) \\right\\|^2 = \\sum_{i=1}^N \\left\\| \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} \\vec{x}_j\\right\\|^2 = \\epsilon(w)\\] From the expressions, we develop a strategy that optimizes one row of matrix \\(w\\) at a time. Now let’s try to rewrite \\(\\epsilon(\\vec{w}_i)=\\left\\| \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} \\vec{x}_j\\right\\|^2\\). \\[\\begin{align} \\epsilon(\\vec{w}_i) &amp;= \\left\\| \\vec{x}_i-\\sum_{j \\in N_i^k} w_{i j} \\vec{x}_j\\right\\|^2 \\\\ &amp; = \\left[ \\sum_{j=1}^N w_{ij} (\\vec{x}_i - \\vec{w}_j) \\right]^T \\left[ \\sum_{l=1}^N w_{il} (\\vec{x}_i - \\vec{w}_l) \\right]^T \\\\ &amp; = \\sum_{j=1}^N \\sum_{l=1}^N w_{ij} w_{il} (\\vec{x}_i -\\vec{x}_j)^T (\\vec{x}_i - \\vec{x}_l) \\\\ &amp; = \\vec{w}_i^T G_i \\vec{w}_i \\end{align}\\] \\(\\vec{w}_i^T = (w_{i1}, w_{i2}, \\dots w_{iN})\\) is the \\(i^{th}\\) row of W. Here \\(G_i \\in \\mathbb{R}^{N \\times N}\\), where entry \\(G_{i}(j.l), \\; 1 \\leq j,l \\leq N\\) can be represented as: \\[ G_{i}(j,l) = \\begin{cases} (\\vec{x}_i - \\vec{x}_j)^T (\\vec{x}_i - \\vec{x}_l) &amp; j,l \\in N_i^k \\\\ 0 &amp; j \\; or \\; l \\notin N_i^k \\end{cases} \\] The \\((j,l)\\) entry of \\(G_i\\) is actually the inner product of \\(\\vec{x}_j\\) and \\(\\vec{x}_l\\) when centered around \\(\\vec{x}_i\\). From this expression, we know that actually \\(G_i\\) is a sparse matrix and can be reduced to a compact matrix \\(\\tilde{G}_i \\in \\mathbb{R}^{k \\times k}\\) that eliminates those empty columns and rows. \\[\\begin{align} \\tilde{G}_i &amp; = (\\vec{x}_{i[1]} - \\vec{x}_i, \\dots, \\vec{x}_{i[k]} - \\vec{x}_i)^T (\\vec{x}_{i[1]} - \\vec{x}_i, \\dots, \\vec{x}_{i[k]} - \\vec{x}_i) \\\\ &amp; = Q_i^T Q_i \\end{align}\\] where \\([1]\\) denotes the first entry in \\(N_i^k\\). So \\(\\tilde{G}_i\\) is actually a real symmetric and positive semi-definite matrix. Now let’s go back to deal with the optimization function — Equation 1 can be solved with Lagrange multiplier given that it has only equality constraints. (More details about the use of Lagrange multiplier can be found in [Lagrange multiplier]https://en.wikipedia.org/wiki/Lagrange_multiplier) Optimizing Equation 1 is equivalent to minimizing (for \\(\\forall 1 \\leq i \\leq N\\)) \\[ f(\\vec{w}_i, \\lambda) = \\vec{w}^T_i G_i \\vec{w}_i - \\lambda (\\vec{w}^T_i \\mathbf{1}_k -1) \\] which has the result: \\[ \\vec{w}_i^{\\star} = \\frac{\\tilde{G}^{-}_i \\mathbf{1}_k}{\\mathbf{1}^T_k \\tilde{G}^{-}_i \\mathbf{1}_k} \\] Complement: As discussed before, we can only proof that \\(\\tilde{G}_i\\) is positive semi-definite, however, we cannot ensure that it is positive definite, which means \\(\\tilde{G}_i\\) is not necessarily invertible. That is why we use the generalize inverse sign here. In practice, it can be done through performing SVD on \\(\\tilde{G}_i\\) and select the first few large singular values and eliminate the rest. Then when computing \\(\\tilde{G}^{-}_i\\), just do the reciprocal of these retained singular values. Actually \\(\\tilde{G}_i\\) only has \\(d\\) (the intrinsic original dimension if you forget) relatively large eigenvalues. The rest are either very small or zeros. So it is very likely that \\(\\tilde{G}_i\\) is singular, making the computation result highly unstable. In {[6]}, the authors proposed to address this issue through regularizing \\(\\tilde{G}_i\\). \\[ \\tilde{G}_i \\leftarrow \\tilde{G}_i+ \\left(\\frac{\\Delta^2}{k}\\right) \\operatorname{Tr}\\left(\\tilde{G}_i\\right) \\mathbf{I} \\] Here \\(Tr(\\tilde{G}_i)\\) denotes the trace of \\(\\tilde{G}_i\\) and \\(\\Delta \\ll 1\\). 6.3.2.3 Embedding In the previous step, we have recovered the optimal weight matrix \\[ \\mathbf{W} = \\left(\\begin{array}{l} \\vec{w}_1^T \\\\ \\vdots \\\\ \\vec{w}_N^T \\end{array}\\right) \\] The optimal weights \\(\\mathbb{W}\\) reflects local, linear geometry around each \\(\\vec{x}_i\\), thus if the configuration \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N \\in \\mathbb{R}^d\\) are the lower dimensional representation, they should also “match” the local geometry. Following this idea, the objective function is: (2) \\[ \\begin{aligned} &amp; \\underset{\\mathbf{Y}}{\\text{argmin}} \\sum_{i=1}^N \\left\\| \\vec{y}_i - \\sum_{j=1}^{N} w_{ij} \\vec{y}_j^T \\right\\|^2 \\\\ = &amp; \\underset{\\mathbf{Y}}{\\text{argmin}} \\sum_{i=1}^N \\left\\| \\sum_{j=1}^N w_{ij} (\\vec{y}_i - \\vec{y}_j)^T \\right\\|^2 \\\\ = &amp; \\underset{\\mathbf{Y}}{\\text{argmin}} \\left\\| \\mathbf{Y - WY} \\right\\|_F^2 \\\\ = &amp; \\underset{\\mathbf{Y}}{\\text{argmin}} \\left\\| (\\mathbf{I}_N - \\mathbf{W}) \\mathbf{Y} \\right\\|_F^2 \\\\ = &amp; \\underset{\\mathbf{Y}}{\\text{argmin}} \\; \\text{Tr} \\left[ \\mathbf{Y}^T (\\mathbf{I}_N - \\mathbf{W})^T (\\mathbf{I}_N - \\mathbf{W}) \\mathbf{Y} \\right] \\end{aligned} \\] where \\(\\mathbf{Y}=(\\vec{y}_1 | \\vec{y}_2 | \\dots | \\vec{y}_N)^T\\) There are two constraints: \\(\\mathbf{1}_N^T \\mathbf{Y} = \\vec{0}\\). This forces \\(\\vec{y}\\) to be centered \\(\\frac{1}{N} \\mathbf{Y}^T \\mathbf{Y} = \\mathbf{I}_d\\). This fixes rotation and scaling. Key Observation Considering the final expression of Equation 2, the optimization function is now equivalent to finding \\(\\vec{y}_i\\)s that minimizes \\(\\mathbf{Y}^T (\\mathbf{I}_N - \\mathbf{W})^T (\\mathbf{I}_N - \\mathbf{W}) \\mathbf{Y}\\). Here we introduce \\(\\mathbf{M} = (\\mathbf{I}_N - \\mathbf{W})^T (\\mathbf{I}_N - \\mathbf{W})\\), which is a positive semi-definite matrix. Since \\(\\mathbf{M} \\mathbf{1}_N = (\\mathbf{I} - \\mathbf{W})^T (\\mathbf{1}_N - \\mathbf{W} \\mathbf{1}_N) = \\vec{0}\\), \\(\\mathbf{1}_N\\) is an eigen-vector of \\(\\mathbf{M}\\) with eigenvalue zero. \\[ \\begin{aligned} \\mathbf{Y}^T (\\mathbf{I}_N - \\mathbf{W})^T (\\mathbf{I}_N - \\mathbf{W}) \\mathbf{Y} = \\mathbf{Y}^T \\mathbf{M} \\mathbf{Y} \\end{aligned} \\] From constraint (b), we know that columns of \\(\\mathbf{Y}\\) are orthogonal to each other. As a result, this whole problem can be simplified to finding the eigen-vectors of \\(\\mathbf{M}\\) with the smallest eigenvalues. Compute eigen-vectors with the smallest \\(d+1\\) eigenvalues \\(0=\\lambda_1 \\leq \\lambda_2 &lt; \\dots &lt; \\lambda_{d+1}\\), eliminate \\(\\mathbf{1}_N\\) (the first one). The remaining \\(d\\) vectors are respectively \\(\\vec{v}_2, \\vec{v}_3, \\dots \\vec{v}_{d+1} \\in \\mathbb{R}^N\\). So \\(\\mathbf{Y} = (\\vec{v}_2 | \\vec{v}_2 | \\dots | \\vec{v}_{d+1})\\), we successfully recover the corresponding \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N \\in \\mathbb{R}^d\\). An illustration of the algorithm In the original paper [4], the authors provide a very intuitive plot that summarizes the above three steps. LLE_illustration Parameter Tuning There are two parameters to tune in LLE, i.e. (the number of neighbors: \\(k \\,\\); the dimension of the recovered configuration: \\(d \\,\\)). For selection of \\(d\\), we usually use a reverse scree plot and find the elbow point. It is worth noting that we are choosing the smallest \\(d+1\\) eigenvalues and compute their corresponding eigen-vectors here. Since the eigen-vectors and eigenvalues of a particular matrix is super sensitive to any sort of noises or perturbations, especially for those small eigenvalues, it is hard to accurately derive the corresponding eigen-vectors \\(\\vec{v}_2, \\dots, \\vec{v}_{d+1}\\). This is called ill-conditioned eigen-problem. Choose the optimal \\(k\\) LLE seeks to preserve local structure through nearest neighbor connections. This is the key point to LLE. As a result, we may use the neighbor set of the original \\(\\vec{x}_1, \\vec{x}_2, \\dots, \\vec{x}_N \\in \\mathbb{R}^D\\) and \\(\\vec{y}_1, \\vec{y}_2, \\dots, \\vec{y}_N \\in \\mathbb{R}^d\\) as a criteria. As explained before, we use \\(N_i^k\\) to denote the indices of k-nearest neighbors to \\(\\vec{x}_i\\). Similarly, we can also use \\(V_i^k\\) to denote the indices of k-nearest neighbors to \\(\\vec{y}_i\\). They should be as close as possible. So our objective function here is: \\[ Q(k)= \\frac{\\sum_{i=1}^N \\left| N_i^k \\cap V_i^k \\right|}{Nk} \\] Plot \\(Q(k)\\) against \\(k\\), select \\(k^{\\star}\\) where the increase of \\(Q(k)\\) becomes negligible. 6.3.3 Strengths and Weaknesses of LLE 6.3.3.1 Strengths High Computation Efficiency The low computation cost of LLE algorithm may be its most shinning advantage over other manifold learning methods, and it is actually one of its biggest selling point when it was first introduced. The LLE algorithm Involves solving a sparse eigen problem, with computational complexity of roughly \\(O(N^2 d^2 + N d^3)\\) where \\(N\\) is the number of data points and \\(d\\) is the dimension of the recovered configuration. In comparison, ISOMAP requires computing shortest paths between all pairs of points, which is typically done using Dijkstra’s or Floyd-Warshall algorithm, leading to a complexity of \\(O(N^2 log N)\\) or \\(O(N^3)\\) respectively. Then, it involves eigen decomposition similar to classical MDS which is \\(O(N^3)\\). In practice, \\(d \\ll N\\), hence the computation cost of LLE is lower than that of ISOMAP in most cases. Few parameters to tune There are only two parameters to tune, respectively the number of neighbors included in the map: \\(k\\), and the dimensional of the original configuration: \\(d\\). In addition, there exist clear methods to find the optimal \\(k\\) and \\(d\\), as stated in the previous part. This makes LLE algorithm easy to find the optimal parameters. 6.3.3.2 Weaknesses Sensitivity to tuning parameters The result of LLE is quite sensitive to its two control parameters: the number of neighbors \\(k \\,\\) and the dimensional of the original configuration: \\(d\\). Here we use the Swiss Roll example to illustrate this. LLE is optimal at \\(k=45\\). However, when \\(k=40\\), the recovered lower-dimensional configuration is wrong (Green points and yellow points overlap, which is not the case in Swiss Roll); and when we slightly increase \\(k\\) to 50, the recovered two-dimensional expression is not necessarily a rectangle. set.seed(42) # Install and load necessary packages # install.packages(&quot;dimRed&quot;) # install.packages(&quot;rgl&quot;) library(dimRed) library(rgl) library(Rdimtools) library(scatterplot3d) # Generate Swiss roll-shaped data S &lt;- rep(0, 2000) Swiss &lt;- matrix(NA, nrow = 2000, ncol = 3) for (n in 1:2000) { s &lt;- runif(1, min = 3 * pi / 2, max = 9 * pi / 2) t &lt;- runif(1, min = 0, max = 20) S[n] &lt;- s Swiss[n, ] &lt;- c(s * cos(s), t, s * sin(s)) } par(mfrow = c(2,2)) scatterplot3d(Swiss, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S), xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(do.lle(Swiss, ndim = 2, type = c(&quot;knn&quot;,40))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=40&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss, ndim = 2, type = c(&quot;knn&quot;,45))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=45&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss, ndim = 2, type = c(&quot;knn&quot;,50))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=50&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) Vulnerable to sparse or unevenly-distributed samples The vulnerability towards sparsity and uneven distribution exists in almost all manifold learning methods, including ISOMAP, as we have illustrated in the previous section. LLE is not immune to this either. When a data set is unevenly distributed, since LLE relies on the original Euclidean distance metric, it tends to select neighbors from a singular direction where these neighbors are densely clustered. Clearly, using these selected neighbors to reconstruct the reference point results in significant redundancy in that specific direction. Concurrently, essential information from other directions or regions is not retained for the reconstruction of the reference point. As a result, these selected neighbors are inadequate for accurately representing and reconstructing the reference point. Consequently, much of the intrinsic structure and internal features will be lost after dimension reduction using LLE. We borrow the “stretched” Swiss Roll example from ISOMAP section and try LLE this time. LLE also cracks in this scenario, regardless of the chosen parameter \\(k\\). set.seed(42) S &lt;- rep(0,2000) Swiss_sparse &lt;- matrix(NA, nrow = 2000, ncol = 3) for( n in 1:2000){ s &lt;- runif(1, min = 3*pi/2, max = 9*pi/2) t &lt;- runif(1, min = 0, max = 60) S[n] &lt;- s Swiss_sparse[n, ] &lt;- c( s*cos(s), t, s*sin(s) ) } par(mfrow = c(2,2)) scatterplot3d(Swiss_sparse, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S), main = &quot;Swiss Roll with Sparse Region&quot;, xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(do.lle(Swiss_sparse, ndim = 2, type = c(&quot;knn&quot;,35))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=35&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss_sparse, ndim = 2, type = c(&quot;knn&quot;,40))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=40&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss_sparse, ndim = 2, type = c(&quot;knn&quot;,45))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=45&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) In addition, we manually create a Swiss Roll with some sparse regions and try LLE on it. From naked eye, these sparse regions won’t affect the overall manifold look. However, since LLE utilizes the local structure, it is greatly affected. library(scatterplot3d) library(Rdimtools) set.seed(42) S &lt;- rep(0,2000) Swiss &lt;- matrix(NA, nrow = 2000, ncol = 3) for(n in 1:2000){ s &lt;- runif(1, min = 3*pi/2, max = 9*pi/2) t &lt;- runif(1, min = 0, max = 15) S[n] &lt;- s Swiss[n, ] &lt;- c( s*cos(s), t, s*sin(s) ) } # Manually create a sparse region mask &lt;- Swiss[,2] &gt; 5 &amp; Swiss[,2] &lt; 10 &amp; Swiss[,3] &gt; -5 &amp; Swiss[,3] &lt; 5 Swiss_masked &lt;- Swiss[!mask, ] S &lt;- S[!mask] par(mfrow = c(2,2)) scatterplot3d(Swiss_masked, color = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S), main = &quot;Swiss Roll with Sparse Region&quot;, xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3])) plot(do.lle(Swiss_masked, ndim = 2, type = c(&quot;knn&quot;,20))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=20&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss_masked, ndim = 2, type = c(&quot;knn&quot;,30))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=30&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) plot(do.lle(Swiss_masked, ndim = 2, type = c(&quot;knn&quot;,50))$Y, xlab = &quot;1st Dimension&quot;, ylab = &quot;2nd Dimension&quot;, main = &quot;K=50&quot;, col = myColorRamp(c(&quot;red&quot;,&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;yellow&quot;), S)) Sensitivity to noise LLE is super sensitive to noise. Even a small noise would cause failure in deriving low dimensional configurations. Justin Wang, et.al utilize various visualization examples to illustrate this drawback in their paper [7], you may take a look if you are interested. Various algorithms have been developed to address this issue, i.e., Robustly Locally Linear Embedding (RLLE) [8], and Locally Linear Embedding with Additive Noise (LLEAN) [7]. The former works well when outliers exist, while the latter has a satisfactory performance when the original points are distorted with noises. References "],["autoencoders-aes.html", "6.4 Autoencoders (AEs)", " 6.4 Autoencoders (AEs) 6.4.1 Introduction Autoencoders, emanating from the domain of neural network research, represent a class of unsupervised deep learning models. At its core, autoencoders seek to learn a compressed, efficient representation of input data by leveraging a network structure that encodes the data into a reduced dimensionality and subsequently decodes it to reconstruct the original data. The typical architecture of an autoencoder comprises three main components: Encoder: A function \\(f(\\vec{x})\\) that compresses the input \\(\\vec{x}\\)s into a latent representation. Latent Space: The reduced dimensionality representation, often denoted as \\(\\vec{z}\\), where \\(\\vec{z} = f(\\vec{x})\\). Decoder: A function \\(g(\\vec{z})\\) that aims to reconstruct the original input from the latent representation. The primary objective during the training phase of an autoencoder is to minimize the reconstruction error, often quantified using metrics such as Mean Squared Error (MSE) between the input data and its reconstructed counterpart. The minimization forces the model to capture salient features of the data in the latent space, thereby enabling efficient data compression, noise reduction, and feature extraction. The utility of Autoencoders has been demonstrated in a wide array of applications, from dimensionality reduction, anomaly detection, denoising, to more complex tasks such as generating new data instances. Variations and extensions of the basic Autoencoder model, including Variational Autoencoders (VAEs) and Denoising Autoencoders, have further broadened their applicability by introducing probabilistic interpretations and noise robustness, respectively. In the broader context of machine learning and artificial intelligence, Autoencoders exemplify the power of unsupervised learning paradigms, emphasizing the capability of neural networks to derive meaningful representations from data without explicit labeling. 6.4.2 Algorithm 6.4.2.1 Notations: Input - Dataset \\(\\mathbf{X} = \\{\\vec{x}_1, \\vec{x}_2, \\dots, \\vec{x}_n\\}\\) where \\(\\vec{x}_i\\) represents each data sample. - Encoder function with parameters \\(\\theta_e\\): \\(f_{\\theta_e}(x)\\) - Decoder function with parameters \\(\\theta_d\\): \\(g_{\\theta_d}(z)\\) - Objective function to measure reconstruction error, e.g., Mean Squared Error (MSE). Output: - Trained parameters \\(\\theta_e^{\\star}\\) and \\(\\theta_d^{\\star}\\) that minimize the reconstruction error. 6.4.2.2 Steps Breakdown: Initialization: Initialize network weights (parameters \\(\\theta_e\\) for encoder and \\(\\theta_d\\) for decoder) using a method such as Xavier initialization or random initialization. Forward Pass: For each data sample \\(\\vec{x}_i\\): Encode input data sample to get the latent representation: \\[ \\vec{z}_i = f_{\\theta_e}(\\vec{x}_i) \\] Decode the latent representation to get the reconstructed data: \\[ \\vec{x}&#39;_i = g_{\\theta_d}(\\vec{z}_i) \\] Compute Loss: Calculate the reconstruction loss for the sample. For example, if using MSE: \\[ L(\\vec{x}_i, \\vec{x}&#39;_i) = ||\\vec{x}_i - \\vec{x}&#39;_i||^2 \\] Accumulate the loss for all samples to get the total loss. Backward Pass: Using a method like gradient descent or one of its variants (e.g., Adam, RMSProp): Compute the gradients of the loss with respect to the network parameters (\\(\\theta_e\\) and \\(\\theta_d\\)). Update the weights in the direction that minimizes the loss. Iterate: Repeat steps 2-4 for a predefined number of epochs or until the change in reconstruction error between epochs falls below a specified threshold. Model Retrieval: After training, retrieve the encoder \\(f_{\\theta_e^{\\star}}\\) and decoder \\(g_{\\theta_d^{\\star}}\\) with the optimal parameters \\(\\theta_e^{\\star}\\) and \\(\\theta_d^{\\star}\\). 6.4.3 Example # # keras provides an interface to the Keras deep learning library in Python # library(keras) # # # load the Minst dataset # mnist &lt;- dslabs::read_mnist( # path = NULL, # download = FALSE, # destdir = tempdir(), # url = &quot;https://www2.harvardx.harvard.edu/courses/IDS_08_v2_03/&quot;, # keep.files = TRUE # ) # # x_train &lt;- mnist$train$images # x_test &lt;- mnist$test$images # # Reshape the data and normalize # gc() # library(reticulate) # x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784)) # x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784)) # x_train &lt;- x_train / 255 # x_test &lt;- x_test / 255 # encoding_dim &lt;- 32 # for compression # # input_img &lt;- layer_input(shape = c(784)) # # # Encoder layers # encoded &lt;- layer_dense(input_img, encoding_dim, activation = &#39;relu&#39;) # # # Decoder layers # decoded &lt;- layer_dense(encoded, 784, activation = &#39;sigmoid&#39;) # # # Create the autoencoder model # autoencoder &lt;- keras_model(input_img, decoded) # autoencoder %&gt;% fit( # x_train, x_train, # epochs = 50, # batch_size = 256, # shuffle = TRUE, # validation_data = list(x_test, x_test) # ) # # Get the reconstructed images # decoded_imgs &lt;- autoencoder %&gt;% predict(x_test) # # # Display the original and reconstructed images # par(mfrow=c(2,10), mai=c(0.2, 0.2, 0.2, 0.2)) # for (i in 1:10) { # # Original # img &lt;- array_reshape(x_test[i,], c(28, 28)) # image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, main = &quot;Original&quot;) # # Reconstruction # img &lt;- array_reshape(decoded_imgs[i,], c(28, 28)) # image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, main = &quot;Reconstructed&quot;) # } "],["additional-methods.html", "6.5 Additional methods", " 6.5 Additional methods "],["exercises-3.html", "6.6 Exercises", " 6.6 Exercises "],["clustering-1.html", "Chapter 7 Clustering ", " Chapter 7 Clustering "],["hierarchical.html", "7.1 Hierarchical", " 7.1 Hierarchical "],["center-based.html", "7.2 Center-based", " 7.2 Center-based "],["model-based.html", "7.3 Model-based", " 7.3 Model-based 7.3.1 k-means 7.3.2 k-mediods "],["spectral.html", "7.4 Spectral", " 7.4 Spectral "]]
