<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Minimizing clustering loss functions | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Minimizing clustering loss functions | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Minimizing clustering loss functions | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="center-based-clustering.html"/>
<link rel="next" href="average-silhouette.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="exercises.html"><a href="exercises.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="exercises.html"><a href="exercises.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="exercises.html"><a href="exercises.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="exercises.html"><a href="exercises.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pca.html"><a href="sec-pca.html#pca-in-practice"><i class="fa fa-check"></i><b>4.1.2</b> PCA in Practice</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding an NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
<li class="chapter" data-level="4.3.6" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-and-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.3.6</b> NMF and Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernel-pca.html"><a href="kernel-pca.html"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-on-a-manifold.html"><a href="data-on-a-manifold.html"><i class="fa fa-check"></i><b>6.1</b> Data on a manifold</a></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html"><i class="fa fa-check"></i><b>6.4</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.5</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-2"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-3"><i class="fa fa-check"></i><b>6.5.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.5.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.5.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.6</b> Additional methods</a></li>
<li class="chapter" data-level="6.7" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="center-based-clustering.html"><a href="center-based-clustering.html"><i class="fa fa-check"></i><b>7.1</b> Center-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="center-based-clustering.html"><a href="center-based-clustering.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="center-based-clustering.html"><a href="center-based-clustering.html#k-center-and-k-medoids"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(k\)</span>-center and <span class="math inline">\(k\)</span>-medoids</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="minimizing-clustering-loss-functions.html"><a href="minimizing-clustering-loss-functions.html"><i class="fa fa-check"></i><b>7.2</b> Minimizing clustering loss functions</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="minimizing-clustering-loss-functions.html"><a href="minimizing-clustering-loss-functions.html#lloyds-algorithm-for-k-means"><i class="fa fa-check"></i><b>7.2.1</b> Lloyd’s Algorithm for <span class="math inline">\(k\)</span>-Means</a></li>
<li class="chapter" data-level="7.2.2" data-path="minimizing-clustering-loss-functions.html"><a href="minimizing-clustering-loss-functions.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>7.2.2</b> Strengths and Weaknesses</a></li>
<li class="chapter" data-level="7.2.3" data-path="minimizing-clustering-loss-functions.html"><a href="minimizing-clustering-loss-functions.html#addressing-limitations-computational"><i class="fa fa-check"></i><b>7.2.3</b> Addressing limitations (computational)</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="average-silhouette.html"><a href="average-silhouette.html"><i class="fa fa-check"></i><b>7.3</b> Average Silhouette</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="r-child-topicshierarchical-clustering.html"><a href="r-child-topicshierarchical-clustering.html"><i class="fa fa-check"></i><b>8</b> <code>{r child = 'topics/Hierarchical-Clustering.Rmd'} #</code></a></li>
<li class="chapter" data-level="9" data-path="r-child-topicsmodel-based-clustering.html"><a href="r-child-topicsmodel-based-clustering.html"><i class="fa fa-check"></i><b>9</b> <code>{r child = 'topics/Model-Based-Clustering.Rmd'} #</code></a></li>
<li class="chapter" data-level="10" data-path="r-child-topicsspectral-clustering.html"><a href="r-child-topicsspectral-clustering.html"><i class="fa fa-check"></i><b>10</b> <code>{r child = 'topics/Spectral-Clustering.Rmd'} #</code></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="minimizing-clustering-loss-functions" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Minimizing clustering loss functions<a href="minimizing-clustering-loss-functions.html#minimizing-clustering-loss-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For any of the three preceding methods, we want to find an optimal partitioning and/or set of centers which minimize their associated loss. One naive approach would be to search or all possible partitioning of our data into <span class="math inline">\(k\)</span> clusters (<span class="math inline">\(k\)</span>-means) or all possible choices of <span class="math inline">\(k\)</span> center points from our data, but one can imagine how computationally unfeasible this approach becomes for even moderate amounts of data. Instead, we will turn to greedy, iterative algorithms which converge to (locally) optimal solutions. The standard approach for <span class="math inline">\(k\)</span>-means is based on Lloyd’s algorithm with a special initialization to avoid convergence to local minima. Later versions of this text will discuss greedy approaches for <span class="math inline">\(k\)</span>-center and the standard partitioning around medians (PAM) algorithm for <span class="math inline">\(k\)</span>-medoids.</p>
<div id="lloyds-algorithm-for-k-means" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Lloyd’s Algorithm for <span class="math inline">\(k\)</span>-Means<a href="minimizing-clustering-loss-functions.html#lloyds-algorithm-for-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Initialize</strong>: Choose initial centers randomly from the data.</li>
<li><strong>Cluster Assignment</strong>: Assign each point to the nearest center.</li>
<li><strong>Center Update</strong>: Recompute cluster centers based on current assignments.</li>
<li><strong>Repeat</strong> steps 2-3 until convergence.</li>
</ol>
<p>:::{.example name=“Lloyd’s algorithm and Iris Data}
In this example, we’ll consider the three dimensional <strong>iris</strong> data. For visualization, we’ll plot the first two PC. Centers will be outlined in black.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-200-1.png" width="672" />
:::</p>
</div>
<div id="strengths-and-weaknesses" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Strengths and Weaknesses<a href="minimizing-clustering-loss-functions.html#strengths-and-weaknesses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The advantages and disadvantages of center-based clustering algorithms can be separated into two main categories: computational and geometric. Some potential issues can be mitigated such as outliers and scale imbalance In theory, the worst case for convergence takes <span class="math inline">\(\propto N^{k+1}\)</span> iterations, but Lloyd’s algorithm is typically very fast in practice. Most implementations, allow the user to set a maximum number of iterations and converge to approximate suboptimal solutions if the maximum number of iterations is reached. PAM and greedy <span class="math inline">\(k\)</span>-center algorithms are typically much, much slower in practice.</p>
<p>Centers can be interpreted as <em>representative</em> prototypes of each cluster allowing new data to be merged into cluster with nearest center. However, sample averages in <span class="math inline">\(k\)</span>-means may not resemble actual data points whereas centers in <span class="math inline">\(k\)</span>-center and <span class="math inline">\(k\)</span>-medoids are restricted to observations. Furthermore, clusters are based off interpretable notion of (Euclidean) distance so that points in a cluster are closer to one another than to points in other clusters.</p>
<ul>
<li>Requires choosing <span class="math inline">\(K\)</span> in advance (more on this shortly)
<ul>
<li>No connection between <span class="math inline">\(K\)</span>-means cluster solution and <span class="math inline">\((K+1)\)</span>-means cluster solutions</li>
<li>Need to rerun for different values of <span class="math inline">\(K\)</span></li>
</ul></li>
<li>Convergence is dependent on initial condition</li>
</ul>
<p><img src="_main_files/figure-html/unnamed-chunk-201-1.png" width="672" /></p>
</div>
<div id="addressing-limitations-computational" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Addressing limitations (computational)<a href="minimizing-clustering-loss-functions.html#addressing-limitations-computational" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Avoiding convergence to poor local optimal
<ul>
<li>Run many different ICs to convergence and pick best solution based on <span class="math inline">\(T_K\)</span> and/or</li>
<li>Use better initialization, e.g. <span class="math inline">\(k\)</span>-means++, which is default in <em>sklearn</em></li>
</ul></li>
<li>Choosing <span class="math inline">\(K\)</span>
<ul>
<li>Many different methods which tend to give (slightly) different results</li>
<li>Direct methods: optimize a target score
- Elbow, sihouette diagnostics</li>
<li>Testing methods: comparison to null model
- Gap Statistic
<img src="_main_files/figure-html/unnamed-chunk-202-1.png" width="672" /></li>
</ul></li>
</ul>
<div id="elbow-plot" class="section level4 hasAnchor" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> Elbow plot<a href="minimizing-clustering-loss-functions.html#elbow-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The inertia will decrease as the number of clusters increases.<br />
</li>
<li>To choose <span class="math inline">\(K\)</span> we find the inertia as a function of the number of clusters.</li>
</ul>
</div>
<div id="calinski-harabasz-index" class="section level4 hasAnchor" number="7.2.3.2">
<h4><span class="header-section-number">7.2.3.2</span> Calinski-Harabasz index<a href="minimizing-clustering-loss-functions.html#calinski-harabasz-index" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="gap-statistic" class="section level4 hasAnchor" number="7.2.3.3">
<h4><span class="header-section-number">7.2.3.3</span> Gap Statistic<a href="minimizing-clustering-loss-functions.html#gap-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Due to Tibshirani et al., (JRSS-B, 2001).</p></li>
<li><p>Idea: For a particular choice of K clusters</p>
<ul>
<li><p>Compare the total within cluster variation to the expected within-cluster variation under the assumption that the data have no obvious clustering (i.e., randomly distributed).</p></li>
<li><p>Can be used to select an optimal number of clusters or</p></li>
<li><p>As evidence that there is no clustering structure</p></li>
</ul></li>
</ul>
</div>
<div id="gap-statistic-algorithm" class="section level4 hasAnchor" number="7.2.3.4">
<h4><span class="header-section-number">7.2.3.4</span> Gap Statistic: Algorithm<a href="minimizing-clustering-loss-functions.html#gap-statistic-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>Cluster the data at varying number of total clusters <span class="math inline">\(K\)</span>. Let <span class="math inline">\(T_K\)</span> be the total within-cluster sum of squared distances.</p></li>
<li><p>Generate <span class="math inline">\(B\)</span> reference data sets of size <span class="math inline">\(N\)</span>, with the simulated values of variable <span class="math inline">\(j\)</span> uniformly generated over the range of the observed variable <span class="math inline">\(j\)</span> . Typically <span class="math inline">\(B = 500.\)</span></p></li>
<li><p>For each generated data set <span class="math inline">\(b=1,\dots, B\)</span> perform the clustering for each <span class="math inline">\(K\)</span>. Compute the total within-cluster sum of squared distances <span class="math inline">\(T_K^{(b)}\)</span>.</p></li>
<li><p>Compute the Gap statistic <span class="math display">\[Gap(K) = \bar{w} - \log (T_K), \qquad \bar{w} = \frac{1}{B} \sum_{b=1}^B \log(T_K^{(b)})\]</span></p></li>
<li><p>Compute the sample variance <span class="math display">\[var(K) = \frac{1}{B-1}\sum_{b=1}^B \left(\log(T_K^{(b)}) - \bar{w}\right)^2,\]</span>
and define <span class="math inline">\(s_K = \sqrt{var(K)(1+1/B)}\)</span></p></li>
<li><p>Finally, choose the number of clusters as the smallest K such that
<span class="math display">\[Gap(K) \ge Gap(K + 1) − s_{K+1}\]</span>
#### Silhouette plots and coefficients</p></li>
</ol>
<p>For a given clustering, we would like to determine how well each sample is clustered.</p>
<p><span class="math display">\[\begin{split}
a_i &amp;= \text{avg. dissimilarity of } \vec{x}_i \text{with all other samples in same cluster} \\
b_i &amp;= \text{avg. dissimilarity of } \vec{x}_i \text{ with samples }\text{ in } the \text{ } closest \text{ cluster}
\end{split}\]</span></p>
<p>We then define <span class="math display">\[s_i = \frac{b_i - a_i}{\max\{a_i,b_i\}} \in (-1,1)\]</span>
as the silhouette for <span class="math inline">\(\vec{x}_i.\)</span></p>
<ul>
<li>Observations with <span class="math inline">\(s_i \approx 1\)</span> are well clustered</li>
<li>Observations with <span class="math inline">\(s_i \approx 0\)</span> are between clusters</li>
<li>Observations with <span class="math inline">\(s_i &lt; 0\)</span> are probably in wrong cluster</li>
</ul>
<p>We can use any dissimilarity!</p>
<ul>
<li><p>Can use silhouettes as diagnostics of any method!</p></li>
<li><p>A great clustering will have high silhouettes for all samples.</p></li>
<li><p>To compare different values of <span class="math inline">\(K\)</span> (and different methods), we can compute the average silhouette
<span class="math display">\[S_K = \frac{1}{N}\sum_{i=1}^N s_i\]</span>
over a range of values of <span class="math inline">\(K\)</span> and choose the <span class="math inline">\(K\)</span> which maximizes <span class="math inline">\(S_K\)</span>.</p></li>
</ul>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="center-based-clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="average-silhouette.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/young1062/introUL06-clustering.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
