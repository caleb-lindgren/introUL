<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.4 Linear Algebra | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="2.4 Linear Algebra | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.4 Linear Algebra | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="expectation-mean-and-covariance.html"/>
<link rel="next" href="exercises.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability-review.html"><a href="probability-review.html"><i class="fa fa-check"></i><b>2</b> Probability Review</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#symmetry-positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Symmetry, Positive Definiteness, and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-methods.html"><a href="linear-methods.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-1-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation 1: Iterative Projections</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pca.html"><a href="sec-pca.html#derivation-2-optimal-linear-subspace"><i class="fa fa-check"></i><b>4.1.2</b> Derivation 2: Optimal Linear Subspace</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="singular-value-decomposition-1.html"><a href="singular-value-decomposition-1.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="manifold-learning.html"><a href="manifold-learning.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="background.html"><a href="background.html#data-on-a-manifold"><i class="fa fa-check"></i><b>6.1.1</b> Data on a manifold</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction-1"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-2"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.4</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-3"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.4.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.5</b> Additional methods</a></li>
<li class="chapter" data-level="6.6" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering-1.html"><a href="clustering-1.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hierarchical.html"><a href="hierarchical.html"><i class="fa fa-check"></i><b>7.1</b> Hierarchical</a></li>
<li class="chapter" data-level="7.2" data-path="center-based.html"><a href="center-based.html"><i class="fa fa-check"></i><b>7.2</b> Center-based</a></li>
<li class="chapter" data-level="7.3" data-path="model-based.html"><a href="model-based.html"><i class="fa fa-check"></i><b>7.3</b> Model-based</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-based.html"><a href="model-based.html#k-means"><i class="fa fa-check"></i><b>7.3.1</b> k-means</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-based.html"><a href="model-based.html#k-mediods"><i class="fa fa-check"></i><b>7.3.2</b> k-mediods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>7.4</b> Spectral</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-algebra" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Linear Algebra<a href="linear-algebra.html#linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="assumed-background" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Assumed Background<a href="linear-algebra.html#assumed-background" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This text assumes familiarity with definitions from a standard undergraduate course in linear algebra including but not limited to linear spaces, subspaces, spans and bases, and matrix multiplication. However, we have elected to provide review of some of the most commonly used ideas in the methods we’ll cover in the following subsections. For a more thorough treatment of linear algebra, please see REFERENCES</p>
</div>
<div id="interpretations-of-matrix-multiplication" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Interpretations of Matrix Multiplication<a href="linear-algebra.html#interpretations-of-matrix-multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this text, comfort with common calculations in linear algebra will be very important. Herein, we assume the reader has some exposure to these materials at an undergraduate level including the summation of vectors or matrices. The familiarity with matrix-vector and matrix-matrix multiplication will play a central role as we have already seen in the case of the data matrix formulation of the sample mean and sample covariance. However, rote familiarity with computation will not be sufficient to build intuition for the methods we’ll discuss. As such, we’ll begin with a review of a few important ways one can view matrix-vector (and matrix-matrix) multiplication which will be helpful later. Those who feel comfortable with the myriad interpretations of matrix multiplication in terms of linear combinations of the rows and columns may skip to the next section.</p>
<p>Suppose we have matrix <span class="math inline">\({\bf A}\in\mathbb{R}^{m\times n}\)</span> and vector <span class="math inline">\(\vec{x}\in\mathbb{R}^n\)</span>. If we let <span class="math inline">\(\vec{a}_1^T,\dots, \vec{a}_M^T\in\mathbb{R}^n\)</span> denote the rows of <span class="math inline">\({\bf A}\)</span>, then the most commonly cited formula for computing <span class="math inline">\({\bf A}\vec{x}\)</span> is <span class="math display">\[{\bf A}\vec{x} = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_m^T\end{bmatrix} \vec{x} = \begin{bmatrix}\vec{a}_1^T \vec{x} \\ \vdots \\ \vec{a}_m^T\vec{x}\end{bmatrix}\]</span>
wherein we take the inner product of the rows of <span class="math inline">\({\bf A}\)</span> with vector <span class="math inline">\(\vec{x}\)</span>. We can expand this definition to matrix-matrix multiplication. If <span class="math inline">\({\bf B}\in\mathbb{R}^{n\times k}\)</span> has columns <span class="math inline">\(\vec{b}_1,\dots,\vec{b}_k\)</span> then
<span class="math display">\[({\bf AB})_{ij} = \vec{a}_i^T\vec{b}_j\]</span>
where we take the inner product of the <span class="math inline">\(i\)</span>th row of <span class="math inline">\({\bf A}\)</span> with the <span class="math inline">\(j\)</span>th column of <span class="math inline">\({\bf B}\)</span> to get the <span class="math inline">\(ij\)</span>th entry of <span class="math inline">\({\bf AB}.\)</span> This is perfectly reasonable method of computation, but alternative perspectives are helpful, particularly when we consider different factorization of the data matrix in later chapters..</p>
<p>Returning to <span class="math inline">\({\bf A}\vec{x}\)</span>, suppose now that <span class="math inline">\({\bf A}\)</span> has columns <span class="math inline">\(\vec{\alpha}_1,\dots,\vec{\alpha}_n\)</span> and <span class="math inline">\(\vec{x} = (x_1,\dots,x_n)^T\)</span>, then we may view <span class="math inline">\({\bf A}\vec{x}\)</span> as a linear combination of the columns of <span class="math inline">\({\bf A}\)</span> so that <span class="math display">\[{\bf A}\vec{x} = \begin{bmatrix}\vec{\alpha}_1  \,| &amp; \cdots &amp;|\, \vec{\alpha}_n \end{bmatrix} \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = x_1\vec{\alpha}_1 + \dots + x_n \vec{\alpha}_n = \sum_{j=1}^n x_j\vec{\alpha}_j.\]</span>
Here, we added vertical columns between the vectors <span class="math inline">\(\vec{\alpha}_i\)</span> to make clear that <span class="math inline">\(\begin{bmatrix}\vec{\alpha}_1 \,| &amp; \cdots &amp;|\, \vec{\alpha}_n \end{bmatrix}\)</span> is a matrix. We can extend this perspective to see that the columns of <span class="math inline">\({\bf AB}\)</span> are comprised of different linear combinations of the columns of <span class="math inline">\({\bf A}\)</span>. Specifically, the <span class="math inline">\(j\)</span> column of <span class="math inline">\({\bf AB}\)</span> is a linear combination of the columns of <span class="math inline">\({\bf A}\)</span> using the entries in the <span class="math inline">\(j\)</span>th column of <span class="math inline">\({\bf B}\)</span>. More specifically, the <span class="math inline">\(j\)</span>th column of <span class="math inline">\({\bf AB}\)</span> is the linear combination <span class="math display">\[\sum_{i=1}^n {\bf B}_{ij}\vec{\alpha}_{i}.\]</span></p>
<p>Our final observations follows by taking these insights on linear combinations of columns and transposing the entire operation. What can we say about the rows of <span class="math inline">\({\bf AB}\)</span>? We can rewrite <span class="math inline">\({\bf AB} = ({\bf B}^T{\bf A}^T)^T\)</span>. The columns of <span class="math inline">\({\bf B}^T{\bf A}^T\)</span> are linear combinations of the columns of <span class="math inline">\({\bf B}^T\)</span>. Since the columns of <span class="math inline">\({\bf B}^T\)</span> are the rows of <span class="math inline">\({\bf B}\)</span>, it follows that the rows of <span class="math inline">\({\bf AB} = ({\bf B}^T{\bf A}^T)^T\)</span> are linear combinations of the rows of <span class="math inline">\({\bf B}\)</span> with weights given by the entries in each row of <span class="math inline">\({\bf A}\)</span> respectively. In mathematical notation, the <span class="math inline">\(i\)</span>th row of <span class="math inline">\({\bf AB}\)</span> is <span class="math display">\[\sum_{j=1}^n {\bf A}_{ij} \vec{\beta}_j^T\]</span>
where <span class="math inline">\(\vec{\beta}_1^T,\dots, \vec{\beta}_n^T\)</span> are the rows of <span class="math inline">\({\bf B}.\)</span></p>
</div>
<div id="norms-and-distances" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Norms and Distances<a href="linear-algebra.html#norms-and-distances" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this text, we will use <span class="math inline">\(\| \cdot \|\)</span> to denote the usual Euclidean (or <span class="math inline">\(\ell_2\)</span>) norm, which for a vector, <span class="math inline">\(\vec{x} = (x_1,\dots,x_d)\in\mathbb{R}^d\)</span>, is
<span class="math display">\[\|\vec{x}\| = \left(\sum_{j=1}^d x_j^2 \right)^{1/2}.\]</span>
We may then define the Euclidean distance between two <span class="math inline">\(d\)</span>-dimension vectors <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span> to be <span class="math display">\[\|\vec{x}-\vec{y}\| = \left(\sum_{j=1}^d (x_j - y_j)^2\right)^{1/2}.\]</span>
Euclidean distance is the most commonly used notion of distance (or norm or metric) between two vectors, but it is far from the only option. We can consider the general <span class="math inline">\(\ell_p\)</span> norm <span class="math display">\[\|\vec{x}\|_p = \left(\sum_{j=1}^d x_j^p\right)^{1/p}\]</span> which coincides with the Euclidean norm for <span class="math inline">\(p=2\)</span>. Two other special cases include <span class="math inline">\(p=1\)</span> also known as the Manhattan distance and <span class="math inline">\(p = \infty\)</span> also known as the sup-norm <span class="math display">\[\|\vec{x}\|_\infty = \max_{j=1,\dots, d} |x_j|.\]</span></p>
<p>We can also extend this notions of vector norms to a measure of the norm of a matrix. Two important cases are the <span class="math inline">\(\ell_2\)</span> norm of a matrix and the Frobenius norm. For matrix <span class="math inline">\({\bf A}\in\mathbb{R}^{m\times n}\)</span>, this norm is
<span class="math display">\[\begin{equation}
\|{\bf A}\| = \sup_{\vec{x}\in\mathbb{R}^n \text{ s.t. } \vec{x}\ne \vec{0}} \frac{\|{\bf A}\vec{x}\|}{\|\vec{x}\|} = \sup_{\vec{x}\in\mathbb{R}^n \text{ s.t. }\|\vec{x}\|=1} \|{\bf A}\vec{x}\|.
\end{equation}\]</span>
You can interpret <span class="math inline">\(\|{\bf A}\|\)</span> as the largest relative change in the Euclidean length of a vector after it is multiplied by <span class="math inline">\({\bf A}\)</span>. The Frobenius extends the algebraic definition of a matrix to a matrix. For <span class="math inline">\({\bf A}\in\mathbb{R}^{m\times n}\)</span>, its Frobenius norm is
<span class="math display">\[\begin{equation}
\|{\bf A}\|_F = \left(\sum_{i=1}^m\sum_{j=1}^n {\bf A}_{ij}^2\right)^{1/2}.
\end{equation}\]</span>
The <span class="math inline">\(\ell_2\)</span> distance between two matrices is then <span class="math inline">\(\|{\bf A}-{\bf B}\|\)</span> and the Frobenius distance between two matrices is <span class="math inline">\(\|{\bf A} - {\bf B}\|\)</span> where both <span class="math inline">\({\bf A}\)</span> and <span class="math inline">\({\bf B}\)</span> have the same number of rows and columns.</p>
</div>
<div id="important-properties" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Important properties<a href="linear-algebra.html#important-properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A few additional definitions that we will use throughout the text are provided below without examples.</p>
<div class="definition">
<p><span id="def:def-symm" class="definition"><strong>Definition 2.4  (Symmetric Matric) </strong></span>A matrix <span class="math inline">\({\bf A}\in \mathbb{R}^{d\times d}\)</span> is symmetric if <span class="math inline">\({\bf A} = {\bf A}^T.\)</span></p>
</div>
<p>::: {.definition #def-eigen name = “Eigenvectors and Eigenvalues”}
Let <span class="math inline">\({\bf A}\in\mathbb{R}^{d\times d}\)</span>. If there is a scalar <span class="math inline">\(\lambda\)</span> and vector <span class="math inline">\(\vec{x}\ne \vec{0}\)</span> such that <span class="math inline">\({\bf A}\vec{x} = \lambda \vec{x}\)</span> then we say <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\({\bf A}\)</span> with associated eigenvector <span class="math inline">\(\vec{x}.\)</span>
:::</p>
<div class="definition">
<p><span id="def:def-psd" class="definition"><strong>Definition 2.5  (Positive definite) </strong></span>Let <span class="math inline">\({\bf A}\in\mathbb{R}^{d\times d}\)</span>. If <span class="math inline">\(\vec{x}^T{\bf A}\vec{x} &gt;0\)</span> for all <span class="math inline">\(\vec{x}\ne \vec{0}\)</span> then we say <span class="math inline">\({\bf A}\)</span> is positive definite. If instead, <span class="math inline">\(\vec{x}^T{\bf A}\vec{x} \ge 0\)</span> for all <span class="math inline">\(\vec{x}\ne \vec{0}\)</span> we say that <span class="math inline">\({\bf A}\)</span> is positive semi-definite.</p>
</div>
</div>
<div id="matrix-factorizations" class="section level3 hasAnchor" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Matrix Factorizations<a href="linear-algebra.html#matrix-factorizations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two different matrix factorization will arise many times throughout the text. The first, which is commonly presented in linear algebra courses, is the spectral decomposition of a square matrix which is also known as diagonalization or eigenvalue decomposition. Herein, we assume familiarity with eigenvalues and eigenvectors. The second factorization is the singular value decomposition. In the subsequent subsections, we briefly discuss these two factorizations, their geometric interpretation, and some notation that will typically be used in each case.</p>
<div id="eigenvalue-decomposition" class="section level4 hasAnchor" number="2.4.5.1">
<h4><span class="header-section-number">2.4.5.1</span> Eigenvalue Decomposition<a href="linear-algebra.html#eigenvalue-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We begin with the eigenvalue decomposition of a square matrix <span class="math inline">\({\bf A}\in\mathbb{R}^{d\times d}\)</span>. As you may recall, <span class="math inline">\({\bf A}\)</span> will have a set of <span class="math inline">\(d\)</span> eigenvalues <span class="math inline">\(\lambda_1,\dots, \lambda_d\)</span> (which may include repeated values) and associated eigenvectors. A number, <span class="math inline">\(\lambda\)</span>, may be repeated in the list of eigenvalues, and the number of times is called the algebraic multiplicity of <span class="math inline">\(\lambda\)</span>. Each eigenvalue has a least one eigenvector. In cases where the eigenvalue has algebraic multiplicity greater than one, we refer to its geometric multiplicity as the number of linearly independent eigenvectors associated with the eigenvalue.</p>
<p>The algebraic multiplicity is always greater than or equal to the geometric multiplicity. However, this is not always the case, and when this occurs, the matrix cannot be diagonalized. Fortunately, we will largely be dealing with symmetric matrices for which diagonalization is guaranteed by the following theorem.</p>
<div class="theorem">
<p><span id="thm:spectral" class="theorem"><strong>Theorem 2.1  (Spectral Decomposition Theorem for Symmetric Matrices) </strong></span>Any symmetric matrix <span class="math inline">\({\bf A}\in\mathbb{R}^{d\times d}\)</span> can be written as <span class="math display">\[{\bf A} = {\bf U\Lambda U}^T\]</span> where <span class="math inline">\({\bf U}\in\mathbb{R}^{d\times d}\)</span> is an orthonormal matrix and <span class="math inline">\({\bf \Lambda}\)</span> is a diagonal matrix <span class="math display">\[{\bf \Lambda} = \begin{bmatrix} \lambda_1 &amp; 0&amp;0 \\
0&amp; \ddots &amp;0 \\
0&amp;0 &amp;\lambda_d\end{bmatrix}\]</span> where the scalars <span class="math inline">\(\lambda_1,\dots,\lambda_d \in \mathbb{R}\)</span> are the eigenvalues of <span class="math inline">\({\bf A}\)</span> and the corresponding columns of <span class="math inline">\({\bf U}\)</span> are their associated eigenvectors.</p>
</div>
<p>By convention, we will always assume the eigenvalues are in decreasing order so that <span class="math inline">\(\lambda_1\ge \lambda_2 \ge \dots \ge \lambda_d\)</span>. The most common types of symmetric matrices that we will encounter are covariance matrices. In those cases, the spectral decomposition of the covariance can provide some helpful insight about the shape of the associated probability density. We demonstrate this idea graphically in the following examples using the MVN.</p>
<div class="example">
<p><span id="exm:mvn-level-curves" class="example"><strong>Example 2.4  (Level curves of MVN in) </strong></span>Consider</p>
<p>NOTES: Add example of Gaussian densities in <span class="math inline">\(\mathbb{R}^2\)</span> with level curves and eigenvectors/values.</p>
</div>
</div>
<div id="singular-value-decomposition" class="section level4 hasAnchor" number="2.4.5.2">
<h4><span class="header-section-number">2.4.5.2</span> Singular Value Decomposition<a href="linear-algebra.html#singular-value-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The spectral theorem is limited in that it requires a matrix to be both square and symmetric. When focusing on data matrices <span class="math inline">\({\bf X}\in\mathbb{R}^{N\times d}\)</span> both assumptions are extremely unlikely to be satisfied, and we will need a more flexible class of methods. This idea is explored in much greater detail in Chapter 4. For now, we briefly introduce the Singular Value Decomposition and how this factorization provides some insight on the geometric structure of matrix-vector multiplication.</p>
<div class="definition">
<p><span id="def:svd" class="definition"><strong>Definition 2.6  (The Singular Value Decomposition (SVD)) </strong></span>Let <span class="math inline">\({\bf A} \in \mathbb{R}^{m\times n}\)</span> be a rank <span class="math inline">\(k\)</span> matrix. Then <span class="math inline">\({\bf A}\)</span> may be factored as
<span class="math display" id="eq:def-reduced-svd">\[\begin{equation}
{\bf A} = \tilde{\bf U}\tilde{\bf S}\tilde{\bf V}^T
\tag{2.7}
\end{equation}\]</span>
where <span class="math inline">\(\tilde{\bf U}\in\mathbb{R}^{m\times k}\)</span> and <span class="math inline">\(\tilde{\bf V}\in\mathbb{R}^{k\times n}\)</span> have orthonormal columns and <span class="math inline">\(\tilde{\bf S}\in\mathbb{R}^{k\times k}\)</span> is a diagonal matrix with real entries <span class="math inline">\(\sigma_1 \ge \dots \sigma_k \ge 0\)</span> along the diagonal. We refer to <a href="linear-algebra.html#eq:def-reduced-svd">(2.7)</a> as the reduced singular value decomposition of <span class="math inline">\({\bf A}.\)</span> The columns of <span class="math inline">\(\tilde{\bf U}\)</span> (<span class="math inline">\(\tilde{\bf V}\)</span>) are called the left (right) singular vectors of <span class="math inline">\({\bf A}\)</span> and the scalars <span class="math inline">\(\sigma_1\ge \dots\ge \sigma_k\)</span> are referred to as the singular values of <span class="math inline">\({\bf A}.\)</span></p>
<p>Let <span class="math inline">\(\vec{u}_1,\dots,\vec{u}_k\in\mathbb{R}^m\)</span> be the columns of <span class="math inline">\(\tilde{\bf U}\)</span>. When <span class="math inline">\(m &gt;k\)</span>, we may find additional vectors <span class="math inline">\(\vec{u}_{k+1},\dots,\vec{u}_m\)</span> such that <span class="math inline">\(\{\vec{u}_1,\dots,\vec{u}_m\}\)</span> are an orthonormal basis for <span class="math inline">\(\mathbb{R}^m\)</span>. Similarly, we can apply the same reasoning to <span class="math inline">\(\tilde{\bf V}\)</span> to find an orthonormal basis of <span class="math inline">\(\mathbb{R}^n\)</span> with the first <span class="math inline">\(k\)</span> such vectors corresponding to the right singular vectors of <span class="math inline">\({\bf A}\)</span>. The (full) singular value decomposition of <span class="math inline">\({\bf A}\)</span> is
<span class="math display" id="eq:def-svd">\[\begin{equation}
{\bf A} = {\bf US\bf V}^T
\tag{2.8}
\end{equation}\]</span>
where <span class="math display">\[{\bf U} = \begin{bmatrix} \vec{u}_1 &amp; \dots &amp; \vec{u}_m\end{bmatrix}\in\mathbb{R}^{m\times m} \qquad {\bf V} = \begin{bmatrix} \vec{v}_1 &amp; \dots &amp; \vec{v}_n\end{bmatrix}\in\mathbb{R}^{n\times n}\]</span>
are orthonormal matrices. The matrix <span class="math inline">\({\bf S}\in\mathbb{R}^{m\times n}\)</span> is formed by taking <span class="math inline">\(\tilde{S}\)</span> and padding it with zero along the bottom and right so that it is <span class="math inline">\(m\times n\)</span>.</p>
</div>
<p>When considering matrix muliplication <span class="math inline">\({\bf A}\vec{x}\)</span>, the full SVD of <span class="math inline">\({\bf A}\)</span> is helpful for decomposition this matrix multiplication into three more interpretable steps.</p>
</div>
</div>
<div id="symmetry-positive-definiteness-and-matrix-powers" class="section level3 hasAnchor" number="2.4.6">
<h3><span class="header-section-number">2.4.6</span> Symmetry, Positive Definiteness, and Matrix Powers<a href="linear-algebra.html#symmetry-positive-definiteness-and-matrix-powers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="expectation-mean-and-covariance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-probability_review.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
